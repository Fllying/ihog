<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
            "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>
<TITLE>SPAMS: a SPArse Modeling Software, v2.3
</TITLE>

<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
<META name="GENERATOR" content="hevea 1.10">
<STYLE type="text/css">
.li-itemize{margin:1ex 0ex;}
.toc{list-style:none;}
.title{margin:2ex auto;text-align:center}
.flushleft{text-align:left;margin-left:0ex;margin-right:auto;}
.flushright{text-align:right;margin-left:auto;margin-right:0ex;}
DIV TABLE{margin-left:inherit;margin-right:inherit;}
PRE{text-align:left;margin-left:0ex;margin-right:auto;}
BLOCKQUOTE{margin-left:4ex;margin-right:4ex;text-align:left;}
TD P{margin:0px;}
.hbar{border:none;height:2px;width:100%;background-color:black;}
.display{border-collapse:separate;border-spacing:2px;width:auto; border:none;}
.dcell{white-space:nowrap;padding:0px;width:auto; border:none;}
.dcenter{margin:0ex auto;}
.mouselstlisting{font-family:monospace;margin-right:auto;margin-left:0pt;text-align:left}
.mouselstlisting{font-family:monospace;margin-right:auto;margin-left:0pt;text-align:left}
</STYLE>

<META name="Author" content="Julien Mairal">
<link rel="stylesheet" href="doc_spams.css">
</HEAD>
<BODY >
<!--HEVEA command line is: /usr/bin/hevea -O article.hva macrocss.hva doc_spams.tex -->
<!--CUT DEF section 1 --><TABLE CLASS="title"><TR><TD><H1 CLASS="titlemain">SPAMS: a SPArse Modeling Software, v2.3</H1><H3 CLASS="titlerest">Julien Mairal<BR>
 <TT>julien.mairal@m4x.org</TT>
</H3></TD></TR>
</TABLE><!--TOC section Contents-->
<H2 CLASS="section"><!--SEC ANCHOR -->Contents</H2><!--SEC END --><UL CLASS="toc"><LI CLASS="li-toc">
<A HREF="#htoc1">1  Introduction</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc2">2  Installation</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc3">3  Dictionary Learning and Matrix Factorization Toolbox</A>
<UL CLASS="toc"><LI CLASS="li-toc">
<A HREF="#htoc4">3.1  Function spams.trainDL</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc5">3.2  Function spams.trainDL_Memory</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc6">3.3  Function nmf</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc7">3.4  Function nnsc</A>
</LI></UL>
</LI><LI CLASS="li-toc"><A HREF="#htoc8">4  Sparse Decomposition Toolbox</A>
<UL CLASS="toc"><LI CLASS="li-toc">
<A HREF="#htoc9">4.1  Function spams.omp</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc10">4.2  Function spams.ompMask</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc11">4.3  Function spams.lasso</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc12">4.4  Function spams.lassoWeighted</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc13">4.5  Function spams.lassoMask</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc14">4.6  Function spams.cd</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc15">4.7  Function spams.somp</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc16">4.8  Function spams.l1L2BCD</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc17">4.9  Function spams.sparseProject</A>
</LI></UL>
</LI><LI CLASS="li-toc"><A HREF="#htoc18">5  Proximal Toolbox</A>
<UL CLASS="toc"><LI CLASS="li-toc">
<A HREF="#htoc19">5.1  Regularization Functions</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc20">5.2  Function spams.proximalFlat</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc21">5.3  Function spams.proximalTree</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc22">5.4  Function spams.proximalGraph</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc23">5.5  Function spams.proximalPathCoding</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc24">5.6  Function spams.evalPathCoding</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc25">5.7  Problems Addressed</A>
<UL CLASS="toc"><LI CLASS="li-toc">
<A HREF="#htoc26">5.7.1  Regression Problems with the Square Loss</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc27">5.7.2  Classification Problems with the Logistic Loss</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc28">5.7.3  Multi-class Classification Problems with the Softmax Loss</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc29">5.7.4  Multi-task Regression Problems with the Square Loss</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc30">5.7.5  Multi-task Classification Problems with the Logistic Loss</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc31">5.7.6  Multi-task and Multi-class Classification Problems with the Softmax Loss</A>
</LI></UL>
</LI><LI CLASS="li-toc"><A HREF="#htoc32">5.8  Function spams.fistaFlat</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc33">5.9  Function spams.fistaTree</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc34">5.10  Function spams.fistaGraph</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc35">5.11  Function spams.fistaPathCoding</A>
</LI></UL>
</LI><LI CLASS="li-toc"><A HREF="#htoc36">6  Miscellaneous Functions</A>
<UL CLASS="toc"><LI CLASS="li-toc">
<A HREF="#htoc37">6.1  Function spams.conjGrad</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc38">6.2  Function spams.bayer</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc39">6.3  Function spams.calcAAt</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc40">6.4  Function spams.calcXAt</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc41">6.5  Function spams.calcXY</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc42">6.6  Function spams.calcXYt</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc43">6.7  Function spams.calcXtY</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc44">6.8  Function spams.invSym</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc45">6.9  Function spams.normalize</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc46">6.10  Function spams.sort</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc47">6.11  Function mexDisplayPatches</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc48">6.12  Function spams.countPathsDAG</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc49">6.13  Function spams.removeCyclesGraph</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc50">6.14  Function spams.countConnexComponents</A>
</LI></UL>
</LI><LI CLASS="li-toc"><A HREF="#htoc51">A  Duality Gaps with Fenchel Duality</A>
<UL CLASS="toc">
<UL CLASS="toc"><LI CLASS="li-toc">
<A HREF="#htoc52">A.0.1  Duality Gaps without Intercepts</A>
</LI><LI CLASS="li-toc"><A HREF="#htoc53">A.0.2  Duality Gaps with Intercepts</A>
</LI></UL>
</UL>
</LI></UL><!--TOC section Introduction-->
<H2 CLASS="section"><!--SEC ANCHOR --><A NAME="htoc1">1</A>  Introduction</H2><!--SEC END --><P>
SPAMS (SPArse Modeling Software) is an open-source optimization toolbox under
licence GPLv3. It implements algorithms for solving various machine learning
and signal processing problems involving sparse regularizations.</P><P>The library is coded in C++, is compatible with Linux, Mac, and Windows 32bits
and 64bits Operating Systems. It is interfaced with Matlab, but can be called
from any C++ application. A R and Python interface has been developed by
Jean-Paul Chieze.</P><P>It requires an implementation of BLAS and LAPACK for performing efficient
linear algebra operations such as the one provided by matlab/R, atlas, netlib,
or the one provided by Intel (Math Kernel Library). It also exploits
multi-core CPUs when this feature is supported by the compiler, through
OpenMP.</P><P>The current licence is GPLv3 available at
<TT>http://www.gnu.org/licenses/gpl.html</TT>, which limits its usage. For other
usages (such as the use in proprietary softwares), please contact the author.</P><P>Version 2.3 of SPAMS is divided into three “toolboxes” and has a few
additional miscellaneous functions:
</P><UL CLASS="itemize"><LI CLASS="li-itemize">
The <B>Dictionary learning and matrix factorization toolbox</B>
contains the online learning technique of [<A HREF="#mairal7">19</A>, <A HREF="#mairal9">20</A>] and its
variants for solving various matrix factorization problems:
<UL CLASS="itemize"><LI CLASS="li-itemize">
dictionary Learning for sparse coding;
</LI><LI CLASS="li-itemize">sparse principal component analysis (seen as a sparse matrix factorization problem);
</LI><LI CLASS="li-itemize">non-negative matrix factorization;
</LI><LI CLASS="li-itemize">non-negative sparse coding.
</LI></UL>
</LI><LI CLASS="li-itemize">The <B>Sparse decomposition toolbox</B> contains efficient implementations of
<UL CLASS="itemize"><LI CLASS="li-itemize">
Orthogonal Matching Pursuit, (or Forward Selection) [<A HREF="#weisberg">32</A>, <A HREF="#mallat4">24</A>];
</LI><LI CLASS="li-itemize">the LARS/homotopy algorithm [<A HREF="#osborne">27</A>, <A HREF="#efron">8</A>] (variants for solving Lasso and Elastic-Net problems);
</LI><LI CLASS="li-itemize">a weighted version of LARS; 
</LI><LI CLASS="li-itemize">OMP and LARS when data come with a binary mask;
</LI><LI CLASS="li-itemize">a coordinate-descent algorithm for ℓ<SUB>1</SUB>-decomposition problems [<A HREF="#fu">11</A>, <A HREF="#friedman">9</A>, <A HREF="#wu">33</A>]; 
</LI><LI CLASS="li-itemize">a greedy solver for simultaneous signal approximation as defined in [<A HREF="#tropp2">31</A>, <A HREF="#tropp3">30</A>] (SOMP);
</LI><LI CLASS="li-itemize">a solver for simulatneous signal approximation with ℓ<SUB>1</SUB>/ℓ<SUB>2</SUB>-regularization based on block-coordinate descent;
</LI><LI CLASS="li-itemize">a homotopy method for the Fused-Lasso Signal Approximation as defined in [<A HREF="#friedman">9</A>] with the homotopy method presented in the appendix of [<A HREF="#mairal9">20</A>];
</LI><LI CLASS="li-itemize">a tool for projecting efficiently onto a few convex sets
inducing sparsity such as the ℓ<SUB>1</SUB>-ball using the method of
[<A HREF="#brucker">3</A>, <A HREF="#maculan">17</A>, <A HREF="#duchi">7</A>], and Elastic-Net or Fused Lasso constraint sets as
proposed in the appendix of [<A HREF="#mairal9">20</A>].
</LI></UL>
</LI><LI CLASS="li-itemize">The <B>Proximal toolbox</B>: An implementation of proximal methods
(ISTA and FISTA [<A HREF="#beck">1</A>]) for solving a large class of sparse approximation
problems with different combinations of loss and regularizations. One of the main
features of this toolbox is to provide a robust stopping criterion based on
<EM>duality gaps</EM> to control the quality of the optimization, whenever
possible. It also handles sparse feature matrices for large-scale problems. The following regularizations are implemented:
<UL CLASS="itemize"><LI CLASS="li-itemize">
Tikhonov regularization (squared ℓ<SUB>2</SUB>-norm);
</LI><LI CLASS="li-itemize">ℓ<SUB>1</SUB>-norm, ℓ<SUB>2</SUB>, ℓ<SUB>∞</SUB>-norms;
</LI><LI CLASS="li-itemize">Elastic-Net [<A HREF="#zou">35</A>];
</LI><LI CLASS="li-itemize">Fused Lasso [<A HREF="#tibshirani2">29</A>];
</LI><LI CLASS="li-itemize">tree-structured sum of ℓ<SUB>2</SUB>-norms (see [<A HREF="#jenatton3">14</A>, <A HREF="#jenatton4">15</A>]);
</LI><LI CLASS="li-itemize">tree-structured sum of ℓ<SUB>∞</SUB>-norms (see [<A HREF="#jenatton3">14</A>, <A HREF="#jenatton4">15</A>]);
</LI><LI CLASS="li-itemize">general sum of ℓ<SUB>∞</SUB>-norms (see [<A HREF="#mairal10">21</A>, <A HREF="#mairal13">22</A>]);
</LI><LI CLASS="li-itemize">mixed ℓ<SUB>1</SUB>/ℓ<SUB>2</SUB>-norms on matrices [<A HREF="#yuan">34</A>, <A HREF="#obozinski">26</A>];
</LI><LI CLASS="li-itemize">mixed ℓ<SUB>1</SUB>/ℓ<SUB>∞</SUB>-norms on matrices [<A HREF="#yuan">34</A>, <A HREF="#obozinski">26</A>];
</LI><LI CLASS="li-itemize">mixed ℓ<SUB>1</SUB>/ℓ<SUB>2</SUB>-norms on matrices plus ℓ<SUB>1</SUB> [<A HREF="#sprechmann">28</A>, <A HREF="#Friedman2010">10</A>];
</LI><LI CLASS="li-itemize">mixed ℓ<SUB>1</SUB>/ℓ<SUB>∞</SUB>-norms on matrices plus ℓ<SUB>1</SUB>;
</LI><LI CLASS="li-itemize">group-lasso with ℓ<SUB>2</SUB> or ℓ<SUB>∞</SUB>-norms;
</LI><LI CLASS="li-itemize">group-lasso+ℓ<SUB>1</SUB>;
</LI><LI CLASS="li-itemize">multi-task tree-structured sum of ℓ<SUB>∞</SUB>-norms (see [<A HREF="#mairal10">21</A>, <A HREF="#mairal13">22</A>]);
</LI><LI CLASS="li-itemize">trace norm;
</LI><LI CLASS="li-itemize">ℓ<SUB>0</SUB> pseudo-norm (only with ISTA);
</LI><LI CLASS="li-itemize">tree-structured ℓ<SUB>0</SUB> (only with ISTA);
</LI><LI CLASS="li-itemize">rank regularization for matrices (only with ISTA);
</LI><LI CLASS="li-itemize">the path-coding penalties of [<A HREF="#mairal14">23</A>].
</LI></UL>
All of these regularization functions can be used with the following losses
<UL CLASS="itemize"><LI CLASS="li-itemize">
square loss;
</LI><LI CLASS="li-itemize">square loss with missing observations; 
</LI><LI CLASS="li-itemize">logistic loss, weighted logistic loss;
</LI><LI CLASS="li-itemize">multi-class logistic.
</LI></UL>
This toolbox can also enforce non-negativity constraints, handle intercepts and
sparse matrices. There are also a few additional undocumented functionalities,
which are available in the source code.
</LI><LI CLASS="li-itemize">A few tools for performing linear algebra operations such as a
conjugate gradient algorithm, manipulating sparse matrices and graphs.
</LI></UL><P>The toolbox was written by Julien Mairal when he was at INRIA, with the
collaboration of Francis Bach (INRIA), Jean Ponce (Ecole Normale Supérieure),
Guillermo Sapiro (University of Minnesota), Guillaume Obozinski (INRIA) and
Rodolphe Jenatton (INRIA).</P><P>R and Python interfaces have been written by Jean-Paul Chieze (INRIA), and a
few contributors have helped us making compilation scripts for various platforms.</P><!--TOC section Installation-->
<H2 CLASS="section"><!--SEC ANCHOR --><A NAME="htoc2">2</A>  Installation</H2><!--SEC END --><P>
The SPAMS toolbox for Python is ditributed in source mode only.
It should compile on linux and Mac.
The installation procedure is described in the file <CODE>INSTALL-package</CODE>
</P><UL CLASS="itemize"><LI CLASS="li-itemize">
Download the tar gzipped file, unpack it.
</LI><LI CLASS="li-itemize">Enter directory spams-python
</LI><LI CLASS="li-itemize">execute<BR>
 <CODE>python setup.py install --prefix=&lt;your-installation-dir&gt;</CODE>
</LI></UL><P>You have the choice of the BLAS library, but the Intel 
MKL is recommended for the best performance.</P><P>If you want to add or change libraries, you must modify
the file <CODE>setup.py</CODE>.</P><P>The documentation is available in pdf and html format in the doc subdirectory.
</P><!--TOC section Dictionary Learning and Matrix Factorization Toolbox-->
<H2 CLASS="section"><!--SEC ANCHOR --><A NAME="htoc3">3</A>  Dictionary Learning and Matrix Factorization Toolbox</H2><!--SEC END --><P>
This is the section for dictionary learning and matrix factorization, corresponding to [<A HREF="#mairal7">19</A>, <A HREF="#mairal9">20</A>].</P><!--TOC subsection Function spams.trainDL-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc4">3.1</A>  Function spams.trainDL</H3><!--SEC END --><P>
This is the main function of the toolbox, implementing the learning algorithms of [<A HREF="#mairal9">20</A>]. 
Given a training set <I><B>x</B></I><SUP>1</SUP>,…, . It aims at solving
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>D</B></I> ∈ <FONT COLOR=red><I>C</I></FONT></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">lim</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>n</I> → +∞</TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">α<I><SUP>i</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell">⎛<BR>
⎜<BR>
⎝</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||<I><B>x</B><SUP>i</SUP></I>−<I><B>D</B></I>α<I><SUP>i</SUP></I>||<SUB>2</SUB><SUP>2</SUP> + ψ(α<I><SUP>i</SUP></I>)</TD><TD CLASS="dcell">⎞<BR>
⎟<BR>
⎠</TD><TD CLASS="dcell">.
    (1)</TD></TR>
</TABLE><P>
ψ is a sparsity-inducing regularizer and <FONT COLOR=red><I>C</I></FONT> is a constraint set for the dictionary. As shown in [<A HREF="#mairal9">20</A>] 
and in the help file below, various combinations can be used for ψ and <FONT COLOR=red><I>C</I></FONT> for solving different matrix factorization problems.
What is more, positivity constraints can be added to α as well. The function admits several modes for choosing the optimization parameters, using the parameter-free strategy proposed in [<A HREF="#mairal7">19</A>], or using the parameters <I>t</I><SUB>0</SUB> and ρ presented
in [<A HREF="#mairal9">20</A>]. <B>Note that for problems of a reasonable size, and when ψ is the ℓ<SUB>1</SUB>-norm, 
the function spams.trainDL_Memory can be faster but uses more memory.</B> </P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: trainDL<BR>
#<BR>
# Usage: spams.trainDL(X,return_model= False,model= None,D = None,numThreads = -1,batchsize = -1,<BR>
#               K= -1,lambda1= None,lambda2= 10e-10,iter=-1,t0=1e-5,mode=spams_wrap.PENALTY,<BR>
#               posAlpha=False,posD=False,expand=False,modeD=spams_wrap.L2,whiten=False,<BR>
#               clean=True,verbose=True,gamma1=0.,gamma2=0.,rho=1.0,iter_updateD=None,<BR>
#               stochastic_deprecated=False,modeParam=0,batch=False,log_deprecated=False,<BR>
#               logName='')<BR>
#<BR>
# Description:<BR>
#     trainDL is an efficient implementation of the<BR>
#     dictionary learning technique presented in<BR>
#     <BR>
#     "Online Learning for Matrix Factorization and Sparse Coding"<BR>
#     by Julien Mairal, Francis Bach, Jean Ponce and Guillermo Sapiro<BR>
#     arXiv:0908.0050<BR>
#     <BR>
#     "Online Dictionary Learning for Sparse Coding"      <BR>
#     by Julien Mairal, Francis Bach, Jean Ponce and Guillermo Sapiro<BR>
#     ICML 2009.<BR>
#     <BR>
#     Note that if you use mode=1 or 2, if the training set has a<BR>
#     reasonable size and you have enough memory on your computer, you <BR>
#     should use trainDL_Memory instead.<BR>
#     <BR>
#     <BR>
#     It addresses the dictionary learning problems<BR>
#        1) if mode=0<BR>
#     min_{D in C} (1/n) sum_{i=1}^n (1/2)||x_i-Dalpha_i||_2^2  s.t. ...<BR>
#                                                  ||alpha_i||_1 &lt;= lambda1<BR>
#        2) if mode=1<BR>
#     min_{D in C} (1/n) sum_{i=1}^n  ||alpha_i||_1  s.t.  ...<BR>
#                                           ||x_i-Dalpha_i||_2^2 &lt;= lambda1<BR>
#        3) if mode=2<BR>
#     min_{D in C} (1/n) sum_{i=1}^n (1/2)||x_i-Dalpha_i||_2^2 + ... <BR>
#                                  lambda1||alpha_i||_1 + lambda1_2||alpha_i||_2^2<BR>
#        4) if mode=3, the sparse coding is done with OMP<BR>
#     min_{D in C} (1/n) sum_{i=1}^n (1/2)||x_i-Dalpha_i||_2^2  s.t. ... <BR>
#                                                  ||alpha_i||_0 &lt;= lambda1<BR>
#        5) if mode=4, the sparse coding is done with OMP<BR>
#     min_{D in C} (1/n) sum_{i=1}^n  ||alpha_i||_0  s.t.  ...<BR>
#                                           ||x_i-Dalpha_i||_2^2 &lt;= lambda1<BR>
#        6) if mode=5, the sparse coding is done with OMP<BR>
#     min_{D in C} (1/n) sum_{i=1}^n 0.5||x_i-Dalpha_i||_2^2 +lambda1||alpha_i||_0  <BR>
#     <BR>
#     <BR>
#     C is a convex set verifying<BR>
#        1) if modeD=0<BR>
#           C={  D in Real^{m x p}  s.t.  forall j,  ||d_j||_2^2 &lt;= 1 }<BR>
#        2) if modeD=1<BR>
#           C={  D in Real^{m x p}  s.t.  forall j,  ||d_j||_2^2 + ... <BR>
#                                                  gamma1||d_j||_1 &lt;= 1 }<BR>
#        3) if modeD=2<BR>
#           C={  D in Real^{m x p}  s.t.  forall j,  ||d_j||_2^2 + ... <BR>
#                                  gamma1||d_j||_1 + gamma2 FL(d_j) &lt;= 1 }<BR>
#        4) if modeD=3<BR>
#           C={  D in Real^{m x p}  s.t.  forall j,  (1-gamma1)||d_j||_2^2 + ... <BR>
#                                  gamma1||d_j||_1 &lt;= 1 }<BR>
#                                  <BR>
#     Potentially, n can be very large with this algorithm.<BR>
#<BR>
# Inputs:<BR>
#       X:  double m x n matrix   (input signals)<BR>
#               m is the signal size<BR>
#               n is the number of signals to decompose<BR>
#       return_model:     <BR>
#               if true the function will return the model<BR>
#               as a named list ('A' = A, 'B' = B, 'iter' = n)<BR>
#       model:        None or model (as A,B,iter) to use as initialisation<BR>
#       D: (optional) double m x p matrix   (dictionary)<BR>
#         p is the number of elements in the dictionary<BR>
#         When D is not provided, the dictionary is initialized <BR>
#         with random elements from the training set.<BR>
#       K: (size of the dictionary, optional is D is provided)<BR>
#       lambda1:  (parameter)<BR>
#       lambda2:  (optional, by default 0)<BR>
#       iter: (number of iterations).  If a negative number is <BR>
#          provided it will perform the computation during the<BR>
#          corresponding number of seconds. For instance iter=-5<BR>
#          learns the dictionary during 5 seconds.<BR>
#       mode: (optional, see above, by default 2) <BR>
#       posAlpha: (optional, adds positivity constraints on the<BR>
#         coefficients, false by default, not compatible with <BR>
#         mode =3,4)<BR>
#       modeD: (optional, see above, by default 0)<BR>
#       posD: (optional, adds positivity constraints on the <BR>
#         dictionary, false by default, not compatible with <BR>
#         modeD=2)<BR>
#       gamma1: (optional parameter for modeD &gt;= 1)<BR>
#       gamma2: (optional parameter for modeD = 2)<BR>
#       batchsize: (optional, size of the minibatch, by default <BR>
#          512)<BR>
#       iter_updateD: (optional, number of BCD iterations for the dictionary<BR>
#          update step, by default 1)<BR>
#       modeParam: (optimization mode).<BR>
#          1) if modeParam=0, the optimization uses the <BR>
#             parameter free strategy of the ICML paper<BR>
#          2) if modeParam=1, the optimization uses the <BR>
#             parameters rho as in arXiv:0908.0050<BR>
#          3) if modeParam=2, the optimization uses exponential <BR>
#             decay weights with updates of the form <BR>
#             A_{t} &lt;- rho A_{t-1} + alpha_t alpha_t^T<BR>
#       rho: (optional) tuning parameter (see paper arXiv:0908.0050)<BR>
#       t0: (optional) tuning parameter (see paper arXiv:0908.0050)<BR>
#       clean: (optional, true by default. prunes <BR>
#          automatically the dictionary from unused elements).<BR>
#       verbose: (optional, true by default, increase verbosity)<BR>
#       numThreads: (optional, number of threads for exploiting<BR>
#          multi-core / multi-cpus. By default, it takes the value -1,<BR>
#          which automatically selects all the available CPUs/cores).<BR>
#       expand:    undocumented; modify at your own risks!<BR>
#       whiten:    undocumented; modify at your own risks!<BR>
#       stochastic_deprecated:    undocumented; modify at your own risks!<BR>
#       batch:    undocumented; modify at your own risks!<BR>
#       log_deprecated:    undocumented; modify at your own risks!<BR>
#       logName:    undocumented; modify at your own risks!<BR>
#<BR>
# Output:<BR>
#       D:     double m x p matrix   (dictionary)<BR>
#       model:  the model as A B iter<BR>
#        D = spams.trainDL(X,return_model = False,...)<BR>
#        (D,model) = spams.trainDL(X,return_model = True,...)<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2009 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#<BR>
# Note:<BR>
#     this function admits a few experimental usages, which have not<BR>
#     been extensively tested:<BR>
#         - single precision setting <BR>
#</FONT></TD></TR>
</TABLE><P>The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
img_file = <FONT COLOR="red">'../extdata/boat.png'</FONT><BR>
<FONT COLOR="blue"><B>try</B></FONT>:<BR>
    img = Image.open(img_file)<BR>
<FONT COLOR="blue"><B>except</B></FONT>:<BR>
    <FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"Cannot load image %s : skipping test"</FONT> %img_file<BR>
I = np.array(img) / 255.<BR>
<FONT COLOR="blue"><B>if</B></FONT> I.ndim == 3:<BR>
    A = np.asfortranarray(I.reshape((I.shape[0],I.shape[1] * I.shape[2])))<BR>
    rgb = True<BR>
<FONT COLOR="blue"><B>else</B></FONT>:<BR>
    A = np.asfortranarray(I)<BR>
    rgb = False<BR>
<BR>
m = 8;n = 8;<BR>
X = spams.im2col_sliding(A,m,n,rgb)<BR>
<BR>
X = X - np.tile(np.mean(X,0),(X.shape[0],1))<BR>
X = np.asfortranarray(X / np.tile(np.sqrt((X * X).sum(axis=0)),(X.shape[0],1)))<BR>
param = { <FONT COLOR="red">'K'</FONT> : 100, <FONT COLOR="#007F00"># learns a dictionary with 100 elements</FONT><BR>
          <FONT COLOR="red">'lambda1'</FONT> : 0.15, <FONT COLOR="red">'numThreads'</FONT> : 4, <FONT COLOR="red">'batchsize'</FONT> : 400,<BR>
          <FONT COLOR="red">'iter'</FONT> : 1000}<BR>
<BR>
<FONT COLOR="#007F00">########## FIRST EXPERIMENT ###########</FONT><BR>
tic = time.time()<BR>
D = spams.trainDL(X,**param)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'time of computation for Dictionary Learning: %f'</FONT> %t<BR>
<BR>
<FONT COLOR="#007F00">##param['approx'] = 0</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'Evaluating cost function...'</FONT><BR>
lparam = _extract_lasso_param(param)<BR>
alpha = spams.lasso(X,D = D,**lparam)<BR>
xd = X - D * alpha<BR>
R = np.mean(0.5 * (xd * xd).sum(axis=0) + param[<FONT COLOR="red">'lambda1'</FONT>] * np.abs(alpha).sum(axis=0))<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"objective function: %f"</FONT> %R<BR>
<BR>
<FONT COLOR="#007F00">#### SECOND EXPERIMENT ####</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"*********** SECOND EXPERIMENT ***********"</FONT><BR>
<BR>
X1 = X[:,0:X.shape[1]/2]<BR>
X2 = X[:,X.shape[1]/2 -1:]<BR>
param[<FONT COLOR="red">'iter'</FONT>] = 500<BR>
tic = time.time()<BR>
(D,model) = spams.trainDL(X1,return_model = True,**param)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'time of computation for Dictionary Learning: %f\n'</FONT> %t<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'Evaluating cost function...'</FONT><BR>
alpha = spams.lasso(X,D = D,**lparam)<BR>
xd = X - D * alpha<BR>
R = np.mean(0.5 * (xd * xd).sum(axis=0) + param[<FONT COLOR="red">'lambda1'</FONT>] * np.abs(alpha).sum(axis=0))<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"objective function: %f"</FONT> %R<BR>
<BR>
<FONT COLOR="#007F00"># Then reuse the learned model to retrain a few iterations more.</FONT><BR>
param2 = param.copy()<BR>
param2[<FONT COLOR="red">'D'</FONT>] = D<BR>
tic = time.time()<BR>
(D,model) = spams.trainDL(X2,return_model = True,model = model,**param2)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'time of computation for Dictionary Learning: %f'</FONT> %t<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'Evaluating cost function...'</FONT><BR>
alpha = spams.lasso(X,D = D,**lparam)<BR>
xd = X - D * alpha<BR>
R = np.mean(0.5 * (xd * xd).sum(axis=0) + param[<FONT COLOR="red">'lambda1'</FONT>] * np.abs(alpha).sum(axis=0))<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"objective function: %f"</FONT> %R<BR>
<BR>
<FONT COLOR="#007F00">#################### THIRD &amp; FOURTH EXPERIMENT ######################<BR>
# let us add sparsity to the dictionary itself</FONT><BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'*********** THIRD EXPERIMENT ***********'</FONT><BR>
param[<FONT COLOR="red">'modeParam'</FONT>] = 0<BR>
param[<FONT COLOR="red">'iter'</FONT>] = 1000<BR>
param[<FONT COLOR="red">'gamma1'</FONT>] = 0.3<BR>
param[<FONT COLOR="red">'modeD'</FONT>] = 1<BR>
<BR>
tic = time.time()<BR>
D = spams.trainDL(X,**param)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'time of computation for Dictionary Learning: %f'</FONT> %t<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'Evaluating cost function...'</FONT><BR>
alpha = spams.lasso(X,D = D,**lparam)<BR>
xd = X - D * alpha<BR>
R = np.mean(0.5 * (xd * xd).sum(axis=0) + param[<FONT COLOR="red">'lambda1'</FONT>] * np.abs(alpha).sum(axis=0))<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"objective function: %f"</FONT> %R<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'*********** FOURTH EXPERIMENT ***********'</FONT><BR>
param[<FONT COLOR="red">'modeParam'</FONT>] = 0<BR>
param[<FONT COLOR="red">'iter'</FONT>] = 1000<BR>
param[<FONT COLOR="red">'gamma1'</FONT>] = 0.3<BR>
param[<FONT COLOR="red">'modeD'</FONT>] = 3<BR>
<BR>
tic = time.time()<BR>
D = spams.trainDL(X,**param)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'time of computation for Dictionary Learning: %f'</FONT> %t<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'Evaluating cost function...'</FONT><BR>
alpha = spams.lasso(X,D = D,**lparam)<BR>
xd = X - D * alpha<BR>
R = np.mean(0.5 * (xd * xd).sum(axis=0) + param[<FONT COLOR="red">'lambda1'</FONT>] * np.abs(alpha).sum(axis=0))<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"objective function: %f"</FONT> %R</TD></TR>
</TABLE><!--TOC subsection Function spams.trainDL_Memory-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc5">3.2</A>  Function spams.trainDL_Memory</H3><!--SEC END --><P>
Memory-consuming version of spams.trainDL. This function is well adapted to small/medium-size problems:
It requires storing all the coefficients α and is therefore impractical
for very large datasets. However, in many situations, one can afford this memory cost and it is better to use this method, which 
is faster than spams.trainDL.
Note that unlike spams.trainDL this function does not allow warm-restart.
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: trainDL_Memory<BR>
#<BR>
# Usage: spams.trainDL_Memory(X,D = None,numThreads = -1,batchsize = -1,K= -1,lambda1= None,iter=-1,<BR>
#                      t0=1e-5,mode=spams_wrap.PENALTY,posD=False,expand=False,<BR>
#                      modeD=spams_wrap.L2,whiten=False,clean=True,gamma1=0.,gamma2=0.,<BR>
#                      rho=1.0,iter_updateD=1,stochastic_deprecated=False,modeParam=0,<BR>
#                      batch=False,log_deprecated=False,logName='')<BR>
#<BR>
# Description:<BR>
#     trainDL_Memory is an efficient but memory consuming <BR>
#     variant of the dictionary learning technique presented in<BR>
#     <BR>
#     "Online Learning for Matrix Factorization and Sparse Coding"<BR>
#     by Julien Mairal, Francis Bach, Jean Ponce and Guillermo Sapiro<BR>
#     arXiv:0908.0050<BR>
#     <BR>
#     "Online Dictionary Learning for Sparse Coding"      <BR>
#     by Julien Mairal, Francis Bach, Jean Ponce and Guillermo Sapiro<BR>
#     ICML 2009.<BR>
#     <BR>
#     Contrary to the approaches above, the algorithm here <BR>
#        does require to store all the coefficients from all the training<BR>
#        signals. For this reason this variant can not be used with large<BR>
#        training sets, but is more efficient than the regular online<BR>
#        approach for training sets of reasonable size.<BR>
#        <BR>
#     It addresses the dictionary learning problems<BR>
#        1) if mode=1<BR>
#     min_{D in C} (1/n) sum_{i=1}^n  ||alpha_i||_1  s.t.  ...<BR>
#                                         ||x_i-Dalpha_i||_2^2 &lt;= lambda1<BR>
#        2) if mode=2<BR>
#     min_{D in C} (1/n) sum_{i=1}^n (1/2)||x_i-Dalpha_i||_2^2 + ... <BR>
#                                                      lambda1||alpha_i||_1  <BR>
#                                                      <BR>
#     C is a convex set verifying<BR>
#        1) if modeD=0<BR>
#           C={  D in Real^{m x p}  s.t.  forall j,  ||d_j||_2^2 &lt;= 1 }<BR>
#        1) if modeD=1<BR>
#           C={  D in Real^{m x p}  s.t.  forall j,  ||d_j||_2^2 + ... <BR>
#                                                  gamma1||d_j||_1 &lt;= 1 }<BR>
#        1) if modeD=2<BR>
#           C={  D in Real^{m x p}  s.t.  forall j,  ||d_j||_2^2 + ... <BR>
#                                  gamma1||d_j||_1 + gamma2 FL(d_j) &lt;= 1 }<BR>
#                                  <BR>
#     Potentially, n can be very large with this algorithm.<BR>
#<BR>
# Inputs:<BR>
#       X:  double m x n matrix   (input signals)<BR>
#               m is the signal size<BR>
#               n is the number of signals to decompose<BR>
#       D: (optional) double m x p matrix   (dictionary)<BR>
#         p is the number of elements in the dictionary<BR>
#         When D is not provided, the dictionary is initialized <BR>
#         with random elements from the training set.<BR>
#       K: (size of the dictionary, optional is D is provided)<BR>
#       lambda1:  (parameter)<BR>
#       iter: (number of iterations).  If a negative number is <BR>
#          provided it will perform the computation during the<BR>
#          corresponding number of seconds. For instance iter=-5<BR>
#          learns the dictionary during 5 seconds.<BR>
#       mode: (optional, see above, by default 2) <BR>
#       modeD: (optional, see above, by default 0)<BR>
#       posD: (optional, adds positivity constraints on the <BR>
#         dictionary, false by default, not compatible with <BR>
#         modeD=2)<BR>
#       gamma1: (optional parameter for modeD &gt;= 1)<BR>
#       gamma2: (optional parameter for modeD = 2)<BR>
#       batchsize: (optional, size of the minibatch, by default <BR>
#         512)<BR>
#       iter_updateD: (optional, number of BCD iterations for the dictionary <BR>
#           update step, by default 1)<BR>
#       modeParam: (optimization mode).<BR>
#         1) if modeParam=0, the optimization uses the <BR>
#            parameter free strategy of the ICML paper<BR>
#         2) if modeParam=1, the optimization uses the <BR>
#            parameters rho as in arXiv:0908.0050<BR>
#         3) if modeParam=2, the optimization uses exponential <BR>
#            decay weights with updates of the form <BR>
#            A_{t} &lt;- rho A_{t-1} + alpha_t alpha_t^T<BR>
#       rho: (optional) tuning parameter (see paper arXiv:0908.0050)<BR>
#       t0: (optional) tuning parameter (see paper arXiv:0908.0050)<BR>
#       clean: (optional, true by default. prunes <BR>
#         automatically the dictionary from unused elements).<BR>
#       numThreads: (optional, number of threads for exploiting<BR>
#         multi-core / multi-cpus. By default, it takes the value -1,<BR>
#         which automatically selects all the available CPUs/cores).<BR>
#       expand:    undocumented; modify at your own risks!<BR>
#       whiten:    undocumented; modify at your own risks!<BR>
#       stochastic_deprecated:    undocumented; modify at your own risks!<BR>
#       batch:    undocumented; modify at your own risks!<BR>
#       log_deprecated:    undocumented; modify at your own risks!<BR>
#       logName:    undocumented; modify at your own risks!<BR>
#<BR>
# Output:<BR>
#       D:     double m x p matrix   (dictionary)<BR>
#       model:  the model as A B iter<BR>
#        D = spams.trainDL_Memory(X,...)<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2009 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#<BR>
# Note:<BR>
#     this function admits a few experimental usages, which have not<BR>
#     been extensively tested:<BR>
#         - single precision setting (even though the output alpha is double <BR>
#           precision)<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
img_file = <FONT COLOR="red">'../extdata/lena.png'</FONT><BR>
<FONT COLOR="blue"><B>try</B></FONT>:<BR>
    img = Image.open(img_file)<BR>
<FONT COLOR="blue"><B>except</B></FONT>:<BR>
    <FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"Cannot load image %s : skipping test"</FONT> %img_file<BR>
I = np.array(img) / 255.<BR>
<FONT COLOR="blue"><B>if</B></FONT> I.ndim == 3:<BR>
    A = np.asfortranarray(I.reshape((I.shape[0],I.shape[1] * I.shape[2])))<BR>
    rgb = True<BR>
<FONT COLOR="blue"><B>else</B></FONT>:<BR>
    A = np.asfortranarray(I)<BR>
    rgb = False<BR>
<BR>
m = 8;n = 8;<BR>
X = spams.im2col_sliding(A,m,n,rgb)<BR>
<BR>
X = X - np.tile(np.mean(X,0),(X.shape[0],1))<BR>
X = np.asfortranarray(X / np.tile(np.sqrt((X * X).sum(axis=0)),(X.shape[0],1)))<BR>
X = np.asfortranarray(X[:,np.arange(0,X.shape[1],10)])<BR>
<BR>
param = { <FONT COLOR="red">'K'</FONT> : 200, <FONT COLOR="#007F00"># learns a dictionary with 100 elements</FONT><BR>
      <FONT COLOR="red">'lambda1'</FONT> : 0.15, <FONT COLOR="red">'numThreads'</FONT> : 4,<BR>
      <FONT COLOR="red">'iter'</FONT> : 100}<BR>
<BR>
<FONT COLOR="#007F00">############# FIRST EXPERIMENT  ##################</FONT><BR>
tic = time.time()<BR>
D = spams.trainDL_Memory(X,**param)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'time of computation for Dictionary Learning: %f'</FONT> %t<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'Evaluating cost function...'</FONT><BR>
lparam = _extract_lasso_param(param)<BR>
alpha = spams.lasso(X,D = D,**lparam)<BR>
xd = X - D * alpha<BR>
R = np.mean(0.5 * (xd * xd).sum(axis=0) + param[<FONT COLOR="red">'lambda1'</FONT>] * np.abs(alpha).sum(axis=0))<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"objective function: %f"</FONT> %R<BR>
<BR>
<FONT COLOR="#007F00">############# SECOND EXPERIMENT  ##################</FONT><BR>
tic = time.time()<BR>
D = spams.trainDL(X,**param)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'time of computation for Dictionary Learning: %f'</FONT> %t<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'Evaluating cost function...'</FONT><BR>
alpha = spams.lasso(X,D = D,**lparam)<BR>
xd = X - D * alpha<BR>
R = np.mean(0.5 * (xd * xd).sum(axis=0) + param[<FONT COLOR="red">'lambda1'</FONT>] * np.abs(alpha).sum(axis=0))<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"objective function: %f"</FONT> %R</TD></TR>
</TABLE><!--TOC subsection Function nmf-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc6">3.3</A>  Function nmf</H3><!--SEC END --><P>
This function is an example on how to use the function spams.trainDL for the
problem of non-negative matrix factorization formulated in [<A HREF="#lee2">16</A>]. Note
that spams.trainDL can be replaced by spams.trainDL_Memory in this function for
small or medium datasets.</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: nmf<BR>
#<BR>
# Usage: spams.nmf(X,return_lasso= False,model= None,numThreads = -1,batchsize = -1,K= -1,iter=-1,<BR>
#           t0=1e-5,clean=True,rho=1.0,modeParam=0,batch=False)<BR>
#<BR>
# Description:<BR>
#     trainDL is an efficient implementation of the<BR>
#     non-negative matrix factorization technique presented in <BR>
#     <BR>
#     "Online Learning for Matrix Factorization and Sparse Coding"<BR>
#     by Julien Mairal, Francis Bach, Jean Ponce and Guillermo Sapiro<BR>
#     arXiv:0908.0050<BR>
#     <BR>
#     "Online Dictionary Learning for Sparse Coding"      <BR>
#     by Julien Mairal, Francis Bach, Jean Ponce and Guillermo Sapiro<BR>
#     ICML 2009.<BR>
#     <BR>
#     Potentially, n can be very large with this algorithm.<BR>
#<BR>
# Inputs:<BR>
#       X:  double m x n matrix   (input signals)<BR>
#               m is the signal size<BR>
#               n is the number of signals to decompose<BR>
#       return_lasso:     <BR>
#                          if true the function will return a tuple of matrices.<BR>
#       K: (number of required factors)<BR>
#       iter: (number of iterations).  If a negative number <BR>
#         is provided it will perform the computation during the<BR>
#         corresponding number of seconds. For instance iter=-5<BR>
#         learns the dictionary during 5 seconds.<BR>
#       batchsize: (optional, size of the minibatch, by default <BR>
#          512)<BR>
#       modeParam: (optimization mode).<BR>
#          1) if modeParam=0, the optimization uses the <BR>
#             parameter free strategy of the ICML paper<BR>
#          2) if modeParam=1, the optimization uses the <BR>
#             parameters rho as in arXiv:0908.0050<BR>
#          3) if modeParam=2, the optimization uses exponential <BR>
#             decay weights with updates of the form  <BR>
#             A_{t} &lt;- rho A_{t-1} + alpha_t alpha_t^T<BR>
#       rho: (optional) tuning parameter (see paper <BR>
#       arXiv:0908.0050)<BR>
#       t0: (optional) tuning parameter (see paper <BR>
#       arXiv:0908.0050)<BR>
#       clean: (optional, true by default. prunes automatically <BR>
#         the dictionary from unused elements).<BR>
#       batch: (optional, false by default, use batch learning <BR>
#         instead of online learning)<BR>
#       numThreads: (optional, number of threads for exploiting<BR>
#            multi-core / multi-cpus. By default, it takes the value -1,<BR>
#            which automatically selects all the available CPUs/cores).<BR>
#       model: struct (optional) learned model for "retraining" the data.<BR>
#<BR>
# Output:<BR>
#       U: double m x p matrix   <BR>
#       V: double p x n matrix   (optional)<BR>
#       model: struct (optional) learned model to be used for <BR>
#         "retraining" the data.<BR>
#         U = spams.nmf(X,return_lasso = False,...)<BR>
#     (U,V) = spams.nmf(X,return_lasso = True,...)<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2009 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
img_file = <FONT COLOR="red">'../extdata/boat.png'</FONT><BR>
<FONT COLOR="blue"><B>try</B></FONT>:<BR>
    img = Image.open(img_file)<BR>
<FONT COLOR="blue"><B>except</B></FONT>:<BR>
    <FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"Cannot load image %s : skipping test"</FONT> %img_file<BR>
I = np.array(img) / 255.<BR>
<FONT COLOR="blue"><B>if</B></FONT> I.ndim == 3:<BR>
    A = np.asfortranarray(I.reshape((I.shape[0],I.shape[1] * I.shape[2])))<BR>
    rgb = True<BR>
<FONT COLOR="blue"><B>else</B></FONT>:<BR>
    A = np.asfortranarray(I)<BR>
    rgb = False<BR>
<BR>
m = 16;n = 16;<BR>
X = spams.im2col_sliding(A,m,n,rgb)<BR>
X = X[:,::10]<BR>
X = np.asfortranarray(X / np.tile(np.sqrt((X * X).sum(axis=0)),(X.shape[0],1)))<BR>
<FONT COLOR="#007F00">########## FIRST EXPERIMENT ###########</FONT><BR>
tic = time.time()<BR>
(U,V) = spams.nmf(X,return_lasso= True,K = 49,numThreads=4,iter = -5)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'time of computation for Dictionary Learning: %f'</FONT> %t<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'Evaluating cost function...'</FONT><BR>
Y = X - U * V<BR>
R = np.mean(0.5 * (Y * Y).sum(axis=0))<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'objective function: %f'</FONT> %R</TD></TR>
</TABLE><!--TOC subsection Function nnsc-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc7">3.4</A>  Function nnsc</H3><!--SEC END --><P>
This function is an example on how to use the function spams.trainDL for the
problem of non-negative sparse coding as defined in [<A HREF="#hoyer">13</A>]. Note that
spams.trainDL can be replaced by spams.trainDL_Memory in this function for small or
medium datasets.</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: nmf<BR>
#<BR>
# Usage: spams.nnsc(X,return_lasso= False,model= None,lambda1= None,numThreads = -1,batchsize = -1,<BR>
#            K= -1,iter=-1,t0=1e-5,clean=True,rho=1.0,modeParam=0,batch=False)<BR>
#<BR>
# Description:<BR>
#     trainDL is an efficient implementation of the<BR>
#     non-negative sparse coding technique presented in <BR>
#     <BR>
#     "Online Learning for Matrix Factorization and Sparse Coding"<BR>
#     by Julien Mairal, Francis Bach, Jean Ponce and Guillermo Sapiro<BR>
#     arXiv:0908.0050<BR>
#     <BR>
#     "Online Dictionary Learning for Sparse Coding"      <BR>
#     by Julien Mairal, Francis Bach, Jean Ponce and Guillermo Sapiro<BR>
#     ICML 2009.<BR>
#     <BR>
#     Potentially, n can be very large with this algorithm.<BR>
#<BR>
# Inputs:<BR>
#       X:  double m x n matrix   (input signals)<BR>
#               m is the signal size<BR>
#               n is the number of signals to decompose<BR>
#       return_lasso:     <BR>
#                          if true the function will return a tuple of matrices.<BR>
#       K: (number of required factors)<BR>
#       lambda1: (parameter)<BR>
#       iter: (number of iterations).  If a negative number <BR>
#          is provided it will perform the computation during the<BR>
#          corresponding number of seconds. For instance iter=-5<BR>
#          learns the dictionary during 5 seconds.<BR>
#       batchsize: (optional, size of the minibatch, by default <BR>
#          512)<BR>
#       modeParam: (optimization mode).<BR>
#          1) if modeParam=0, the optimization uses the <BR>
#             parameter free strategy of the ICML paper<BR>
#          2) if modeParam=1, the optimization uses the <BR>
#             parameters rho as in arXiv:0908.0050<BR>
#          3) if modeParam=2, the optimization uses exponential <BR>
#             decay weights with updates of the form <BR>
#             A_{t} &lt;- rho A_{t-1} + alpha_t alpha_t^T<BR>
#       rho: (optional) tuning parameter (see paper<BR>
#       arXiv:0908.0050)<BR>
#       t0: (optional) tuning parameter (see paper <BR>
#       arXiv:0908.0050)<BR>
#       clean: (optional, true by default. prunes automatically <BR>
#          the dictionary from unused elements).<BR>
#       batch: (optional, false by default, use batch learning <BR>
#          instead of online learning)<BR>
#       numThreads: (optional, number of threads for exploiting<BR>
#          multi-core / multi-cpus. By default, it takes the value -1,<BR>
#          which automatically selects all the available CPUs/cores).<BR>
#       model: struct (optional) learned model for "retraining" the data.<BR>
#<BR>
# Output:<BR>
#       U: double m x p matrix   <BR>
#       V: double p x n matrix   (optional)<BR>
#       model: struct (optional) learned model to be used for <BR>
#          "retraining" the data.<BR>
#          U = spams.nnsc(X,return_lasso = False,...)<BR>
#      (U,V) = spams.nnsc(X,return_lasso = True,...)<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2009 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#</FONT></TD></TR>
</TABLE><!--TOC section Sparse Decomposition Toolbox-->
<H2 CLASS="section"><!--SEC ANCHOR --><A NAME="htoc8">4</A>  Sparse Decomposition Toolbox</H2><!--SEC END --><P>
This toolbox implements several algorithms for solving signal reconstruction problems. It is mostly adapted for solving a large number of small/medium scale problems, but can be also efficient sometimes with large scale ones.
</P><!--TOC subsection Function spams.omp-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc9">4.1</A>  Function spams.omp</H3><!--SEC END --><P>
This is a fast implementation of the Orthogonal Matching Pursuit algorithm (or forward selection) [<A HREF="#mallat4">24</A>, <A HREF="#weisberg">32</A>]. Given a matrix of signals <I><B>X</B></I>=[<I><B>x</B></I><SUP>1</SUP>,…,<I><B>x</B><SUP>n</SUP></I>] in ℝ<SUP><I>m</I> × <I>n</I></SUP> and a dictionary <I><B>D</B></I>=[<I><B>d</B></I><SUP>1</SUP>,…,<I><B>d</B><SUP>p</SUP></I>] in ℝ<SUP><I>m</I> × <I>p</I></SUP>, the algorithm computes a matrix <I><B>A</B></I>=[α<SUP>1</SUP>,…,α<I><SUP>n</SUP></I>] in ℝ<SUP><I>p</I> × <I>n</I></SUP>,
where for each column <I><B>x</B></I> of <I><B>X</B></I>, it returns a coefficient vector α which is an approximate solution of the following NP-hard problem
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">α ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||<I><B>x</B></I>−<I><B>D</B></I>α||<SUB>2</SUB><SUP>2</SUP>   s.t.   ||α||<SUB>0</SUB> ≤ <I>L</I>,
    (2)</TD></TR>
</TABLE><P>
or 
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">α ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell">  ||α||<SUB>0</SUB>   s.t.   ||<I><B>x</B></I>−<I><B>D</B></I>α||<SUB>2</SUB><SUP>2</SUP> ≤ ε,
    (3)</TD></TR>
</TABLE><P>
or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">α ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>x</B></I>−<I><B>D</B></I>α||<SUB>2</SUB><SUP>2</SUP> + λ ||α||<SUB>0</SUB>.
    (4)</TD></TR>
</TABLE><P>
For efficienty reasons, the method first computes the covariance matrix
<I><B>D</B><SUP>T</SUP><B>D</B></I>, then for each signal, it computes <I><B>D</B><SUP>T</SUP><B>x</B></I> and performs the
decomposition with a Cholesky-based algorithm (see [<A HREF="#cotter">6</A>] for instance).</P><P>Note that spams.omp can return the “greedy” regularization path if needed (see below):
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: omp<BR>
#<BR>
# Usage: spams.omp(X,D,L=None,eps= None,lambda1 = None,return_reg_path = False,numThreads = -1)<BR>
#<BR>
# Description:<BR>
#     omp is an efficient implementation of the<BR>
#     Orthogonal Matching Pursuit algorithm. It is optimized<BR>
#     for solving a large number of small or medium-sized <BR>
#     decomposition problem (and not for a single large one).<BR>
#     It first computes the Gram matrix D'D and then perform<BR>
#     a Cholesky-based OMP of the input signals in parallel.<BR>
#     X=[x^1,...,x^n] is a matrix of signals, and it returns<BR>
#     a matrix A=[alpha^1,...,alpha^n] of coefficients.<BR>
#     <BR>
#     it addresses for all columns x of X, <BR>
#         min_{alpha} ||alpha||_0  s.t  ||x-Dalpha||_2^2 &lt;= eps<BR>
#         or<BR>
#         min_{alpha} ||x-Dalpha||_2^2  s.t. ||alpha||_0 &lt;= L<BR>
#         or<BR>
#         min_{alpha} 0.5||x-Dalpha||_2^2 + lambda1||alpha||_0 <BR>
#<BR>
# Inputs:<BR>
#       X:  double m x n matrix   (input signals)<BR>
#            m is the signal size<BR>
#            n is the number of signals to decompose<BR>
#       D:  double m x p matrix   (dictionary)<BR>
#          p is the number of elements in the dictionary<BR>
#          All the columns of D should have unit-norm !<BR>
#       return_reg_path:     <BR>
#                          if true the function will return a tuple of matrices.<BR>
#       L: (optional, maximum number of elements in each decomposition, <BR>
#          min(m,p) by default)<BR>
#       eps: (optional, threshold on the squared l2-norm of the residual,<BR>
#          0 by default<BR>
#       lambda1: (optional, penalty parameter, 0 by default<BR>
#       numThreads: (optional, number of threads for exploiting<BR>
#       multi-core / multi-cpus. By default, it takes the value -1,<BR>
#       which automatically selects all the available CPUs/cores).<BR>
#<BR>
# Output:<BR>
#       A: double sparse p x n matrix (output coefficients)<BR>
#         path (optional): double dense p x L matrix (regularization path of the first signal)<BR>
#         A = spams.omp(X,D,L,eps,return_reg_path = False,...)<BR>
#     (A,path) = spams.omp(X,D,L,eps,return_reg_path = True,...)<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2009 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#<BR>
# Note:<BR>
#     this function admits a few experimental usages, which have not<BR>
#     been extensively tested:<BR>
#      - single precision setting (even though the output alpha is double <BR>
#        precision)<BR>
#      - Passing an int32 vector of length n to L provides<BR>
#        a different parameter L for each input signal x_i<BR>
#      - Passing a double vector of length n to eps and or lambda1 <BR>
#        provides a different parameter eps (or lambda1) for each input signal x_i<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
np.random.seed(0)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'test omp'</FONT><BR>
X = np.asfortranarray(np.random.normal(size=(64,100000)))<BR>
D = np.asfortranarray(np.random.normal(size=(64,200)))<BR>
D = np.asfortranarray(D / np.tile(np.sqrt((D*D).sum(axis=0)),(D.shape[0],1)))<BR>
L = 10<BR>
eps = 0.1<BR>
numThreads = -1<BR>
tic = time.time()<BR>
alpha = spams.omp(X,D,L=L,eps= eps,return_reg_path = False,numThreads = numThreads)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"%f signals processed per second\n"</FONT> %(float(X.shape[1]) / t)<BR>
<FONT COLOR="#007F00">########################################<BR>
# Regularization path of a single signal <BR>
########################################</FONT><BR>
X = np.asfortranarray(np.random.normal(size=(64,1)))<BR>
D = np.asfortranarray(np.random.normal(size=(64,10)))<BR>
D = np.asfortranarray(D / np.tile(np.sqrt((D*D).sum(axis=0)),(D.shape[0],1)))<BR>
L = 5<BR>
(alpha,path) = spams.omp(X,D,L=L,eps= eps,return_reg_path = True,numThreads = numThreads)</TD></TR>
</TABLE><!--TOC subsection Function spams.ompMask-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc10">4.2</A>  Function spams.ompMask</H3><!--SEC END --><P>
This is a variant of spams.omp with the possibility of handling a binary mask. 
Given a binary mask <I><B>B</B></I>=[β<SUP>1</SUP>,…,β<I><SUP>n</SUP></I>] in {0,1}<SUP><I>m</I> × <I>n</I></SUP>, it returns a matrix <I><B>A</B></I>=[α<SUP>1</SUP>,…,α<I><SUP>n</SUP></I>] such that for every column <I><B>x</B></I> of <I><B>X</B></I>, β of <I><B>B</B></I>, it computes a column α of <I><B>A</B></I> by addressing
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">α ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||diag(β)(<I><B>x</B></I>−<I><B>D</B></I>α)||<SUB>2</SUB><SUP>2</SUP>   s.t.   ||α||<SUB>0</SUB> ≤ <I>L</I>,
    (5)</TD></TR>
</TABLE><P>
or 
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">α ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell">  ||α||<SUB>0</SUB>   s.t.   ||diag(β)(<I><B>x</B></I>−<I><B>D</B></I>α)||<SUB>2</SUB><SUP>2</SUP> ≤ ε</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">||β||<SUB>0</SUB></TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>m</I></TD></TR>
</TABLE></TD><TD CLASS="dcell">,
    (6)</TD></TR>
</TABLE><P>
or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">α ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||diag(β)(<I><B>x</B></I>−<I><B>D</B></I>α)||<SUB>2</SUB><SUP>2</SUP> +λ||α||<SUB>0</SUB>.
    (7)</TD></TR>
</TABLE><P>
where diag(β) is a diagonal matrix with the entries of β on the diagonal.</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: ompMask<BR>
#<BR>
# Usage: spams.ompMask(X,D,B,L=None,eps= None,lambda1 = None,return_reg_path = False,<BR>
#               numThreads = -1)<BR>
#<BR>
# Description:<BR>
#     ompMask is a variant of mexOMP that allow using<BR>
#     a binary mask B<BR>
#     <BR>
#     for all columns x of X, and columns beta of B, it computes a column <BR>
#         alpha of A by addressing<BR>
#         min_{alpha} ||alpha||_0  s.t  ||diag(beta)*(x-Dalpha)||_2^2 <BR>
#                                                               &lt;= eps*||beta||_0/m<BR>
#         or<BR>
#         min_{alpha} ||diag(beta)*(x-Dalpha)||_2^2  s.t. ||alpha||_0 &lt;= L<BR>
#         or<BR>
#         min_{alpha} 0.5||diag(beta)*(x-Dalpha)||_2^2  + lambda1||alpha||_0<BR>
#<BR>
# Inputs:<BR>
#       X:  double m x n matrix   (input signals)<BR>
#            m is the signal size<BR>
#            n is the number of signals to decompose<BR>
#       D:  double m x p matrix   (dictionary)<BR>
#          p is the number of elements in the dictionary<BR>
#          All the columns of D should have unit-norm !<BR>
#       B:  boolean m x n matrix   (mask)<BR>
#             p is the number of elements in the dictionary<BR>
#       return_reg_path:     <BR>
#                          if true the function will return a tuple of matrices.<BR>
#       L: (optional, maximum number of elements in each decomposition, <BR>
#          min(m,p) by default)<BR>
#       eps: (optional, threshold on the squared l2-norm of the residual,<BR>
#          0 by default<BR>
#       lambda1: (optional, penalty parameter, 0 by default<BR>
#       numThreads: (optional, number of threads for exploiting<BR>
#       multi-core / multi-cpus. By default, it takes the value -1,<BR>
#       which automatically selects all the available CPUs/cores).<BR>
#<BR>
# Output:<BR>
#       A: double sparse p x n matrix (output coefficients)<BR>
#         path (optional): double dense p x L matrix <BR>
#                                     (regularization path of the first signal)<BR>
#                                     A = spams.ompMask(X,D,B,L,eps,return_reg_path = False,...)<BR>
#                                 (A,path) = spams.ompMask(X,D,B,L,eps,return_reg_path = True,...)<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2010 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#<BR>
# Note:<BR>
#     this function admits a few experimental usages, which have not<BR>
#     been extensively tested:<BR>
#      - single precision setting (even though the output alpha is double <BR>
#        precision)<BR>
#      - Passing an int32 vector of length n to L provides<BR>
#        a different parameter L for each input signal x_i<BR>
#      - Passing a double vector of length n to eps and or lambda1 <BR>
#        provides a different parameter eps (or lambda1) for each input signal x_i<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
np.random.seed(0)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'test ompMask'</FONT><BR>
<BR>
<FONT COLOR="#007F00">########################################    <BR>
# Decomposition of a large number of signals<BR>
########################################    </FONT><BR>
X = np.asfortranarray(np.random.normal(size=(300,300)))<BR>
X = np.asfortranarray(X / np.tile(np.sqrt((X*X).sum(axis=0)),(X.shape[0],1)))<BR>
D = np.asfortranarray(np.random.normal(size=(300,50)))<BR>
D = np.asfortranarray(D / np.tile(np.sqrt((D*D).sum(axis=0)),(D.shape[0],1)))<BR>
mask = np.asfortranarray((X &gt; 0))  <FONT COLOR="#007F00"># generating a binary mask</FONT><BR>
L = 20<BR>
eps = 0.1<BR>
numThreads=-1<BR>
tic = time.time()<BR>
alpha = spams.ompMask(X,D,mask,L = L,eps = eps,return_reg_path = False,numThreads = numThreads)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"%f signals processed per second\n"</FONT> %(float(X.shape[1]) / t)</TD></TR>
</TABLE><!--TOC subsection Function spams.lasso-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc11">4.3</A>  Function spams.lasso</H3><!--SEC END --><P>
This is a fast implementation of the LARS algorithm [<A HREF="#efron">8</A>] (variant for solving the Lasso) for solving the Lasso or Elastic-Net. Given a matrix of signals <I><B>X</B></I>=[<I><B>x</B></I><SUP>1</SUP>,…,<I><B>x</B><SUP>n</SUP></I>] in ℝ<SUP><I>m</I> × <I>n</I></SUP> and a dictionary <I><B>D</B></I> in ℝ<SUP><I>m</I> × <I>p</I></SUP>, depending on the input parameters, the algorithm returns a matrix of coefficients <I><B>A</B></I>=[α<SUP>1</SUP>,…,α<I><SUP>n</SUP></I>] in ℝ<SUP><I>p</I> × <I>n</I></SUP> such that for every column <I><B>x</B></I> of <I><B>X</B></I>, the corresponding column α of <I><B>A</B></I> is the solution of
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">α ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||<I><B>x</B></I>−<I><B>D</B></I>α||<SUB>2</SUB><SUP>2</SUP>   s.t.   ||α||<SUB>1</SUB> ≤ λ,
    (8)</TD></TR>
</TABLE><P>
or 
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">α ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell">  ||α||<SUB>1</SUB>   s.t.   ||<I><B>x</B></I>−<I><B>D</B></I>α||<SUB>2</SUB><SUP>2</SUP> ≤ λ, <A NAME="eq:lasso2"></A>
    (9)</TD></TR>
</TABLE><P>
or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">α ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>x</B></I>−<I><B>D</B></I>α||<SUB>2</SUB><SUP>2</SUP> + λ ||α||<SUB>1</SUB> + </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">λ<SUB>2</SUB></TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||α||<SUB>2</SUB><SUP>2</SUP>. <A NAME="eq:lasso"></A>
    (10)</TD></TR>
</TABLE><P>
For efficiency reasons, the method first compute the covariance matrix <I><B>D</B><SUP>T</SUP><B>D</B></I>, then
for each signal, it computes <I><B>D</B><SUP>T</SUP><B>x</B></I> and performs the decomposition with a
Cholesky-based algorithm (see [<A HREF="#efron">8</A>] for instance). The implementation
has also an option to add <B>positivity constraints</B> on the solutions
α. When the solution is very sparse and the problem size is
reasonable, this approach can be very efficient. Moreover, it gives the
solution with an exact precision, and its performance does not depend on the
correlation of the dictionary elements, except when the solution is not unique
(the algorithm breaks in this case).</P><P>Note that spams.lasso can return the whole regularization path of the first signal <I><B>x</B></I><SUB>1</SUB> 
and can handle implicitely the matrix <I><B>D</B></I> if the quantities <I><B>D</B><SUP>T</SUP><B>D</B></I> and <I><B>D</B><SUP>T</SUP><B>x</B></I> are passed
as an argument, see below:</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: lasso<BR>
#<BR>
# Usage: spams.lasso(X,D= None,Q = None,q = None,return_reg_path = False,L= -1,lambda1= None,<BR>
#             lambda2= 0.,mode= spams_wrap.PENALTY,pos= False,ols= False,numThreads= -1,<BR>
#             max_length_path= -1,verbose=False,cholesky= False)<BR>
#<BR>
# Description:<BR>
#     lasso is an efficient implementation of the<BR>
#     homotopy-LARS algorithm for solving the Lasso. <BR>
#     <BR>
#     If the function is called this way spams.lasso(X,D = D, Q = None,...),<BR>
#     it aims at addressing the following problems<BR>
#     for all columns x of X, it computes one column alpha of A<BR>
#     that solves<BR>
#       1) when mode=0<BR>
#         min_{alpha} ||x-Dalpha||_2^2 s.t. ||alpha||_1 &lt;= lambda1<BR>
#       2) when mode=1<BR>
#         min_{alpha} ||alpha||_1 s.t. ||x-Dalpha||_2^2 &lt;= lambda1<BR>
#       3) when mode=2<BR>
#         min_{alpha} 0.5||x-Dalpha||_2^2 + lambda1||alpha||_1 +0.5 lambda2||alpha||_2^2<BR>
#         <BR>
#     If the function is called this way spams.lasso(X,D = None, Q = Q, q = q,...),<BR>
#     it solves the above optimisation problem, when Q=D'D and q=D'x.<BR>
#     <BR>
#     Possibly, when pos=true, it solves the previous problems<BR>
#     with positivity constraints on the vectors alpha<BR>
#<BR>
# Inputs:<BR>
#       X:  double m x n matrix   (input signals)<BR>
#               m is the signal size<BR>
#               n is the number of signals to decompose<BR>
#       D:  double m x p matrix   (dictionary)<BR>
#             p is the number of elements in the dictionary<BR>
#       Q:             p x p double matrix (Q = D'D)<BR>
#       q:             p x n double matrix (q = D'X)<BR>
#       verbose:       verbose mode<BR>
#       return_reg_path:     <BR>
#                          if true the function will return a tuple of matrices.<BR>
#       lambda1:  (parameter)<BR>
#       lambda2:  (optional parameter for solving the Elastic-Net)<BR>
#                      for mode=0 and mode=1, it adds a ridge on the Gram Matrix<BR>
#       L: (optional), maximum number of steps of the homotopy algorithm (can<BR>
#                be used as a stopping criterion)<BR>
#       pos: (optional, adds non-negativity constraints on the<BR>
#         coefficients, false by default)<BR>
#       mode: (see above, by default: 2)<BR>
#       numThreads: (optional, number of threads for exploiting<BR>
#         multi-core / multi-cpus. By default, it takes the value -1,<BR>
#         which automatically selects all the available CPUs/cores).<BR>
#       cholesky: (optional, default false),  choose between Cholesky <BR>
#         implementation or one based on the matrix inversion Lemma<BR>
#       ols: (optional, default false), perform an orthogonal projection<BR>
#         before returning the solution.<BR>
#       max_length_path: (optional) maximum length of the path, by default 4*p<BR>
#<BR>
# Output:<BR>
#       A: double sparse p x n matrix (output coefficients)<BR>
#       path: optional,  returns the regularisation path for the first signal<BR>
#       A = spams.lasso(X,return_reg_path = False,...)<BR>
#   (A,path) = spams.lasso(X,return_reg_path = True,...)<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2009 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#<BR>
# Note:<BR>
#     this function admits a few experimental usages, which have not<BR>
#     been extensively tested:<BR>
#         - single precision setting (even though the output alpha is double <BR>
#           precision)<BR>
#<BR>
# Examples:<BR>
#       import numpy as np<BR>
#       m = 5;n = 10;nD = 5<BR>
#       np.random.seed(0)<BR>
#       X = np.asfortranarray(np.random.normal(size=(m,n)))<BR>
#       X = np.asfortranarray(X / np.tile(np.sqrt((X*X).sum(axis=0)),(X.shape[0],1)))<BR>
#       D = np.asfortranarray(np.random.normal(size=(100,200)))<BR>
#       D = np.asfortranarray(D / np.tile(np.sqrt((D*D).sum(axis=0)),(D.shape[0],1)))<BR>
#       alpha = spams.lasso(X,D = D,return_reg_path = FALSE,lambda1 = 0.15)<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
np.random.seed(0)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"test lasso"</FONT><BR>
<FONT COLOR="#007F00">##############################################<BR>
# Decomposition of a large number of signals<BR>
##############################################<BR>
# data generation</FONT><BR>
X = np.asfortranarray(np.random.normal(size=(100,100000)))<BR>
X = np.asfortranarray(X / np.tile(np.sqrt((X*X).sum(axis=0)),(X.shape[0],1)))<BR>
D = np.asfortranarray(np.random.normal(size=(100,200)))<BR>
D = np.asfortranarray(D / np.tile(np.sqrt((D*D).sum(axis=0)),(D.shape[0],1)))<BR>
<FONT COLOR="#007F00"># parameter of the optimization procedure are chosen<BR>
#param.L=20; # not more than 20 non-zeros coefficients (default: min(size(D,1),size(D,2)))</FONT><BR>
param = {<BR>
    <FONT COLOR="red">'lambda1'</FONT> : 0.15, <FONT COLOR="#007F00"># not more than 20 non-zeros coefficients</FONT><BR>
    <FONT COLOR="red">'numThreads'</FONT> : -1, <FONT COLOR="#007F00"># number of processors/cores to use; the default choice is -1</FONT><BR>
    <FONT COLOR="#007F00"># and uses all the cores of the machine</FONT><BR>
    <FONT COLOR="red">'mode'</FONT> : spams.PENALTY}        <FONT COLOR="#007F00"># penalized formulation</FONT><BR>
<BR>
tic = time.time()<BR>
alpha = spams.lasso(X,D = D,return_reg_path = False,**param)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"%f signals processed per second\n"</FONT> %(float(X.shape[1]) / t)<BR>
<FONT COLOR="#007F00">########################################<BR>
# Regularization path of a single signal <BR>
########################################</FONT><BR>
X = np.asfortranarray(np.random.normal(size=(64,1)))<BR>
D = np.asfortranarray(np.random.normal(size=(64,10)))<BR>
D = np.asfortranarray(D / np.tile(np.sqrt((D*D).sum(axis=0)),(D.shape[0],1)))<BR>
(alpha,path) = spams.lasso(X,D = D,return_reg_path = True,**param)</TD></TR>
</TABLE><!--TOC subsection Function spams.lassoWeighted-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc12">4.4</A>  Function spams.lassoWeighted</H3><!--SEC END --><P>
This is a fast implementation of a weighted version of LARS [<A HREF="#efron">8</A>]. Given a matrix of signals <I><B>X</B></I>=[<I><B>x</B></I><SUP>1</SUP>,…,<I><B>x</B><SUP>n</SUP></I>] in ℝ<SUP><I>m</I> × <I>n</I></SUP>, a matrix of weights <I><B>W</B></I>=[<I><B>w</B></I><SUP>1</SUP>,…,<I><B>w</B><SUP>n</SUP></I>] ∈ ℝ<SUP><I>p</I> × <I>n</I></SUP>, and a dictionary <I><B>D</B></I> in ℝ<SUP><I>m</I> × <I>p</I></SUP>, depending on the input parameters, the algorithm returns a matrix of coefficients <I><B>A</B></I>=[α<SUP>1</SUP>,…,α<I><SUP>n</SUP></I>] in ℝ<SUP><I>p</I> × <I>n</I></SUP>,
such that for every column <I><B>x</B></I> of <I><B>X</B></I>, <I><B>w</B></I> of <I><B>W</B></I>, it computes a column α of <I><B>A</B></I>, which is the solution of
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">α ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||<I><B>x</B></I>−<I><B>D</B></I>α||<SUB>2</SUB><SUP>2</SUP>   s.t.   ||diag(<I><B>w</B></I>)α||<SUB>1</SUB> ≤ λ,
    (11)</TD></TR>
</TABLE><P>
or 
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">α ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell">  ||diag(<I><B>w</B></I>)α||<SUB>1</SUB>   s.t.   ||<I><B>x</B></I>−<I><B>D</B></I>α||<SUB>2</SUB><SUP>2</SUP> ≤ λ,
    (12)</TD></TR>
</TABLE><P>
or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">α ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>x</B></I>−<I><B>D</B></I>α||<SUB>2</SUB><SUP>2</SUP> + λ ||diag(<I><B>w</B></I>)α||<SUB>1</SUB>.
    (13)</TD></TR>
</TABLE><P>
The implementation has also an option to add <B>positivity constraints</B> on
the solutions α. This function is potentially useful for
implementing efficiently the randomized Lasso of [<A HREF="#meinshausen">25</A>], or reweighted-ℓ<SUB>1</SUB> schemes [<A HREF="#candes4">4</A>].</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: lassoWeighted.  <BR>
#<BR>
# Usage: spams.lassoWeighted(X,D,W,L= -1,lambda1= None,mode= spams_wrap.PENALTY,pos= False,<BR>
#                     numThreads= -1,verbose = False)<BR>
#<BR>
# Description:<BR>
#     lassoWeighted is an efficient implementation of the<BR>
#     LARS algorithm for solving the weighted Lasso. It is optimized<BR>
#     for solving a large number of small or medium-sized <BR>
#     decomposition problem (and not for a single large one).<BR>
#     It first computes the Gram matrix D'D and then perform<BR>
#     a Cholesky-based OMP of the input signals in parallel.<BR>
#     For all columns x of X, and w of W, it computes one column alpha of A<BR>
#     which is the solution of<BR>
#       1) when mode=0<BR>
#         min_{alpha} ||x-Dalpha||_2^2   s.t.  <BR>
#                                     ||diag(w)alpha||_1 &lt;= lambda1<BR>
#       2) when mode=1<BR>
#         min_{alpha} ||diag(w)alpha||_1  s.t.<BR>
#                                        ||x-Dalpha||_2^2 &lt;= lambda1<BR>
#       3) when mode=2<BR>
#         min_{alpha} 0.5||x-Dalpha||_2^2  +  <BR>
#                                         lambda1||diag(w)alpha||_1 <BR>
#     Possibly, when pos=true, it solves the previous problems<BR>
#     with positivity constraints on the vectors alpha<BR>
#<BR>
# Inputs:<BR>
#       X:  double m x n matrix   (input signals)<BR>
#               m is the signal size<BR>
#               n is the number of signals to decompose<BR>
#       D:  double m x p matrix   (dictionary)<BR>
#             p is the number of elements in the dictionary<BR>
#       W:  double p x n matrix   (weights)<BR>
#       verbose:       verbose mode<BR>
#       lambda1:  (parameter)<BR>
#       L: (optional, maximum number of elements of each <BR>
#       decomposition)<BR>
#       pos: (optional, adds positivity constraints on the<BR>
#       coefficients, false by default)<BR>
#       mode: (see above, by default: 2)<BR>
#       numThreads: (optional, number of threads for exploiting<BR>
#       multi-core / multi-cpus. By default, it takes the value -1,<BR>
#       which automatically selects all the available CPUs/cores).<BR>
#<BR>
# Output:<BR>
#       A: double sparse p x n matrix (output coefficients)<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2009 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#<BR>
# Note:<BR>
#     this function admits a few experimental usages, which have not<BR>
#     been extensively tested:<BR>
#         - single precision setting (even though the output alpha is double <BR>
#           precision)<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
np.random.seed(0)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"test lasso weighted"</FONT><BR>
<FONT COLOR="#007F00">##############################################<BR>
# Decomposition of a large number of signals<BR>
##############################################<BR>
# data generation</FONT><BR>
X = np.asfortranarray(np.random.normal(size=(64,10000)))<BR>
X = np.asfortranarray(X / np.tile(np.sqrt((X*X).sum(axis=0)),(X.shape[0],1)))<BR>
D = np.asfortranarray(np.random.normal(size=(64,256)))<BR>
D = np.asfortranarray(D / np.tile(np.sqrt((D*D).sum(axis=0)),(D.shape[0],1)))<BR>
param = { <FONT COLOR="red">'L'</FONT> : 20,<BR>
    <FONT COLOR="red">'lambda1'</FONT> : 0.15, <FONT COLOR="red">'numThreads'</FONT> : 8, <FONT COLOR="red">'mode'</FONT> : spams.PENALTY}<BR>
W = np.asfortranarray(np.random.random(size = (D.shape[1],X.shape[1])))<BR>
tic = time.time()<BR>
alpha = spams.lassoWeighted(X,D,W,**param)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"%f signals processed per second\n"</FONT> %(float(X.shape[1]) / t)</TD></TR>
</TABLE><!--TOC subsection Function spams.lassoMask-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc13">4.5</A>  Function spams.lassoMask</H3><!--SEC END --><P>
This is a variant of spams.lasso with the possibility of adding a mask <I><B>B</B></I>=[β<SUP>1</SUP>,…,β<I><SUP>n</SUP></I>], as in mexOMPMask. 
For every column <I><B>x</B></I> of <I><B>X</B></I>, β of <I><B>B</B></I>, it computes a column α of <I><B>A</B></I>, which is the solution of
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">α ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||diag(β)(<I><B>x</B></I>−<I><B>D</B></I>α)||<SUB>2</SUB><SUP>2</SUP>   s.t.   ||α||<SUB>1</SUB> ≤ λ,
    (14)</TD></TR>
</TABLE><P>
or 
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">α ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell">  ||α||<SUB>1</SUB>   s.t.   ||diag(β)(<I><B>x</B></I>−<I><B>D</B></I>α)||<SUB>2</SUB><SUP>2</SUP> ≤ λ</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">||β||<SUB>0</SUB></TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>m</I></TD></TR>
</TABLE></TD><TD CLASS="dcell">, 
    (15)</TD></TR>
</TABLE><P>
or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">α ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||diag(β)(<I><B>x</B></I>−<I><B>D</B></I>α)||<SUB>2</SUB><SUP>2</SUP> + λ </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">||β||<SUB>0</SUB></TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>m</I></TD></TR>
</TABLE></TD><TD CLASS="dcell">||α||<SUB>1</SUB> + </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">λ<SUB>2</SUB></TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||α||<SUB>2</SUB><SUP>2</SUP>. 
    (16)</TD></TR>
</TABLE><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: lassoMask<BR>
#<BR>
# Usage: spams.lassoMask(X,D,B,L= -1,lambda1= None,lambda2= 0.,mode= spams_wrap.PENALTY,pos= False,<BR>
#                 numThreads= -1,verbose = False)<BR>
#<BR>
# Description:<BR>
#     lasso is a variant of lasso that handles<BR>
#     binary masks. It aims at addressing the following problems<BR>
#     for all columns x of X, and beta of B, it computes one column alpha of A<BR>
#     that solves<BR>
#       1) when mode=0<BR>
#         min_{alpha} ||diag(beta)(x-Dalpha)||_2^2 s.t. ||alpha||_1 &lt;= lambda1<BR>
#       2) when mode=1<BR>
#         min_{alpha} ||alpha||_1 s.t. ||diag(beta)(x-Dalpha)||_2^2 <BR>
#                                                              &lt;= lambda1*||beta||_0/m<BR>
#       3) when mode=2<BR>
#         min_{alpha} 0.5||diag(beta)(x-Dalpha)||_2^2 +<BR>
#                                                 lambda1*(||beta||_0/m)*||alpha||_1 +<BR>
#                                                 (lambda2/2)||alpha||_2^2<BR>
#     Possibly, when pos=true, it solves the previous problems<BR>
#     with positivity constraints on the vectors alpha<BR>
#<BR>
# Inputs:<BR>
#       X:  double m x n matrix   (input signals)<BR>
#               m is the signal size<BR>
#               n is the number of signals to decompose<BR>
#       D:  double m x p matrix   (dictionary)<BR>
#             p is the number of elements in the dictionary<BR>
#       B:  boolean m x n matrix   (mask)<BR>
#             p is the number of elements in the dictionary<BR>
#       verbose:       verbose mode<BR>
#       lambda1:  (parameter)<BR>
#       L: (optional, maximum number of elements of each <BR>
#         decomposition)<BR>
#       pos: (optional, adds positivity constraints on the<BR>
#         coefficients, false by default)<BR>
#       mode: (see above, by default: 2)<BR>
#       lambda2:  (optional parameter for solving the Elastic-Net)<BR>
#                      for mode=0 and mode=1, it adds a ridge on the Gram Matrix<BR>
#       numThreads: (optional, number of threads for exploiting<BR>
#         multi-core / multi-cpus. By default, it takes the value -1,<BR>
#         which automatically selects all the available CPUs/cores).<BR>
#<BR>
# Output:<BR>
#       A: double sparse p x n matrix (output coefficients)<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2010 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#<BR>
# Note:<BR>
#     this function admits a few experimental usages, which have not<BR>
#     been extensively tested:<BR>
#         - single precision setting (even though the output alpha is double <BR>
#           precision)<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
np.random.seed(0)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"test lassoMask"</FONT><BR>
<FONT COLOR="#007F00">##############################################<BR>
# Decomposition of a large number of signals<BR>
##############################################<BR>
# data generation</FONT><BR>
X = np.asfortranarray(np.random.normal(size=(300,300)))<BR>
<FONT COLOR="#007F00"># X=X./repmat(sqrt(sum(X.^2)),[size(X,1) 1]);</FONT><BR>
X = np.asfortranarray(X / np.tile(np.sqrt((X*X).sum(axis=0)),(X.shape[0],1)))<BR>
D = np.asfortranarray(np.random.normal(size=(300,50)))<BR>
D = np.asfortranarray(D / np.tile(np.sqrt((D*D).sum(axis=0)),(D.shape[0],1)))<BR>
mask = np.asfortranarray((X &gt; 0))  <FONT COLOR="#007F00"># generating a binary mask</FONT><BR>
param = {<BR>
    <FONT COLOR="red">'lambda1'</FONT> : 0.15, <FONT COLOR="#007F00"># not more than 20 non-zeros coefficients</FONT><BR>
    <FONT COLOR="red">'numThreads'</FONT> : -1, <FONT COLOR="#007F00"># number of processors/cores to use; the default choice is -1</FONT><BR>
    <FONT COLOR="#007F00"># and uses all the cores of the machine</FONT><BR>
    <FONT COLOR="red">'mode'</FONT> : spams.PENALTY}        <FONT COLOR="#007F00"># penalized formulation</FONT><BR>
tic = time.time()<BR>
alpha = spams.lassoMask(X,D,mask,**param)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"%f signals processed per second\n"</FONT> %(float(X.shape[1]) / t)</TD></TR>
</TABLE><!--TOC subsection Function spams.cd-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc14">4.6</A>  Function spams.cd</H3><!--SEC END --><P>
Coordinate-descent approach for solving Eq. (<A HREF="#eq:lasso">10</A>) and
Eq. (<A HREF="#eq:lasso2">9</A>). Note that unlike spams.lasso, it is not implemented to solve the Elastic-Net formulation.
To solve Eq. (<A HREF="#eq:lasso2">9</A>), the algorithm solves a
sequence of problems of the form (<A HREF="#eq:lasso">10</A>) using simple heuristics.
Coordinate descent is very simple and in practice very powerful. It performs
better when the correlation between the dictionary elements is small. </P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: cd<BR>
#<BR>
# Usage: spams.cd(X,D,A0,lambda1 = None,mode= spams_wrap.PENALTY,itermax=100,tol = 0.001,<BR>
#          numThreads =-1)<BR>
#<BR>
# Description:<BR>
#     cd addresses l1-decomposition problem with a <BR>
#     coordinate descent type of approach.<BR>
#     It is optimized for solving a large number of small or medium-sized <BR>
#     decomposition problem (and not for a single large one).<BR>
#     It first computes the Gram matrix D'D.<BR>
#     This method is particularly well adapted when there is low <BR>
#     correlation between the dictionary elements and when one can benefit <BR>
#     from a warm restart.<BR>
#     It aims at addressing the two following problems<BR>
#     for all columns x of X, it computes a column alpha of A such that<BR>
#       2) when mode=1<BR>
#         min_{alpha} ||alpha||_1 s.t. ||x-Dalpha||_2^2 &lt;= lambda1<BR>
#         For this constraint setting, the method solves a sequence of <BR>
#         penalized problems (corresponding to mode=2) and looks<BR>
#         for the corresponding Lagrange multplier with a simple but<BR>
#         efficient heuristic.<BR>
#       3) when mode=2<BR>
#         min_{alpha} 0.5||x-Dalpha||_2^2 + lambda1||alpha||_1 <BR>
#<BR>
# Inputs:<BR>
#       X:  double m x n matrix   (input signals)<BR>
#               m is the signal size<BR>
#               n is the number of signals to decompose<BR>
#       D:  double m x p matrix   (dictionary)<BR>
#             p is the number of elements in the dictionary<BR>
#             All the columns of D should have unit-norm !<BR>
#       A0:  double sparse p x n matrix   (initial guess)<BR>
#       lambda1:  (parameter)<BR>
#       mode: (optional, see above, by default 2)<BR>
#       itermax: (maximum number of iterations)<BR>
#       tol: (tolerance parameter)<BR>
#       numThreads: (optional, number of threads for exploiting<BR>
#       multi-core / multi-cpus. By default, it takes the value -1,<BR>
#       which automatically selects all the available CPUs/cores).<BR>
#<BR>
# Output:<BR>
#       A: double sparse p x n matrix (output coefficients)<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2009 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#<BR>
# Note:<BR>
#     this function admits a few experimental usages, which have not<BR>
#     been extensively tested:<BR>
#         - single precision setting (even though the output alpha <BR>
#           is double precision)<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
np.random.seed(0)<BR>
X = np.asfortranarray(np.random.normal(size = (64,100)))<BR>
X = np.asfortranarray(X / np.tile(np.sqrt((X*X).sum(axis=0)),(X.shape[0],1)))<BR>
D = np.asfortranarray(np.random.normal(size = (64,100)))<BR>
D = np.asfortranarray(D / np.tile(np.sqrt((D*D).sum(axis=0)),(D.shape[0],1)))<BR>
<FONT COLOR="#007F00"># parameter of the optimization procedure are chosen</FONT><BR>
lambda1 = 0.015<BR>
mode = spams.PENALTY<BR>
tic = time.time()<BR>
alpha = spams.lasso(X,D,lambda1 = lambda1,mode = mode,numThreads = 4)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
xd = X - D * alpha<BR>
E = np.mean(0.5 * (xd * xd).sum(axis=0) + lambda1 * np.abs(alpha).sum(axis=0))<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"%f signals processed per second for LARS"</FONT> %(X.shape[1] / t)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'Objective function for LARS: %g'</FONT> %E<BR>
tol = 0.001<BR>
itermax = 1000<BR>
tic = time.time()<BR>
<FONT COLOR="#007F00">#    A0 = ssp.csc_matrix(np.empty((alpha.shape[0],alpha.shape[1])))</FONT><BR>
A0 = ssp.csc_matrix((alpha.shape[0],alpha.shape[1]))<BR>
alpha2 = spams.cd(X,D,A0,lambda1 = lambda1,mode = mode,tol = tol, itermax = itermax,numThreads = 4)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"%f signals processed per second for CD"</FONT> %(X.shape[1] / t)<BR>
xd = X - D * alpha2<BR>
E = np.mean(0.5 * (xd * xd).sum(axis=0) + lambda1 * np.abs(alpha).sum(axis=0))<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'Objective function for CD: %g'</FONT> %E<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'With Random Design, CD can be much faster than LARS'</FONT></TD></TR>
</TABLE><!--TOC subsection Function spams.somp-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc15">4.7</A>  Function spams.somp</H3><!--SEC END --><P>
This is a fast implementation of the Simultaneous Orthogonal Matching Pursuit algorithm. Given a set of matrices <I><B>X</B></I>=[<I><B>X</B></I><SUP>1</SUP>,…,<I><B>X</B><SUP>n</SUP></I>] in ℝ<SUP><I>m</I> × <I>N</I></SUP>, where the <I><B>X</B><SUP>i</SUP></I>’s are in ℝ<SUP><I>m</I> × <I>n<SUB>i</SUB></I></SUP>, and a dictionary <I><B>D</B></I> in ℝ<SUP><I>m</I> × <I>p</I></SUP>, the algorithm returns a matrix of coefficients <I><B>A</B></I>=[<I><B>A</B></I><SUP>1</SUP>,…,<I><B>A</B><SUP>n</SUP></I>] in ℝ<SUP><I>p</I> × <I>N</I></SUP> which is an approximate solution of the following NP-hard problem
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
∀ <I>i</I>   </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>A</B><SUP>i</SUP></I> ∈ ℝ<SUP><I>p</I> × <I>n<SUB>i</SUB></I></SUP></TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||<I><B>X</B><SUP>i</SUP></I>−<I><B>DA</B><SUP>i</SUP></I>||<I><SUB>F</SUB></I><SUP>2</SUP>   s.t.   ||<I><B>A</B><SUP>i</SUP></I>||<SUB>0,∞</SUB> ≤ <I>L</I>.
    (17)</TD></TR>
</TABLE><P>
or 
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
∀ <I>i</I>   </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>A</B><SUP>i</SUP></I> ∈ ℝ<SUP><I>p</I> × <I>n<SUB>i</SUB></I></SUP></TD></TR>
</TABLE></TD><TD CLASS="dcell">  ||<I><B>A</B><SUP>i</SUP></I>||<SUB>0,∞</SUB>   s.t.   ||<I><B>X</B><SUP>i</SUP></I>−<I><B>DA</B><SUP>i</SUP></I>||<I><SUB>F</SUB></I><SUP>2</SUP> ≤ ε <I>n<SUB>i</SUB></I>.
    (18)</TD></TR>
</TABLE><P>
To be efficient, the method first compute the covariance matrix <I><B>D</B><SUP>T</SUP><B>D</B></I>, then for each signal, it computes <I><B>D</B><SUP>T</SUP><B>X</B><SUP>i</SUP></I> and performs the decomposition with a Cholesky-based algorithm.</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: somp<BR>
# (this function has not been intensively tested).<BR>
#<BR>
# Usage: spams.somp(X,D,list_groups,L = None,eps = 0.,numThreads = -1)<BR>
#<BR>
# Description:<BR>
#     somp is an efficient implementation of a<BR>
#     Simultaneous Orthogonal Matching Pursuit algorithm. It is optimized<BR>
#     for solving a large number of small or medium-sized <BR>
#     decomposition problem (and not for a single large one).<BR>
#     It first computes the Gram matrix D'D and then perform<BR>
#     a Cholesky-based OMP of the input signals in parallel.<BR>
#     It aims at addressing the following NP-hard problem<BR>
#     <BR>
#     X is a matrix structured in groups of signals, which we denote<BR>
#     by X=[X_1,...,X_n]<BR>
#     <BR>
#     for all matrices X_i of X, <BR>
#         min_{A_i} ||A_i||_{0,infty}  s.t  ||X_i-D A_i||_2^2 &lt;= eps*n_i<BR>
#         where n_i is the number of columns of X_i<BR>
#         <BR>
#         or<BR>
#         <BR>
#         min_{A_i} ||X_i-D A_i||_2^2  s.t. ||A_i||_{0,infty} &lt;= L<BR>
#<BR>
# Inputs:<BR>
#       X:  double m x N matrix   (input signals)<BR>
#            m is the signal size<BR>
#            N is the total number of signals <BR>
#       D:  double m x p matrix   (dictionary)<BR>
#          p is the number of elements in the dictionary<BR>
#          All the columns of D should have unit-norm !<BR>
#       list_groups : int32 vector containing the indices (starting at 0)<BR>
#          of the first elements of each groups.<BR>
#       L: (maximum number of elements in each decomposition)<BR>
#       eps: (threshold on the squared l2-norm of the residual<BR>
#       numThreads: (optional, number of threads for exploiting<BR>
#       multi-core / multi-cpus. By default, it takes the value -1,<BR>
#       which automatically selects all the available CPUs/cores).<BR>
#<BR>
# Output:<BR>
#       alpha: double sparse p x N matrix (output coefficients)<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2010 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#<BR>
# Note:<BR>
#     this function admits a few experimental usages, which have not<BR>
#     been extensively tested:<BR>
#      - single precision setting (even though the output alpha is double <BR>
#        precision)<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
np.random.seed(0)<BR>
X = np.asfortranarray(np.random.normal(size = (64,10000)))<BR>
D = np.asfortranarray(np.random.normal(size = (64,200)))<BR>
D = np.asfortranarray(D / np.tile(np.sqrt((D*D).sum(axis=0)),(D.shape[0],1)))<BR>
ind_groups = np.array(xrange(0,10000,10),dtype=np.int32)<BR>
tic = time.time()<BR>
alpha = spams.somp(X,D,ind_groups,L = 10,eps = 0.1,numThreads=-1)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"%f signals processed per second"</FONT> %(X.shape[1] / t)</TD></TR>
</TABLE><!--TOC subsection Function spams.l1L2BCD-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc16">4.8</A>  Function spams.l1L2BCD</H3><!--SEC END --><P>
This is a fast implementation of a simultaneous signal decomposition formulation. Given a set of matrices <I><B>X</B></I>=[<I><B>X</B></I><SUP>1</SUP>,…,<I><B>X</B><SUP>n</SUP></I>] in ℝ<SUP><I>m</I> × <I>N</I></SUP>, where the <I><B>X</B><SUP>i</SUP></I>’s are in ℝ<SUP><I>m</I> × <I>n<SUB>i</SUB></I></SUP>, and a dictionary <I><B>D</B></I> in ℝ<SUP><I>m</I> × <I>p</I></SUP>, the algorithm returns a matrix of coefficients <I><B>A</B></I>=[<I><B>A</B></I><SUP>1</SUP>,…,<I><B>A</B><SUP>n</SUP></I>] in ℝ<SUP><I>p</I> × <I>N</I></SUP> which is an approximate solution of the following NP-hard problem
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
∀ <I>i</I>   </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>A</B><SUP>i</SUP></I> ∈ ℝ<SUP><I>p</I> × <I>n<SUB>i</SUB></I></SUP></TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||<I><B>X</B><SUP>i</SUP></I>−<I><B>DA</B><SUP>i</SUP></I>||<I><SUB>F</SUB></I><SUP>2</SUP>   s.t.   ||<I><B>A</B><SUP>i</SUP></I>||<SUB>1,2</SUB> ≤ </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">λ</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>n<SUB>i</SUB></I></TD></TR>
</TABLE></TD><TD CLASS="dcell">.
    (19)</TD></TR>
</TABLE><P>
or 
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
∀ <I>i</I>   </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>A</B><SUP>i</SUP></I> ∈ ℝ<SUP><I>p</I> × <I>n<SUB>i</SUB></I></SUP></TD></TR>
</TABLE></TD><TD CLASS="dcell">  ||<I><B>A</B><SUP>i</SUP></I>||<SUB>1,2</SUB>   s.t.   ||<I><B>X</B><SUP>i</SUP></I>−<I><B>DA</B><SUP>i</SUP></I>||<I><SUB>F</SUB></I><SUP>2</SUP> ≤ λ <I>n<SUB>i</SUB></I>.
    (20)</TD></TR>
</TABLE><P>
To be efficient, the method first compute the covariance matrix <I><B>D</B><SUP>T</SUP><B>D</B></I>, then for each signal, it computes <I><B>D</B><SUP>T</SUP><B>X</B><SUP>i</SUP></I> and performs the decomposition with a Cholesky-based algorithm.</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: l1L2BCD<BR>
# (this function has not been intensively tested).<BR>
#<BR>
# Usage: spams.l1L2BCD(X,D,alpha0,list_groups,lambda1 = None,mode= spams_wrap.PENALTY,itermax = 100,<BR>
#               tol = 1e-3,numThreads = -1)<BR>
#<BR>
# Description:<BR>
#     l1L2BCD is a solver for a <BR>
#     Simultaneous signal decomposition formulation based on block <BR>
#     coordinate descent.<BR>
#     <BR>
#     X is a matrix structured in groups of signals, which we denote<BR>
#     by X=[X_1,...,X_n]<BR>
#     <BR>
#     if mode=2, it solves<BR>
#         for all matrices X_i of X, <BR>
#         min_{A_i} 0.5||X_i-D A_i||_2^2 + lambda1/sqrt(n_i)||A_i||_{1,2}  <BR>
#         where n_i is the number of columns of X_i<BR>
#     if mode=1, it solves<BR>
#         min_{A_i} ||A_i||_{1,2} s.t. ||X_i-D A_i||_2^2  &lt;= n_i lambda1<BR>
#<BR>
# Inputs:<BR>
#       X:  double m x N matrix   (input signals)<BR>
#            m is the signal size<BR>
#            N is the total number of signals <BR>
#       D:  double m x p matrix   (dictionary)<BR>
#          p is the number of elements in the dictionary<BR>
#       alpha0: double dense p x N matrix (initial solution)<BR>
#       list_groups : int32 vector containing the indices (starting at 0)<BR>
#          of the first elements of each groups.<BR>
#       lambda1: (regularization parameter)<BR>
#       mode: (see above, by default 2)<BR>
#       itermax: (maximum number of iterations, by default 100)<BR>
#       tol: (tolerance parameter, by default 0.001)<BR>
#       numThreads: (optional, number of threads for exploiting<BR>
#       multi-core / multi-cpus. By default, it takes the value -1,<BR>
#       which automatically selects all the available CPUs/cores).<BR>
#<BR>
# Output:<BR>
#       alpha: double sparse p x N matrix (output coefficients)<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2010 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#<BR>
# Note:<BR>
#     this function admits a few experimental usages, which have not<BR>
#     been extensively tested:<BR>
#      - single precision setting (even though the output alpha is double <BR>
#        precision)<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
np.random.seed(0)<BR>
X = np.asfortranarray(np.random.normal(size = (64,100)))<BR>
D = np.asfortranarray(np.random.normal(size = (64,200)))<BR>
D = np.asfortranarray(D / np.tile(np.sqrt((D*D).sum(axis=0)),(D.shape[0],1)))<BR>
ind_groups = np.array(xrange(0,X.shape[1],10),dtype=np.int32) <FONT COLOR="#007F00">#indices of the first signals in each group<BR>
# parameters of the optimization procedure are chosen</FONT><BR>
itermax = 100<BR>
tol = 1e-3<BR>
mode = spams.PENALTY<BR>
lambda1 = 0.15 <FONT COLOR="#007F00"># squared norm of the residual should be less than 0.1</FONT><BR>
numThreads = -1 <FONT COLOR="#007F00"># number of processors/cores to use the default choice is -1</FONT><BR>
                <FONT COLOR="#007F00"># and uses all the cores of the machine</FONT><BR>
alpha0 = np.zeros((D.shape[1],X.shape[1]),dtype=np.float64,order=<FONT COLOR="red">"FORTRAN"</FONT>)<BR>
tic = time.time()<BR>
alpha = spams.l1L2BCD(X,D,alpha0,ind_groups,lambda1 = lambda1,mode = mode,itermax = itermax,tol = tol,numThreads = numThreads)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"%f signals processed per second"</FONT> %(X.shape[1] / t)</TD></TR>
</TABLE><!--TOC subsection Function spams.sparseProject-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc17">4.9</A>  Function spams.sparseProject</H3><!--SEC END --><P>
This is a multi-purpose function, implementing fast algorithms for projecting
on convex sets, but it also solves the fused lasso signal approximation
problem. The proposed method is detailed in [<A HREF="#mairal9">20</A>]. The main problems
addressed by this function are the following: Given a matrix
<I><B>U</B></I>=[<I><B>u</B></I><SUB>1</SUB>,…,<I><B>u</B><SUB>n</SUB></I>] in ℝ<SUP><I>m</I> × <I>n</I></SUP>, it finds a matrix
<I><B>V</B></I>=[<I><B>v</B></I><SUB>1</SUB>,…,<I><B>v</B><SUB>n</SUB></I>] in ℝ<SUP><I>m</I> × <I>n</I></SUP> so that for all column <I><B>u</B></I> of <I><B>U</B></I>,
it computes a column <I><B>v</B></I> of <I><B>V</B></I> solving
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>v</B></I> ∈ ℝ<I><SUP>m</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||<I><B>u</B></I>−<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP>    s.t.   ||<I><B>v</B></I>||<SUB>1</SUB> ≤ τ,
    (21)</TD></TR>
</TABLE><P>
or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>v</B></I> ∈ ℝ<I><SUP>m</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||<I><B>u</B></I>−<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP>    s.t.   λ<SUB>1</SUB>||<I><B>v</B></I>||<SUB>1</SUB> +λ<SUB>2</SUB>||<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP>≤ τ,
    (22)</TD></TR>
</TABLE><P>
or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>v</B></I> ∈ ℝ<I><SUP>m</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||<I><B>u</B></I>−<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP>    s.t.   λ<SUB>1</SUB>||<I><B>v</B></I>||<SUB>1</SUB> +λ<SUB>2</SUB>||<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP>+ λ<SUB>3</SUB> <I>FL</I>(<I><B>v</B></I>) ≤ τ,
    (23)</TD></TR>
</TABLE><P>
or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>v</B></I> ∈ ℝ<I><SUP>m</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>u</B></I>−<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP> + λ<SUB>1</SUB>||<I><B>v</B></I>||<SUB>1</SUB> +λ<SUB>2</SUB>||<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP>+ λ<SUB>3</SUB> <I>FL</I>(<I><B>v</B></I>).
    (24)</TD></TR>
</TABLE><P>
Note that for the two last cases, the method performs a small approximation.
The method follows the regularization path, goes from one kink to another, and 
stop whenever the constraint is not satisfied anymore. The solution returned 
by the algorithm is the one obtained at the last kink of the regularization path,
which is in practice close, but not exactly the same as the solution.
This will be corrected in a future release of the toolbox.</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: sparseProject<BR>
#<BR>
# Usage: spams.sparseProject(U,thrs = 1.0,mode = 1,lambda1 = 0.0,lambda2 = 0.0,lambda3 = 0.0,<BR>
#                     pos = 0,numThreads = -1)<BR>
#<BR>
# Description:<BR>
#     sparseProject solves various optimization <BR>
#     problems, including projections on a few convex sets.<BR>
#     It aims at addressing the following problems<BR>
#     for all columns u of U in parallel<BR>
#       1) when mode=1 (projection on the l1-ball)<BR>
#           min_v ||u-v||_2^2  s.t.  ||v||_1 &lt;= thrs<BR>
#       2) when mode=2<BR>
#           min_v ||u-v||_2^2  s.t. ||v||_2^2 + lamuda1||v||_1 &lt;= thrs<BR>
#       3) when mode=3<BR>
#           min_v ||u-v||_2^2  s.t  ||v||_1 + 0.5lamuda1||v||_2^2 &lt;= thrs <BR>
#       4) when mode=4<BR>
#           min_v 0.5||u-v||_2^2 + lamuda1||v||_1  s.t  ||v||_2^2 &lt;= thrs<BR>
#       5) when mode=5<BR>
#           min_v 0.5||u-v||_2^2 + lamuda1||v||_1 +lamuda2 FL(v) + ... <BR>
#                                                   0.5lamuda_3 ||v||_2^2<BR>
#          where FL denotes a "fused lasso" regularization term.<BR>
#       6) when mode=6<BR>
#          min_v ||u-v||_2^2 s.t lamuda1||v||_1 +lamuda2 FL(v) + ...<BR>
#                                             0.5lamuda3||v||_2^2 &lt;= thrs<BR>
#                                             <BR>
#        When pos=true and mode &lt;= 4,<BR>
#        it solves the previous problems with positivity constraints <BR>
#<BR>
# Inputs:<BR>
#       U:  double m x n matrix   (input signals)<BR>
#               m is the signal size<BR>
#               n is the number of signals to project<BR>
#       thrs: (parameter)<BR>
#       lambda1: (parameter)<BR>
#       lambda2: (parameter)<BR>
#       lambda3: (parameter)<BR>
#       mode: (see above)<BR>
#       pos: (optional, false by default)<BR>
#       numThreads: (optional, number of threads for exploiting<BR>
#         multi-core / multi-cpus. By default, it takes the value -1,<BR>
#         which automatically selects all the available CPUs/cores).<BR>
#<BR>
# Output:<BR>
#       V: double m x n matrix (output matrix)<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2009 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#<BR>
# Note:<BR>
#     this function admits a few experimental usages, which have not<BR>
#     been extensively tested:<BR>
#         - single precision setting <BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
np.random.seed(0)<BR>
X = np.asfortranarray(np.random.normal(size = (20000,100)))<BR>
X = np.asfortranarray(X / np.tile(np.sqrt((X*X).sum(axis=0)),(X.shape[0],1)))<BR>
param = {<FONT COLOR="red">'numThreads'</FONT> : -1, <FONT COLOR="#007F00"># number of processors/cores to use (-1 =&gt; all cores)</FONT><BR>
         <FONT COLOR="red">'pos'</FONT> : False,<BR>
         <FONT COLOR="red">'mode'</FONT>: 1, <FONT COLOR="#007F00"># projection on the l1 ball</FONT><BR>
         <FONT COLOR="red">'thrs'</FONT> : 2}<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"\n  Projection on the l1 ball"</FONT><BR>
tic = time.time()<BR>
X1 = spams.sparseProject(X,**param)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"  Time : "</FONT>, t<BR>
<FONT COLOR="blue"><B>if</B></FONT> (t != 0):<BR>
    <FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"%f signals of size %d projected per second"</FONT> %((X.shape[1] / t),X.shape[0])<BR>
s = np.abs(X1).sum(axis=0)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"Checking constraint: %f, %f"</FONT> %(min(s),max(s))<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"\n  Projection on the Elastic-Net"</FONT><BR>
param[<FONT COLOR="red">'mode'</FONT>] = 2  <FONT COLOR="#007F00"># projection on the Elastic-Net</FONT><BR>
param[<FONT COLOR="red">'lambda1'</FONT>] = 0.15<BR>
tic = time.time()<BR>
X1 = spams.sparseProject(X,**param)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"  Time : "</FONT>, t<BR>
<FONT COLOR="blue"><B>if</B></FONT> (t != 0):<BR>
    <FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"%f signals of size %d projected per second"</FONT> %((X.shape[1] / t),X.shape[0])<BR>
constraints = (X1*X1).sum(axis=0) + param[<FONT COLOR="red">'lambda1'</FONT>] * np.abs(X1).sum(axis=0)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'Checking constraint: %f, %f (Projection is approximate : stops at a kink)'</FONT> %(min(constraints),max(constraints))<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"\n  Projection on the FLSA"</FONT><BR>
param[<FONT COLOR="red">'mode'</FONT>] = 6       <FONT COLOR="#007F00"># projection on the FLSA</FONT><BR>
param[<FONT COLOR="red">'lambda1'</FONT>] = 0.7<BR>
param[<FONT COLOR="red">'lambda2'</FONT>] = 0.7<BR>
param[<FONT COLOR="red">'lambda3'</FONT>] = 1.0<BR>
X = np.asfortranarray(np.random.random(size = (2000,100)))<BR>
X = np.asfortranarray(X / np.tile(np.sqrt((X*X).sum(axis=0)),(X.shape[0],1)))<BR>
tic = time.time()<BR>
X1 = spams.sparseProject(X,**param)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"  Time : "</FONT>, t<BR>
<FONT COLOR="blue"><B>if</B></FONT> (t != 0):<BR>
    <FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"%f signals of size %d projected per second"</FONT> %((X.shape[1] / t),X.shape[0])<BR>
constraints = 0.5 * param[<FONT COLOR="red">'lambda3'</FONT>] * (X1*X1).sum(axis=0) + param[<FONT COLOR="red">'lambda1'</FONT>] * np.abs(X1).sum(axis=0) + \<BR>
param[<FONT COLOR="red">'lambda2'</FONT>] * np.abs(X1[2:,] - X1[1:-1,]).sum(axis=0)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'Checking constraint: %f, %f (Projection is approximate : stops at a kink)'</FONT> %(min(constraints),max(constraints))</TD></TR>
</TABLE><!--TOC section Proximal Toolbox-->
<H2 CLASS="section"><!--SEC ANCHOR --><A NAME="htoc18">5</A>  Proximal Toolbox</H2><!--SEC END --><P>
The previous toolbox we have presented is well
adapted for solving a large number of small and medium-scale sparse
decomposition problems with the square loss, which is typical from the
classical dictionary learning framework. We now present
a new software package that is adapted for solving a wide range of
possibly large-scale learning problems, with several combinations of losses and
regularization terms. The method implements the proximal methods
of [<A HREF="#beck">1</A>], and includes the proximal solvers for the tree-structured
regularization of [<A HREF="#jenatton3">14</A>], and the solver of [<A HREF="#mairal10">21</A>] for
general structured sparse regularization.
The solver for structured sparse regularization norms includes a C++ max-flow
implementation of the push-relabel algorithm of [<A HREF="#goldberg">12</A>], with
heuristics proposed by [<A HREF="#cherkassky">5</A>].</P><P>This implementation also provides robust stopping criteria based on
<EM>duality gaps</EM>, which are presented in Appendix <A HREF="#appendix">A</A>. It can handle intercepts (unregularized variables). The general formulation that our software
can solve take the form
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>w</B></I> ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> [<I>g</I>(<I><B>w</B></I>) </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=2>▵</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">=</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
</TABLE></TD><TD CLASS="dcell"> <I>f</I>(<I><B>w</B></I>) + λψ(<I><B>w</B></I>)],
</TD></TR>
</TABLE><P>
where <I>f</I> is a smooth loss function and ψ is a regularization function.
When one optimizes a matrix <I><B>W</B></I> in ℝ<SUP><I>p</I> × <I>r</I></SUP> instead of
a vector <I><B>w</B></I> in ℝ<I><SUP>p</SUP></I>, we will write 
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>W</B></I> ∈ ℝ<SUP><I>p</I> × <I>r</I></SUP></TD></TR>
</TABLE></TD><TD CLASS="dcell"> [<I>g</I>(<I><B>W</B></I>) </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=2>▵</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">=</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
</TABLE></TD><TD CLASS="dcell"> <I>f</I>(<I><B>W</B></I>) + λψ(<I><B>W</B></I>)].
</TD></TR>
</TABLE><P>
Note that the software can possibly handle nonnegativity constraints.</P><P>We start by presenting the type of regularization implemented in the software
</P><!--TOC subsection Regularization Functions-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc19">5.1</A>  Regularization Functions</H3><!--SEC END --><P>
Our software can handle the following regularization functions ψ for vectors <I><B>w</B></I> in ℝ<I><SUP>p</SUP></I>:
</P><UL CLASS="itemize"><LI CLASS="li-itemize">
<B>The Tikhonov regularization</B>: ψ(<I><B>w</B></I>) =<FONT SIZE=2><SUP>▵</SUP></FONT> 1/2||<I><B>w</B></I>||<SUB>2</SUB><SUP>2</SUP>.
</LI><LI CLASS="li-itemize"><B>The ℓ<SUB>1</SUB>-norm</B>: ψ(<I><B>w</B></I>) =<FONT SIZE=2><SUP>▵</SUP></FONT> ||<I><B>w</B></I>||<SUB>1</SUB>.
</LI><LI CLASS="li-itemize"><B>The Elastic-Net</B>: ψ(<I><B>w</B></I>) =<FONT SIZE=2><SUP>▵</SUP></FONT> ||<I><B>w</B></I>||<SUB>1</SUB>+γ||<I><B>w</B></I>||<SUB>2</SUB><SUP>2</SUP>.
</LI><LI CLASS="li-itemize"><B>The Fused-Lasso</B>: ψ(<I><B>w</B></I>) =<FONT SIZE=2><SUP>▵</SUP></FONT> ||<I><B>w</B></I>||<SUB>1</SUB>+γ||<I><B>w</B></I>||<SUB>2</SUB><SUP>2</SUP>+γ<SUB>2</SUB>∑<SUB><I>i</I>=1</SUB><SUP><I>p</I>−1</SUP>|<I><B>w</B></I><SUB><I>i</I>+1</SUB>−<I><B>w</B><SUB>i</SUB></I>|.
</LI><LI CLASS="li-itemize"><B>The group Lasso</B>: ψ(<I><B>w</B></I>) =<FONT SIZE=2><SUP>▵</SUP></FONT> ∑<SUB><I>g</I> ∈ <FONT COLOR=red><I>G</I></FONT></SUB> η<I><SUB>g</SUB></I> ||<I><B>w</B><SUB>g</SUB></I>||<SUB>2</SUB>, where <FONT COLOR=red><I>G</I></FONT> are groups of variables.
</LI><LI CLASS="li-itemize"><B>The group Lasso with ℓ<SUB>∞</SUB>-norm</B>: ψ(<I><B>w</B></I>) =<FONT SIZE=2><SUP>▵</SUP></FONT> ∑<SUB><I>g</I> ∈ <FONT COLOR=red><I>G</I></FONT></SUB> η<I><SUB>g</SUB></I> ||<I><B>w</B><SUB>g</SUB></I>||<SUB>∞</SUB>, where <FONT COLOR=red><I>G</I></FONT> are groups of variables.
</LI><LI CLASS="li-itemize"><B>The sparse group Lasso</B>: same as above but with an additional ℓ<SUB>1</SUB> term.
</LI><LI CLASS="li-itemize"><B>The tree-structured sum of ℓ<SUB>2</SUB>-norms</B>: ψ(<I><B>w</B></I>) =<FONT SIZE=2><SUP>▵</SUP></FONT> ∑<SUB><I>g</I> ∈ <FONT COLOR=red><I>G</I></FONT></SUB> η<I><SUB>g</SUB></I> ||<I><B>w</B><SUB>g</SUB></I>||<SUB>2</SUB>, where <FONT COLOR=red><I>G</I></FONT> is a tree-structured set of groups [<A HREF="#jenatton3">14</A>], and the η<I><SUB>g</SUB></I> are positive weights.
</LI><LI CLASS="li-itemize"><B>The tree-structured sum of ℓ<SUB>∞</SUB>-norms</B>: ψ(<I><B>w</B></I>) =<FONT SIZE=2><SUP>▵</SUP></FONT> ∑<SUB><I>g</I> ∈ <FONT COLOR=red><I>G</I></FONT></SUB> η<I><SUB>g</SUB></I> ||<I><B>w</B><SUB>g</SUB></I>||<SUB>∞</SUB>. See [<A HREF="#jenatton3">14</A>]
</LI><LI CLASS="li-itemize"><B>General sum of ℓ<SUB>∞</SUB>-norms</B>: ψ(<I><B>w</B></I>) =<FONT SIZE=2><SUP>▵</SUP></FONT> ∑<SUB><I>g</I> ∈ <FONT COLOR=red><I>G</I></FONT></SUB> η<I><SUB>g</SUB></I> ||<I><B>w</B><SUB>g</SUB></I>||<SUB>∞</SUB>, where no assumption are made on the groups <FONT COLOR=red><I>G</I></FONT>.
</LI></UL><P>
Our software also handles regularization functions ψ on matrices <I><B>W</B></I> in ℝ<SUP><I>p</I> × <I>r</I></SUP> (note that <I><B>W</B></I> can be transposed in these formulations). In particular,
</P><UL CLASS="itemize"><LI CLASS="li-itemize">
<B>The ℓ<SUB>1</SUB>/ℓ<SUB>2</SUB>-norm</B>: ψ(<I><B>W</B></I>) =<FONT SIZE=2><SUP>▵</SUP></FONT> ∑<SUB><I>i</I>=1</SUB><I><SUP>p</SUP></I> ||<I><B>W</B><SUB>i</SUB></I>||<SUB>2</SUB>, where <I><B>W</B><SUB>i</SUB></I> denotes the <I>i</I>-th row of <I><B>W</B></I>.
</LI><LI CLASS="li-itemize"><B>The ℓ<SUB>1</SUB>/ℓ<SUB>∞</SUB>-norm</B>: ψ(<I><B>W</B></I>) =<FONT SIZE=2><SUP>▵</SUP></FONT> ∑<SUB><I>i</I>=1</SUB><I><SUP>p</SUP></I> ||<I><B>W</B><SUB>i</SUB></I>||<SUB>∞</SUB>,
</LI><LI CLASS="li-itemize"><B>The ℓ<SUB>1</SUB>/ℓ<SUB>2</SUB>+ℓ<SUB>1</SUB>-norm</B>: ψ(<I><B>W</B></I>) =<FONT SIZE=2><SUP>▵</SUP></FONT> ∑<SUB><I>i</I>=1</SUB><I><SUP>p</SUP></I> ||<I><B>W</B><SUB>i</SUB></I>||<SUB>2</SUB> + λ<SUB>2</SUB> ∑<SUB><I>i</I>,<I>j</I></SUB>|<I><B>W</B><SUB>ij</SUB></I>|.
</LI><LI CLASS="li-itemize"><B>The ℓ<SUB>1</SUB>/ℓ<SUB>∞</SUB>+ℓ<SUB>1</SUB>-norm</B>: ψ(<I><B>W</B></I>) =<FONT SIZE=2><SUP>▵</SUP></FONT> ∑<SUB><I>i</I>=1</SUB><I><SUP>p</SUP></I> ||<I><B>W</B><SUB>i</SUB></I>||<SUB>∞</SUB>+λ<SUB>2</SUB> ∑<SUB><I>i</I>,<I>j</I></SUB>|<I><B>W</B><SUB>ij</SUB></I>|,
</LI><LI CLASS="li-itemize"><B>The ℓ<SUB>1</SUB>/ℓ<SUB>∞</SUB>-norm on rows and columns</B>: ψ(<I><B>W</B></I>) =<FONT SIZE=2><SUP>▵</SUP></FONT> ∑<SUB><I>i</I>=1</SUB><I><SUP>p</SUP></I> ||<I><B>W</B><SUB>i</SUB></I>||<SUB>∞</SUB>+λ<SUB>2</SUB> ∑<SUB><I>j</I>=1</SUB><I><SUP>r</SUP></I>||<I><B>W</B><SUP>j</SUP></I>||<SUB>∞</SUB>, where <I><B>W</B><SUP>j</SUP></I> denotes the <I>j</I>-th column of <I><B>W</B></I>.
</LI><LI CLASS="li-itemize"><B>The multi-task tree-structured sum of ℓ<SUB>∞</SUB>-norms</B>: 
<TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
ψ(<I><B>W</B></I>)</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=2>▵</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">=</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>r</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>g</I> ∈ <FONT COLOR=red><I>G</I></FONT></TD></TR>
</TABLE></TD><TD CLASS="dcell"> η<I><SUB>g</SUB></I>||<I><B>w</B><SUB>g</SUB><SUP>i</SUP></I>||<SUB>∞</SUB>+ γ </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>g</I> ∈ <FONT COLOR=red><I>G</I></FONT></TD></TR>
</TABLE></TD><TD CLASS="dcell"> η<I><SUB>g</SUB></I> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">max</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>j</I> ∈ <I>g</I></TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>W</B><SUB>j</SUB></I>||<SUB>∞</SUB>,
<A NAME="software:eq:struct"></A>
    (25)</TD></TR>
</TABLE>
where the first double sums is in fact a sum of independent structured norms on the columns <I><B>w</B><SUP>i</SUP></I> of <I><B>W</B></I>, and the right term is a tree-structured regularization norm applied to the ℓ<SUB>∞</SUB>-norm of the rows of <I><B>W</B></I>, thereby inducing the tree-structured regularization at the row level. <FONT COLOR=red><I>G</I></FONT> is here a tree-structured set of groups.
</LI><LI CLASS="li-itemize"><B>The multi-task general sum of ℓ<SUB>∞</SUB>-norms</B> is the same as Eq. (<A HREF="#software:eq:struct">25</A>) except that the groups <FONT COLOR=red><I>G</I></FONT> are general overlapping groups.
</LI><LI CLASS="li-itemize"><B>The trace norm</B>: ψ(<I><B>W</B></I>) =<FONT SIZE=2><SUP>▵</SUP></FONT> ||<I><B>W</B></I>||<SUB>*</SUB>.
</LI></UL><P>
Non-convex regularizations are also implemented with the ISTA algorithm (no duality gaps are of course provided in these cases):
</P><UL CLASS="itemize"><LI CLASS="li-itemize">
<B>The ℓ<SUB>0</SUB>-pseudo-norm</B>: ψ(<I><B>w</B></I>) =<FONT SIZE=2><SUP>▵</SUP></FONT> ||<I><B>w</B></I>||<SUB>0</SUB>.
</LI><LI CLASS="li-itemize"><B>The rank</B>: ψ(<I><B>W</B></I>) =<FONT SIZE=2><SUP>▵</SUP></FONT> randk(<I><B>W</B></I>).
</LI><LI CLASS="li-itemize"><B>The tree-structured ℓ<SUB>0</SUB>-pseudo-norm</B>: ψ(<I><B>w</B></I>) =<FONT SIZE=2><SUP>▵</SUP></FONT> ∑<SUB><I>g</I> ∈ <FONT COLOR=red><I>G</I></FONT></SUB> δ<SUB><I><B>w</B>g</I> ≠ 0</SUB>.
</LI></UL><P>All of these regularization terms for vectors or matrices can be coupled with
nonnegativity constraints. It is also possible to add an intercept, which one
wishes not to regularize, and we will include this possibility in the next
sections. There are also a few hidden undocumented options which are available in the source code.</P><P>We now present 3 functions for computing proximal operators associated to the previous regularization functions.
</P><!--TOC subsection Function spams.proximalFlat-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc20">5.2</A>  Function spams.proximalFlat</H3><!--SEC END --><P>
This function computes the proximal operators associated to many regularization functions, for input signals <I><B>U</B></I>=[<I><B>u</B></I><SUP>1</SUP>,…,<I><B>u</B><SUP>n</SUP></I>] in ℝ<SUP><I>p</I> × <I>n</I></SUP>, it finds a matrix <I><B>V</B></I>=[<I><B>v</B></I><SUP>1</SUP>,…,<I><B>v</B><SUP>n</SUP></I>] in ℝ<SUP><I>p</I> × <I>n</I></SUP> such that:</P><P>•  If one chooses a regularization function on vectors, for every column <I><B>u</B></I> of <I><B>U</B></I>, it computes one column <I><B>v</B></I> of <I><B>V</B></I> solving
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>v</B></I> ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>u</B></I>−<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP> + λ ||<I><B>v</B></I>||<SUB>0</SUB>,
    (26)</TD></TR>
</TABLE><P>
or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>v</B></I> ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>u</B></I>−<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP> + λ ||<I><B>v</B></I>||<SUB>1</SUB>,
    (27)</TD></TR>
</TABLE><P>
or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>v</B></I> ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>u</B></I>−<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP> + </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">λ</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP>,
    (28)</TD></TR>
</TABLE><P>
or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>v</B></I> ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>u</B></I>−<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP> + λ ||<I><B>v</B></I>||<SUB>1</SUB> + λ<SUB>2</SUB>||<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP>,
    (29)</TD></TR>
</TABLE><P>
or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>v</B></I> ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>u</B></I>−<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP> + λ</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>p</I>−1</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>j</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell">|<I><B>v</B></I><SUB><I>j</I>+1</SUB><I><SUP>i</SUP></I>−<I><B>v</B><SUB>j</SUB><SUP>i</SUP></I>|+λ<SUB>2</SUB> ||<I><B>v</B></I>||<SUB>1</SUB> + λ<SUB>3</SUB>||<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP>,
    (30)</TD></TR>
</TABLE><P>
or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>v</B></I> ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>u</B></I>−<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP> + λ </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>g</I> ∈ <FONT COLOR=red><I>T</I></FONT></TD></TR>
</TABLE></TD><TD CLASS="dcell"> δ<I><SUP>g</SUP></I>(<I><B>v</B></I>),
    (31)</TD></TR>
</TABLE><P>
where <FONT COLOR=red><I>T</I></FONT> is a tree-structured set of groups (see [<A HREF="#jenatton4">15</A>]), and δ<I><SUP>g</SUP></I>(<I><B>v</B></I>) = 0 if <I><B>v</B><SUB>g</SUB></I>=0 and 1 otherwise.
It can also solve
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>v</B></I> ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>u</B></I>−<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP> + λ </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>g</I> ∈ <FONT COLOR=red><I>T</I></FONT></TD></TR>
</TABLE></TD><TD CLASS="dcell"> η<I><SUP>g</SUP></I> ||<I><B>v</B><SUB>g</SUB></I>||<SUB>2</SUB>,
    (32)</TD></TR>
</TABLE><P>
or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>v</B></I> ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>u</B></I>−<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP> + λ </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>g</I> ∈ <FONT COLOR=red><I>T</I></FONT></TD></TR>
</TABLE></TD><TD CLASS="dcell"> η<I><SUP>g</SUP></I> ||<I><B>v</B><SUB>g</SUB></I>||<SUB>∞</SUB>,
    (33)</TD></TR>
</TABLE><P>
or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>v</B></I> ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>u</B></I>−<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP> + λ </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>g</I> ∈ <FONT COLOR=red><I>G</I></FONT></TD></TR>
</TABLE></TD><TD CLASS="dcell"> η<I><SUP>g</SUP></I> ||<I><B>v</B><SUB>g</SUB></I>||<SUB>∞</SUB>,
    (34)</TD></TR>
</TABLE><P>
where <FONT COLOR=red><I>G</I></FONT> is any kind of set of groups.</P><P>This function can also solve the following proximal operators on matrices
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>V</B></I> ∈ ℝ<SUP><I>p</I> × <I>n</I></SUP></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>U</B></I>−<I><B>V</B></I>||<I><SUB>F</SUB></I><SUP>2</SUP> + λ </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>p</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||<I><B>V</B><SUB>i</SUB></I>||<SUB>2</SUB>, 
    (35)</TD></TR>
</TABLE><P>
where <I><B>V</B><SUB>i</SUB></I> is the <I>i</I>-th row of <I><B>V</B></I>, or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>V</B></I> ∈ ℝ<SUP><I>p</I> × <I>n</I></SUP></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>U</B></I>−<I><B>V</B></I>||<I><SUB>F</SUB></I><SUP>2</SUP> + λ </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>p</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||<I><B>V</B><SUB>i</SUB></I>||<SUB>∞</SUB>, 
    (36)</TD></TR>
</TABLE><P>
or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>V</B></I> ∈ ℝ<SUP><I>p</I> × <I>n</I></SUP></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>U</B></I>−<I><B>V</B></I>||<I><SUB>F</SUB></I><SUP>2</SUP> + λ </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>p</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||<I><B>V</B><SUB>i</SUB></I>||<SUB>2</SUB> +λ<SUB>2</SUB> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>p</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>j</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> |<I><B>V</B><SUB>ij</SUB></I>|, 
    (37)</TD></TR>
</TABLE><P>
or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>V</B></I> ∈ ℝ<SUP><I>p</I> × <I>n</I></SUP></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>U</B></I>−<I><B>V</B></I>||<I><SUB>F</SUB></I><SUP>2</SUP> + λ </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>p</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||<I><B>V</B><SUB>i</SUB></I>||<SUB>∞</SUB>+λ<SUB>2</SUB> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>p</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>j</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> |<I><B>V</B><SUB>ij</SUB></I>|, 
    (38)</TD></TR>
</TABLE><P>
or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>V</B></I> ∈ ℝ<SUP><I>p</I> × <I>n</I></SUP></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>U</B></I>−<I><B>V</B></I>||<I><SUB>F</SUB></I><SUP>2</SUP> + λ </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>p</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||<I><B>V</B><SUB>i</SUB></I>||<SUB>∞</SUB>+λ<SUB>2</SUB> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>j</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||<I><B>V</B><SUP>j</SUP></I>||<SUB>∞</SUB>.
    (39)</TD></TR>
</TABLE><P>
where <I><B>V</B><SUP>j</SUP></I> is the <I>j</I>-th column of <I><B>V</B></I>.</P><P>See usage details below:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: proximalFlat<BR>
#<BR>
# Usage: spams.proximalFlat(U,return_val_loss = False,numThreads =-1,lambda1=1.0,lambda2=0.,<BR>
#                    lambda3=0.,intercept=False,resetflow=False,regul="",verbose=False,<BR>
#                    pos=False,clever=True,size_group=1,groups = None,transpose=False)<BR>
#<BR>
# Description:<BR>
#     proximalFlat computes proximal operators. Depending<BR>
#         on the value of regul, it computes <BR>
#         <BR>
#         Given an input matrix U=[u^1,\ldots,u^n], it computes a matrix <BR>
#         V=[v^1,\ldots,v^n] such that<BR>
#         if one chooses a regularization functions on vectors, it computes<BR>
#         for each column u of U, a column v of V solving<BR>
#         if regul='l0'<BR>
#             argmin 0.5||u-v||_2^2 + lambda1||v||_0<BR>
#         if regul='l1'<BR>
#             argmin 0.5||u-v||_2^2 + lambda1||v||_1<BR>
#         if regul='l2'<BR>
#             argmin 0.5||u-v||_2^2 + 0.5lambda1||v||_2^2<BR>
#         if regul='elastic-net'<BR>
#             argmin 0.5||u-v||_2^2 + lambda1||v||_1 + lambda1_2||v||_2^2<BR>
#         if regul='fused-lasso'<BR>
#             argmin 0.5||u-v||_2^2 + lambda1 FL(v) + ...<BR>
#                               ...  lambda1_2||v||_1 + lambda1_3||v||_2^2<BR>
#         if regul='linf'<BR>
#             argmin 0.5||u-v||_2^2 + lambda1||v||_inf<BR>
#         if regul='l1-constraint'<BR>
#             argmin 0.5||u-v||_2^2 s.t. ||v||_1 &lt;= lambda1<BR>
#         if regul='l2-not-squared'<BR>
#             argmin 0.5||u-v||_2^2 + lambda1||v||_2<BR>
#         if regul='group-lasso-l2'  <BR>
#             argmin 0.5||u-v||_2^2 + lambda1 sum_g ||v_g||_2 <BR>
#             where the groups are either defined by groups or by size_group,<BR>
#         if regul='group-lasso-linf'<BR>
#             argmin 0.5||u-v||_2^2 + lambda1 sum_g ||v_g||_inf<BR>
#         if regul='sparse-group-lasso-l2'  <BR>
#             argmin 0.5||u-v||_2^2 + lambda1 sum_g ||v_g||_2 + lambda1_2 ||v||_1<BR>
#             where the groups are either defined by groups or by size_group,<BR>
#         if regul='sparse-group-lasso-linf'<BR>
#             argmin 0.5||u-v||_2^2 + lambda1 sum_g ||v_g||_inf + lambda1_2 ||v||_1<BR>
#         if regul='trace-norm-vec' <BR>
#             argmin 0.5||u-v||_2^2 + lambda1 ||mat(v)||_* <BR>
#            where mat(v) has size_group rows<BR>
#            <BR>
#         if one chooses a regularization function on matrices<BR>
#         if regul='l1l2',  V= <BR>
#             argmin 0.5||U-V||_F^2 + lambda1||V||_{1/2}<BR>
#         if regul='l1linf',  V= <BR>
#             argmin 0.5||U-V||_F^2 + lambda1||V||_{1/inf}<BR>
#         if regul='l1l2+l1',  V= <BR>
#             argmin 0.5||U-V||_F^2 + lambda1||V||_{1/2} + lambda1_2||V||_{1/1}<BR>
#         if regul='l1linf+l1',  V= <BR>
#             argmin 0.5||U-V||_F^2 + lambda1||V||_{1/inf} + lambda1_2||V||_{1/1}<BR>
#         if regul='l1linf+row-column',  V= <BR>
#             argmin 0.5||U-V||_F^2 + lambda1||V||_{1/inf} + lambda1_2||V'||_{1/inf}<BR>
#         if regul='trace-norm',  V= <BR>
#             argmin 0.5||U-V||_F^2 + lambda1||V||_*<BR>
#         if regul='rank',  V= <BR>
#             argmin 0.5||U-V||_F^2 + lambda1 rank(V)<BR>
#         if regul='none',  V= <BR>
#             argmin 0.5||U-V||_F^2 <BR>
#             <BR>
#         for all these regularizations, it is possible to enforce non-negativity constraints<BR>
#         with the option pos, and to prevent the last row of U to be regularized, with<BR>
#         the option intercept<BR>
#<BR>
# Inputs:<BR>
#       U:  double m x n matrix   (input signals)<BR>
#               m is the signal size<BR>
#       return_val_loss:     <BR>
#               if true the function will return a tuple of matrices.<BR>
#       lambda1:  (regularization parameter)<BR>
#       regul: (choice of regularization, see above)<BR>
#       lambda2:  (optional, regularization parameter)<BR>
#       lambda3:  (optional, regularization parameter)<BR>
#       verbose: (optional, verbosity level, false by default)<BR>
#       intercept: (optional, last row of U is not regularized,<BR>
#         false by default)<BR>
#       transpose: (optional, transpose the matrix in the regularization function)<BR>
#       size_group: (optional, for regularization functions assuming a group<BR>
#         structure). It is a scalar. When groups is not specified, it assumes<BR>
#         that the groups are the sets of consecutive elements of size size_group<BR>
#       groups: (int32, optional, for regularization functions assuming a group<BR>
#         structure. It is an int32 vector of size m containing the group indices of the<BR>
#         variables (first group is 1).<BR>
#       pos: (optional, adds positivity constraints on the<BR>
#         coefficients, false by default)<BR>
#       numThreads: (optional, number of threads for exploiting<BR>
#         multi-core / multi-cpus. By default, it takes the value -1,<BR>
#         which automatically selects all the available CPUs/cores).<BR>
#       resetflow:    undocumented; modify at your own risks!<BR>
#       clever:    undocumented; modify at your own risks!<BR>
#<BR>
# Output:<BR>
#       V: double m x n matrix (output coefficients)<BR>
#       val_regularizer: double 1 x n vector (value of the regularization<BR>
#       term at the optimum).<BR>
#       val_loss:        vector of size U.shape[1]<BR>
#         alpha = spams.proximalFlat(U,return_val_loss = False,...)<BR>
#         (alpha,val_loss) = spams.proximalFlat(U,return_val_loss = True,...)<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2010 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#<BR>
# Note:<BR>
#     Valid values for the regularization parameter (regul) are:<BR>
#       "l0", "l1", "l2", "linf", "l2-not-squared", "elastic-net", "fused-lasso",<BR>
#       "group-lasso-l2", "group-lasso-linf", "sparse-group-lasso-l2",<BR>
#       "sparse-group-lasso-linf", "l1l2", "l1linf", "l1l2+l1", "l1linf+l1",<BR>
#       "tree-l0", "tree-l2", "tree-linf", "graph", "graph-ridge", "graph-l2",<BR>
#       "multi-task-tree", "multi-task-graph", "l1linf-row-column", "trace-norm",<BR>
#       "trace-norm-vec", "rank", "rank-vec", "none"<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
param = {<FONT COLOR="red">'numThreads'</FONT> : -1,<FONT COLOR="red">'verbose'</FONT> : True,<BR>
         <FONT COLOR="red">'lambda1'</FONT> : 0.1 }<BR>
m = 100;n = 1000<BR>
U = np.asfortranarray(np.random.normal(size = (m,n)))<BR>
<BR>
<FONT COLOR="#007F00"># test L0</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"\nprox l0"</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l0'</FONT><BR>
param[<FONT COLOR="red">'pos'</FONT>] = False       <FONT COLOR="#007F00"># false by default</FONT><BR>
param[<FONT COLOR="red">'intercept'</FONT>] = False <FONT COLOR="#007F00"># false by default</FONT><BR>
alpha = spams.proximalFlat(U,False,**param)<BR>
<BR>
<FONT COLOR="#007F00"># test L1</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"\nprox l1, intercept, positivity constraint"</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l1'</FONT><BR>
param[<FONT COLOR="red">'pos'</FONT>] = True       <FONT COLOR="#007F00"># can be used with all the other regularizations</FONT><BR>
param[<FONT COLOR="red">'intercept'</FONT>] = True <FONT COLOR="#007F00"># can be used with all the other regularizations</FONT><BR>
alpha = spams.proximalFlat(U,False,**param)<BR>
<BR>
<FONT COLOR="#007F00"># test L2</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"\nprox squared-l2"</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l2'</FONT><BR>
param[<FONT COLOR="red">'pos'</FONT>] = False<BR>
param[<FONT COLOR="red">'intercept'</FONT>] = False<BR>
alpha = spams.proximalFlat(U,False,**param)<BR>
<BR>
<FONT COLOR="#007F00"># test elastic-net</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"\nprox elastic-net"</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'elastic-net'</FONT><BR>
param[<FONT COLOR="red">'lambda2'</FONT>] = 0.1<BR>
alpha = spams.proximalFlat(U,**param)<BR>
<BR>
<FONT COLOR="#007F00"># test fused-lasso</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"\nprox fused lasso"</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'fused-lasso'</FONT><BR>
param[<FONT COLOR="red">'lambda2'</FONT>] = 0.1<BR>
param[<FONT COLOR="red">'lambda3'</FONT>] = 0.1<BR>
alpha = spams.proximalFlat(U,**param)<BR>
<BR>
<FONT COLOR="#007F00"># test l1l2</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"\nprox mixed norm l1/l2"</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l1l2'</FONT><BR>
alpha = spams.proximalFlat(U,**param)<BR>
<BR>
<FONT COLOR="#007F00"># test l1linf</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"\nprox mixed norm l1/linf"</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l1linf'</FONT><BR>
alpha = spams.proximalFlat(U,**param)<BR>
<BR>
<FONT COLOR="#007F00"># test l1l2+l1</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"\nprox mixed norm l1/l2 + l1"</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l1l2+l1'</FONT><BR>
param[<FONT COLOR="red">'lambda2'</FONT>] = 0.1<BR>
alpha = spams.proximalFlat(U,**param)<BR>
<BR>
<FONT COLOR="#007F00"># test l1linf+l1</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"\nprox mixed norm l1/linf + l1"</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l1linf+l1'</FONT><BR>
param[<FONT COLOR="red">'lambda2'</FONT>] = 0.1<BR>
alpha = spams.proximalFlat(U,**param)<BR>
<BR>
<FONT COLOR="#007F00"># test l1linf-row-column</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"\nprox mixed norm l1/linf on rows and columns"</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l1linf-row-column'</FONT><BR>
param[<FONT COLOR="red">'lambda2'</FONT>] = 0.1<BR>
alpha = spams.proximalFlat(U,**param)<BR>
<BR>
<FONT COLOR="#007F00"># test none</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"\nprox no regularization"</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'none'</FONT><BR>
alpha = spams.proximalFlat(U,**param)</TD></TR>
</TABLE><!--TOC subsection Function spams.proximalTree-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc21">5.3</A>  Function spams.proximalTree</H3><!--SEC END --><P>
This function computes the proximal operators associated to tree-structured regularization functions, for input signals <I><B>U</B></I>=[<I><B>u</B></I><SUP>1</SUP>,…,<I><B>u</B><SUP>n</SUP></I>] in ℝ<SUP><I>p</I> × <I>n</I></SUP>, and a tree-structured set of groups [<A HREF="#jenatton3">14</A>], it computes a matrix <I><B>V</B></I>=[<I><B>v</B></I><SUP>1</SUP>,…,<I><B>v</B><SUP>n</SUP></I>] in ℝ<SUP><I>p</I> × <I>n</I></SUP>. When one uses a regularization function on vectors, it computes a column <I><B>v</B></I> of <I><B>V</B></I> for every column <I><B>u</B></I> of <I><B>U</B></I>:
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>v</B></I> ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>u</B></I>−<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP> + λ </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>g</I> ∈ <FONT COLOR=red><I>T</I></FONT></TD></TR>
</TABLE></TD><TD CLASS="dcell"> η<I><SUP>g</SUP></I> ||<I><B>v</B><SUB>g</SUB></I>||<SUB>2</SUB>,
    (40)</TD></TR>
</TABLE><P>
or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>v</B></I> ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>u</B></I>−<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP> + λ </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>g</I> ∈ <FONT COLOR=red><I>T</I></FONT></TD></TR>
</TABLE></TD><TD CLASS="dcell"> η<I><SUP>g</SUP></I> ||<I><B>v</B><SUB>g</SUB></I>||<SUB>∞</SUB>,
    (41)</TD></TR>
</TABLE><P>
or
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>v</B></I> ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>u</B></I>−<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP> + λ </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>g</I> ∈ <FONT COLOR=red><I>T</I></FONT></TD></TR>
</TABLE></TD><TD CLASS="dcell"> δ<I><SUP>g</SUP></I>(<I><B>v</B></I>),
    (42)</TD></TR>
</TABLE><P>
where δ<I><SUP>g</SUP></I>(<I><B>v</B></I>)=0 if <I><B>v</B><SUB>g</SUB></I>=0 and 1 otherwise (see appendix of [<A HREF="#jenatton4">15</A>]).</P><P>When the multi-task tree-structured regularization function is used, it solves
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>V</B></I> ∈ ℝ<SUP><I>p</I>× <I>n</I></SUP></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>U</B></I>−<I><B>V</B></I>||<I><SUB>F</SUB></I><SUP>2</SUP> + λ </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>g</I> ∈ <FONT COLOR=red><I>T</I></FONT></TD></TR>
</TABLE></TD><TD CLASS="dcell"> η<I><SUP>g</SUP></I> ||<I><B>v</B><SUB>g</SUB><SUP>i</SUP></I>||<SUB>∞</SUB>+ λ<SUB>2</SUB></TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>g</I> ∈ <FONT COLOR=red><I>T</I></FONT></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">max</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>j</I> ∈ <I>g</I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||<I><B>v</B><SUB>g</SUB><SUP>j</SUP></I>||<SUB>∞</SUB>,
    (43)</TD></TR>
</TABLE><P> 
which is a formulation presented in [<A HREF="#mairal10">21</A>].</P><P>This function can also be used for computing the proximal operators addressed by spams.proximalFlat (it will just not take into account the tree structure). The way the tree is incoded is presented below, (and examples are given in the file test_ProximalTree.m, with more usage details:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: proximalTree<BR>
#<BR>
# Usage: spams.proximalTree(U,tree,return_val_loss = False,numThreads =-1,lambda1=1.0,lambda2=0.,<BR>
#                    lambda3=0.,intercept=False,resetflow=False,regul="",verbose=False,<BR>
#                    pos=False,clever=True,size_group=1,transpose=False)<BR>
#<BR>
# Description:<BR>
#     proximalTree computes a proximal operator. Depending<BR>
#         on the value of regul, it computes <BR>
#         <BR>
#         Given an input matrix U=[u^1,\ldots,u^n], and a tree-structured set of groups T,<BR>
#         it returns a matrix V=[v^1,\ldots,v^n]:<BR>
#         <BR>
#         when the regularization function is for vectors,<BR>
#         for every column u of U, it compute a column v of V solving<BR>
#         if regul='tree-l0'<BR>
#             argmin 0.5||u-v||_2^2 + lambda1 \sum_{g \in T} \delta^g(v)<BR>
#         if regul='tree-l2'<BR>
#           for all i, v^i = <BR>
#             argmin 0.5||u-v||_2^2 + lambda1\sum_{g \in T} \eta_g||v_g||_2<BR>
#         if regul='tree-linf'<BR>
#           for all i, v^i = <BR>
#             argmin 0.5||u-v||_2^2 + lambda1\sum_{g \in T} \eta_g||v_g||_inf<BR>
#             <BR>
#         when the regularization function is for matrices:<BR>
#         if regul='multi-task-tree'<BR>
#            V=argmin 0.5||U-V||_F^2 + lambda1 \sum_{i=1}^n\sum_{g \in T} \eta_g||v^i_g||_inf + ...<BR>
#                                                lambda1_2 \sum_{g \in T} \eta_g max_{j in g}||V_j||_{inf}<BR>
#                                                <BR>
#         it can also be used with any non-tree-structured regularization addressed by proximalFlat<BR>
#         <BR>
#         for all these regularizations, it is possible to enforce non-negativity constraints<BR>
#         with the option pos, and to prevent the last row of U to be regularized, with<BR>
#         the option intercept<BR>
#<BR>
# Inputs:<BR>
#       U:  double m x n matrix   (input signals)<BR>
#               m is the signal size<BR>
#       tree: named list <BR>
#             with four fields, eta_g, groups, own_variables and N_own_variables.<BR>
#             <BR>
#             The tree structure requires a particular organization of groups and variables<BR>
#                * Let us denote by N = |T|, the number of groups.<BR>
#                  the groups should be ordered T={g1,g2,\ldots,gN} such that if gi is included<BR>
#                  in gj, then j &lt;= i. g1 should be the group at the root of the tree <BR>
#                  and contains every variable.<BR>
#                * Every group is a set of  contiguous indices for instance <BR>
#                  gi={3,4,5} or gi={4,5,6,7} or gi={4}, but not {3,5};<BR>
#                * We define root(gi) as the indices of the variables that are in gi,<BR>
#                  but not in its descendants. For instance for<BR>
#                  T={ g1={1,2,3,4},g2={2,3},g3={4} }, then, root(g1)={1}, <BR>
#                  root(g2)={2,3}, root(g3)={4},<BR>
#                  We assume that for all i, root(gi) is a set of contigous variables<BR>
#                * We assume that the smallest of root(gi) is also the smallest index of gi.<BR>
#                <BR>
#                For instance, <BR>
#                  T={ g1={1,2,3,4},g2={2,3},g3={4} }, is a valid set of groups.<BR>
#                  but we can not have<BR>
#                  T={ g1={1,2,3,4},g2={1,2},g3={3} }, since root(g1)={4} and 4 is not the<BR>
#                  smallest element in g1.<BR>
#                  <BR>
#             We do not lose generality with these assumptions since they can be fullfilled for any<BR>
#             tree-structured set of groups after a permutation of variables and a correct ordering of the<BR>
#             groups.<BR>
#             see more examples in test_ProximalTree.m of valid tree-structured sets of groups.<BR>
#             <BR>
#             The first fields sets the weights for every group<BR>
#                tree['eta_g']            double N vector <BR>
#                <BR>
#             The next field sets inclusion relations between groups <BR>
#             (but not between groups and variables):<BR>
#                tree['groups']           sparse (double or boolean) N x N matrix  <BR>
#                the (i,j) entry is non-zero if and only if i is different than j and <BR>
#                gi is included in gj.<BR>
#                the first column corresponds to the group at the root of the tree.<BR>
#                <BR>
#             The next field define the smallest index of each group gi, <BR>
#             which is also the smallest index of root(gi)<BR>
#             tree['own_variables']    int32 N vector<BR>
#             <BR>
#             The next field define for each group gi, the size of root(gi)<BR>
#             tree['N_own_variables']  int32 N vector <BR>
#             <BR>
#             examples are given in test_ProximalTree.m<BR>
#             <BR>
#       return_val_loss:     <BR>
#               if true the function will return a tuple of matrices.<BR>
#       lambda1:  (regularization parameter)<BR>
#       regul: (choice of regularization, see above)<BR>
#       lambda2:  (optional, regularization parameter)<BR>
#       lambda3:  (optional, regularization parameter)<BR>
#       verbose: (optional, verbosity level, false by default)<BR>
#       intercept: (optional, last row of U is not regularized,<BR>
#         false by default)<BR>
#       pos: (optional, adds positivity constraints on the<BR>
#         coefficients, false by default)<BR>
#       transpose: (optional, transpose the matrix in the regularization function)<BR>
#       size_group: (optional, for regularization functions assuming a group<BR>
#         structure). It is a scalar. When groups is not specified, it assumes<BR>
#         that the groups are the sets of consecutive elements of size size_group<BR>
#       numThreads: (optional, number of threads for exploiting<BR>
#         multi-core / multi-cpus. By default, it takes the value -1,<BR>
#         which automatically selects all the available CPUs/cores).<BR>
#       resetflow:    undocumented; modify at your own risks!<BR>
#       clever:    undocumented; modify at your own risks!<BR>
#<BR>
# Output:<BR>
#       V: double m x n matrix (output coefficients)<BR>
#       val_regularizer: double 1 x n vector (value of the regularization<BR>
#       term at the optimum).<BR>
#       val_loss:        vector of size U.shape[1]<BR>
#         alpha = spams.proximalTree(U,tree,return_val_loss = False,...)<BR>
#         (alpha,val_loss) = spams.proximalTree(U,tree,return_val_loss = True,...)<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2010 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#<BR>
# Note:<BR>
#     Valid values for the regularization parameter (regul) are:<BR>
#       "l0", "l1", "l2", "linf", "l2-not-squared", "elastic-net", "fused-lasso",<BR>
#       "group-lasso-l2", "group-lasso-linf", "sparse-group-lasso-l2",<BR>
#       "sparse-group-lasso-linf", "l1l2", "l1linf", "l1l2+l1", "l1linf+l1",<BR>
#       "tree-l0", "tree-l2", "tree-linf", "graph", "graph-ridge", "graph-l2",<BR>
#       "multi-task-tree", "multi-task-graph", "l1linf-row-column", "trace-norm",<BR>
#       "trace-norm-vec", "rank", "rank-vec", "none"<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
param = {<FONT COLOR="red">'numThreads'</FONT> : -1,<FONT COLOR="red">'verbose'</FONT> : True,<BR>
         <FONT COLOR="red">'pos'</FONT> : False, <FONT COLOR="red">'intercept'</FONT> : False, <FONT COLOR="red">'lambda1'</FONT> : 0.1 }<BR>
m = 10;n = 1000<BR>
U = np.asfortranarray(np.random.normal(size = (m,n)))<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'First tree example'</FONT><BR>
<FONT COLOR="#007F00"># Example 1 of tree structure<BR>
# tree structured groups:<BR>
# g1= {0 1 2 3 4 5 6 7 8 9}<BR>
# g2= {2 3 4}<BR>
# g3= {5 6 7 8 9}</FONT><BR>
own_variables =  np.array([0,2,5],dtype=np.int32) <FONT COLOR="#007F00"># pointer to the first variable of each group</FONT><BR>
N_own_variables =  np.array([2,3,5],dtype=np.int32) <FONT COLOR="#007F00"># number of "root" variables in each group<BR>
# (variables that are in a group, but not in its descendants).<BR>
# for instance root(g1)={0,1}, root(g2)={2 3 4}, root(g3)={5 6 7 8 9}</FONT><BR>
eta_g = np.array([1,1,1],dtype=np.float64) <FONT COLOR="#007F00"># weights for each group, they should be non-zero to use fenchel duality</FONT><BR>
groups = np.asfortranarray([[0,0,0],<BR>
                            [1,0,0],<BR>
                            [1,0,0]],dtype = np.bool)<BR>
<FONT COLOR="#007F00"># first group should always be the root of the tree<BR>
# non-zero entriees mean inclusion relation ship, here g2 is a children of g1,<BR>
# g3 is a children of g1</FONT><BR>
groups = ssp.csc_matrix(groups,dtype=np.bool)<BR>
tree = {<FONT COLOR="red">'eta_g'</FONT>: eta_g,<FONT COLOR="red">'groups'</FONT> : groups,<FONT COLOR="red">'own_variables'</FONT> : own_variables,<BR>
        <FONT COLOR="red">'N_own_variables'</FONT> : N_own_variables}<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\ntest prox tree-l0'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'tree-l2'</FONT><BR>
alpha = spams.proximalTree(U,tree,False,**param)<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\ntest prox tree-linf'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'tree-linf'</FONT><BR>
alpha = spams.proximalTree(U,tree,False,**param)<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'Second tree example'</FONT><BR>
<FONT COLOR="#007F00"># Example 2 of tree structure<BR>
# tree structured groups:<BR>
# g1= {0 1 2 3 4 5 6 7 8 9}    root(g1) = { };<BR>
# g2= {0 1 2 3 4 5}            root(g2) = {0 1 2};<BR>
# g3= {3 4}                    root(g3) = {3 4};<BR>
# g4= {5}                      root(g4) = {5};<BR>
# g5= {6 7 8 9}                root(g5) = { };<BR>
# g6= {6 7}                    root(g6) = {6 7};<BR>
# g7= {8 9}                    root(g7) = {8};<BR>
# g8 = {9}                     root(g8) = {9};</FONT><BR>
own_variables =  np.array([0, 0, 3, 5, 6, 6, 8, 9],dtype=np.int32)<BR>
N_own_variables =  np.array([0,3,2,1,0,2,1,1],dtype=np.int32)<BR>
eta_g = np.array([1,1,1,2,2,2,2.5,2.5],dtype=np.float64)<BR>
groups = np.asfortranarray([[0,0,0,0,0,0,0,0],<BR>
                [1,0,0,0,0,0,0,0],<BR>
                [0,1,0,0,0,0,0,0],<BR>
                [0,1,0,0,0,0,0,0],<BR>
                [1,0,0,0,0,0,0,0],<BR>
                [0,0,0,0,1,0,0,0],<BR>
                [0,0,0,0,1,0,0,0],<BR>
                [0,0,0,0,0,0,1,0]],dtype = np.bool)<BR>
groups = ssp.csc_matrix(groups,dtype=np.bool)<BR>
tree = {<FONT COLOR="red">'eta_g'</FONT>: eta_g,<FONT COLOR="red">'groups'</FONT> : groups, <FONT COLOR="red">'own_variables'</FONT> : own_variables,<BR>
        <FONT COLOR="red">'N_own_variables'</FONT> : N_own_variables}<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\ntest prox tree-l0'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'tree-l0'</FONT><BR>
alpha = spams.proximalTree(U,tree,False,**param)<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\ntest prox tree-l2'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'tree-l2'</FONT><BR>
alpha = spams.proximalTree(U,tree,False,**param)<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\ntest prox tree-linf'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'tree-linf'</FONT><BR>
alpha = spams.proximalTree(U,tree,False,**param)<BR>
<BR>
<FONT COLOR="#007F00"># mexProximalTree also works with non-tree-structured regularization functions</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nprox l1, intercept, positivity constraint'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l1'</FONT><BR>
param[<FONT COLOR="red">'pos'</FONT>] = True       <FONT COLOR="#007F00"># can be used with all the other regularizations</FONT><BR>
param[<FONT COLOR="red">'intercept'</FONT>] = True <FONT COLOR="#007F00"># can be used with all the other regularizations     </FONT><BR>
alpha = spams.proximalTree(U,tree,False,**param)<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nprox multi-task tree'</FONT><BR>
param[<FONT COLOR="red">'pos'</FONT>] = False<BR>
param[<FONT COLOR="red">'intercept'</FONT>] = False<BR>
param[<FONT COLOR="red">'lambda2'</FONT>] = param[<FONT COLOR="red">'lambda1'</FONT>]<BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'multi-task-tree'</FONT><BR>
alpha = spams.proximalTree(U,tree,False,**param)</TD></TR>
</TABLE><!--TOC subsection Function spams.proximalGraph-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc22">5.4</A>  Function spams.proximalGraph</H3><!--SEC END --><P>
This function computes the proximal operators associated to structured sparse regularization, for input signals <I><B>U</B></I>=[<I><B>u</B></I><SUP>1</SUP>,…,<I><B>u</B><SUP>n</SUP></I>] in ℝ<SUP><I>p</I> × <I>n</I></SUP>, and a set of groups [<A HREF="#mairal10">21</A>], it returns a matrix <I><B>V</B></I>=[<I><B>v</B></I><SUP>1</SUP>,…,<I><B>v</B><SUP>n</SUP></I>] in ℝ<SUP><I>p</I> × <I>n</I></SUP>.
When one uses a regularization function on vectors, it computes a column <I><B>v</B></I> of <I><B>V</B></I> for every column <I><B>u</B></I> of <I><B>U</B></I>:
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>v</B></I> ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>u</B></I>−<I><B>v</B></I>||<SUB>2</SUB><SUP>2</SUP> + λ </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>g</I> ∈ <FONT COLOR=red><I>G</I></FONT></TD></TR>
</TABLE></TD><TD CLASS="dcell"> η<I><SUP>g</SUP></I> ||<I><B>v</B><SUB>g</SUB></I>||<SUB>∞</SUB>,
    (44)</TD></TR>
</TABLE><P>
or with a regularization function on matrices, it computes <I><B>V</B></I> solving
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>V</B></I> ∈ ℝ<SUP><I>p</I>× <I>n</I></SUP></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>U</B></I>−<I><B>V</B></I>||<I><SUB>F</SUB></I><SUP>2</SUP> + λ </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>g</I> ∈ <FONT COLOR=red><I>G</I></FONT></TD></TR>
</TABLE></TD><TD CLASS="dcell"> η<I><SUP>g</SUP></I> ||<I><B>v</B><SUB>g</SUB><SUP>i</SUP></I>||<SUB>∞</SUB>+ λ<SUB>2</SUB></TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>g</I> ∈ <FONT COLOR=red><I>G</I></FONT></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">max</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>j</I> ∈ <I>g</I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||<I><B>v</B><SUB>g</SUB><SUP>j</SUP></I>||<SUB>∞</SUB>,
    (45)</TD></TR>
</TABLE><P> 
This function can also be used for computing the proximal operators addressed by spams.proximalFlat. The way the graph is incoded is presented below (and also in the example file test_ProximalGraph.m, with more usage details:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: proximalGraph<BR>
#<BR>
# Usage: spams.proximalGraph(U,graph,return_val_loss = False,numThreads =-1,lambda1=1.0,lambda2=0.,<BR>
#                     lambda3=0.,intercept=False,resetflow=False,regul="",verbose=False,<BR>
#                     pos=False,clever=True,eval= None,size_group=1,transpose=False)<BR>
#<BR>
# Description:<BR>
#     proximalGraph computes a proximal operator. Depending<BR>
#         on the value of regul, it computes <BR>
#         <BR>
#         Given an input matrix U=[u^1,\ldots,u^n], and a set of groups G,<BR>
#         it computes a matrix V=[v^1,\ldots,v^n] such that<BR>
#         <BR>
#         if regul='graph'<BR>
#         for every column u of U, it computes a column v of V solving<BR>
#             argmin 0.5||u-v||_2^2 + lambda1\sum_{g \in G} \eta_g||v_g||_inf<BR>
#             <BR>
#         if regul='graph+ridge'<BR>
#         for every column u of U, it computes a column v of V solving<BR>
#             argmin 0.5||u-v||_2^2 + lambda1\sum_{g \in G} \eta_g||v_g||_inf + lambda1_2||v||_2^2<BR>
#             <BR>
#             <BR>
#         if regul='multi-task-graph'<BR>
#            V=argmin 0.5||U-V||_F^2 + lambda1 \sum_{i=1}^n\sum_{g \in G} \eta_g||v^i_g||_inf + ...<BR>
#                                                lambda1_2 \sum_{g \in G} \eta_g max_{j in g}||V_j||_{inf}<BR>
#                                                <BR>
#         it can also be used with any regularization addressed by proximalFlat<BR>
#         <BR>
#         for all these regularizations, it is possible to enforce non-negativity constraints<BR>
#         with the option pos, and to prevent the last row of U to be regularized, with<BR>
#         the option intercept<BR>
#<BR>
# Inputs:<BR>
#       U:  double p x n matrix   (input signals)<BR>
#               m is the signal size<BR>
#       graph: struct<BR>
#             with three fields, eta_g, groups, and groups_var<BR>
#             <BR>
#             The first fields sets the weights for every group<BR>
#                graph.eta_g            double N vector <BR>
#                <BR>
#             The next field sets inclusion relations between groups <BR>
#             (but not between groups and variables):<BR>
#                graph.groups           sparse (double or boolean) N x N matrix  <BR>
#                the (i,j) entry is non-zero if and only if i is different than j and <BR>
#                gi is included in gj.<BR>
#                <BR>
#             The next field sets inclusion relations between groups and variables<BR>
#                graph.groups_var       sparse (double or boolean) p x N matrix<BR>
#                the (i,j) entry is non-zero if and only if the variable i is included <BR>
#                in gj, but not in any children of gj.<BR>
#                <BR>
#             examples are given in test_ProximalGraph.m<BR>
#             <BR>
#       return_val_loss:     <BR>
#               if true the function will return a tuple of matrices.<BR>
#       lambda1:  (regularization parameter)<BR>
#       regul: (choice of regularization, see above)<BR>
#       lambda2:  (optional, regularization parameter)<BR>
#       lambda3:  (optional, regularization parameter)<BR>
#       verbose: (optional, verbosity level, false by default)<BR>
#       intercept: (optional, last row of U is not regularized,<BR>
#         false by default)<BR>
#       pos: (optional, adds positivity constraints on the<BR>
#         coefficients, false by default)<BR>
#       numThreads: (optional, number of threads for exploiting<BR>
#         multi-core / multi-cpus. By default, it takes the value -1,<BR>
#         which automatically selects all the available CPUs/cores).<BR>
#       resetflow:    undocumented; modify at your own risks!<BR>
#       clever:    undocumented; modify at your own risks!<BR>
#       size_group:    undocumented; modify at your own risks!<BR>
#       transpose:    undocumented; modify at your own risks!<BR>
#<BR>
# Output:<BR>
#       V: double p x n matrix (output coefficients)<BR>
#       val_regularizer: double 1 x n vector (value of the regularization<BR>
#       term at the optimum).<BR>
#       val_loss:        vector of size U.shape[1]<BR>
#         alpha = spams.proximalGraph(U,graph,return_val_loss = False,...)<BR>
#         (alpha,val_loss) = spams.proximalGraph(U,graph,return_val_loss = True,...)<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2010 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#<BR>
# Note:<BR>
#     Valid values for the regularization parameter (regul) are:<BR>
#       "l0", "l1", "l2", "linf", "l2-not-squared", "elastic-net", "fused-lasso",<BR>
#       "group-lasso-l2", "group-lasso-linf", "sparse-group-lasso-l2",<BR>
#       "sparse-group-lasso-linf", "l1l2", "l1linf", "l1l2+l1", "l1linf+l1",<BR>
#       "tree-l0", "tree-l2", "tree-linf", "graph", "graph-ridge", "graph-l2",<BR>
#       "multi-task-tree", "multi-task-graph", "l1linf-row-column", "trace-norm",<BR>
#       "trace-norm-vec", "rank", "rank-vec", "none"<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
np.random.seed(0)<BR>
lambda1 = 0.1 <FONT COLOR="#007F00"># regularization parameter</FONT><BR>
num_threads = -1 <FONT COLOR="#007F00"># all cores (-1 by default)</FONT><BR>
verbose = True   <FONT COLOR="#007F00"># verbosity, false by default</FONT><BR>
pos = False       <FONT COLOR="#007F00"># can be used with all the other regularizations</FONT><BR>
intercept = False <FONT COLOR="#007F00"># can be used with all the other regularizations     </FONT><BR>
<BR>
U = np.asfortranarray(np.random.normal(size = (10,100)))<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'First graph example'</FONT><BR>
<FONT COLOR="#007F00"># Example 1 of graph structure<BR>
# groups:<BR>
# g1= {0 1 2 3}<BR>
# g2= {3 4 5 6}<BR>
# g3= {6 7 8 9}</FONT><BR>
eta_g = np.array([1, 1, 1],dtype=np.float64)<BR>
groups = ssp.csc_matrix(np.zeros((3,3)),dtype = np.bool)<BR>
groups_var = ssp.csc_matrix(<BR>
    np.array([[1, 0, 0],<BR>
              [1, 0, 0],<BR>
              [1, 0, 0],<BR>
              [1, 1, 0],<BR>
              [0, 1, 0],<BR>
              [0, 1, 0],<BR>
              [0, 1, 1],<BR>
              [0, 0, 1],<BR>
              [0, 0, 1],<BR>
              [0, 0, 1]],dtype=np.bool),dtype=np.bool)<BR>
graph = {<FONT COLOR="red">'eta_g'</FONT>: eta_g,<FONT COLOR="red">'groups'</FONT> : groups,<FONT COLOR="red">'groups_var'</FONT> : groups_var}<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\ntest prox graph'</FONT><BR>
regul=<FONT COLOR="red">'graph'</FONT><BR>
alpha = spams.proximalGraph(U,graph,False,lambda1 = lambda1,numThreads  = num_threads ,verbose = verbose,pos = pos,intercept = intercept,regul = regul)<BR>
<BR>
<FONT COLOR="#007F00"># Example 2 of graph structure<BR>
# groups:<BR>
# g1= {0 1 2 3}<BR>
# g2= {3 4 5 6}<BR>
# g3= {6 7 8 9}<BR>
# g4= {0 1 2 3 4 5}<BR>
# g5= {6 7 8}</FONT><BR>
eta_g = np.array([1, 1, 1, 1, 1],dtype=np.float64)<BR>
groups = ssp.csc_matrix(<BR>
    np.array([[0, 0, 0, 1, 0],<BR>
              [0, 0, 0, 0, 0],<BR>
              [0, 0, 0, 0, 0],<BR>
              [0, 0, 0, 0, 0],<BR>
              [0, 0, 1, 0, 0]],dtype=np.bool),dtype=np.bool)<BR>
<BR>
groups_var = ssp.csc_matrix(<BR>
    np.array([[1, 0, 0, 0, 0],<BR>
              [1, 0, 0, 0, 0],<BR>
              [1, 0, 0, 0, 0],<BR>
              [1, 1, 0, 0, 0],<BR>
              [0, 1, 0, 1, 0],<BR>
              [0, 1, 0, 1, 0],<BR>
              [0, 1, 0, 0, 1],<BR>
              [0, 0, 0, 0, 1],<BR>
              [0, 0, 0, 0, 1],<BR>
              [0, 0, 1, 0, 0]],dtype=np.bool),dtype=np.bool)<BR>
graph = {<FONT COLOR="red">'eta_g'</FONT>: eta_g,<FONT COLOR="red">'groups'</FONT> : groups,<FONT COLOR="red">'groups_var'</FONT> : groups_var}<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\ntest prox graph'</FONT><BR>
alpha = spams.proximalGraph(U,graph,False,lambda1 = lambda1,numThreads  = num_threads ,verbose = verbose,pos = pos,intercept = intercept,regul = regul)<BR>
<FONT COLOR="#007F00">#</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\ntest prox multi-task-graph'</FONT><BR>
regul = <FONT COLOR="red">'multi-task-graph'</FONT><BR>
lambda2 = 0.1<BR>
alpha = spams.proximalGraph(U,graph,False,lambda1 = lambda1,lambda2 = lambda2,numThreads  = num_threads ,verbose = verbose,pos = pos,intercept = intercept,regul = regul)<BR>
<FONT COLOR="#007F00">#</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\ntest no regularization'</FONT><BR>
regul = <FONT COLOR="red">'none'</FONT><BR>
alpha = spams.proximalGraph(U,graph,False,lambda1 = lambda1,lambda2 = lambda2,numThreads  = num_threads ,verbose = verbose,pos = pos,intercept = intercept,regul = regul)</TD></TR>
</TABLE><!--TOC subsection Function spams.proximalPathCoding-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc23">5.5</A>  Function spams.proximalPathCoding</H3><!--SEC END --><P>
This function computes the proximal operators associated to the path coding penalties of [<A HREF="#mairal14">23</A>]. 
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# The python function is not yet implemented.<BR>
#</FONT></TD></TR>
</TABLE><P>
This function is associated to a function to evaluate the penalties:
</P><!--TOC subsection Function spams.evalPathCoding-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc24">5.6</A>  Function spams.evalPathCoding</H3><!--SEC END --><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# The python function is not yet implemented.<BR>
#</FONT></TD></TR>
</TABLE><P>After having presented the regularization terms which our software can handle,
we present the various formulations that we address
</P><!--TOC subsection Problems Addressed-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc25">5.7</A>  Problems Addressed</H3><!--SEC END --><P>
We present here regression or classification formulations and their multi-task variants.
</P><!--TOC subsubsection Regression Problems with the Square Loss-->
<H4 CLASS="subsubsection"><!--SEC ANCHOR --><A NAME="htoc26">5.7.1</A>  Regression Problems with the Square Loss</H4><!--SEC END --><P> Given a training set {<I><B>x</B><SUP>i</SUP></I>,<I>y<SUB>i</SUB></I>}<SUB><I>i</I>=1</SUB><I><SUP>n</SUP></I>, with <I><B>x</B><SUP>i</SUP></I> ∈ ℝ<I><SUP>p</SUP></I> and <I>y<SUB>i</SUB></I> ∈ ℝ for all <I>i</I> in [ 1;<I>n</I> ], we address
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>w</B></I> ∈ ℝ<I><SUP>p</SUP></I>, <I>b</I> ∈ ℝ</TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">(<I>y<SUB>i</SUB></I>−<I><B>w</B></I><SUP>⊤</SUP><I><B>x</B><SUP>i</SUP></I>−<I>b</I>)<SUP>2</SUP> + λψ(<I><B>w</B></I>),
</TD></TR>
</TABLE><P>
where <I>b</I> is an optional variable acting as an “intercept”, which is not regularized, and ψ
can be any of the regularization functions presented above. 
Let us consider the vector <I><B>y</B></I> in ℝ<I><SUP>n</SUP></I> that carries the entries <I>y<SUB>i</SUB></I>. 
The problem without the intercept takes the following form, which we have already
encountered in the previous toolbox, but with different notations:
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>w</B></I> ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>y</B></I>−<B><I>Xw</I></B>||<SUB>2</SUB><SUP>2</SUP> + λψ(<I><B>w</B></I>),
</TD></TR>
</TABLE><P>
where the <I><B>X</B></I>=[<I><B>x</B><SUP>i</SUP></I>,…,<I><B>x</B><SUP>n</SUP></I>]<I><SUP>T</SUP></I> (the <I><B>x</B><SUP>i</SUP></I>’s are here the rows of <I><B>X</B></I>).
</P><!--TOC subsubsection Classification Problems with the Logistic Loss-->
<H4 CLASS="subsubsection"><!--SEC ANCHOR --><A NAME="htoc27">5.7.2</A>  Classification Problems with the Logistic Loss</H4><!--SEC END --><P>
The next formulation that our software can solve is the regularized logistic regression formulation.
We are again given a training set {<I><B>x</B><SUP>i</SUP></I>,<I>y<SUB>i</SUB></I>}<SUB><I>i</I>=1</SUB><I><SUP>n</SUP></I>, with <I><B>x</B><SUP>i</SUP></I> ∈
ℝ<I><SUP>p</SUP></I>, but the variables <I>y<SUB>i</SUB></I> are now in {−1,+1} for all <I>i</I> in
[ 1;<I>n</I> ]. The optimization problem we address is
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">   </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>w</B></I> ∈ ℝ<I><SUP>p</SUP></I>, <I>b</I> ∈ ℝ</TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
</TABLE></TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> log(1+<I>e</I><SUP>−<I>y<SUB>i</SUB></I>(<B><I>w</I></B>⊤<I><B>x</B>i</I>+<I>b</I>)</SUP> + λψ(<I><B>w</B></I>),
</TD></TR>
</TABLE><P>
with again ψ taken to be one of the regularization function presented above, and <I>b</I> is an optional intercept.
</P><!--TOC subsubsection Multi-class Classification Problems with the Softmax Loss-->
<H4 CLASS="subsubsection"><!--SEC ANCHOR --><A NAME="htoc28">5.7.3</A>  Multi-class Classification Problems with the Softmax Loss</H4><!--SEC END --><P>
We have also implemented a multi-class logistic classifier (or softmax).
For a classification problem with <I>r</I> classes, we are given a training set {<I><B>x</B><SUP>i</SUP></I>,<I>y<SUB>i</SUB></I>}<SUB><I>i</I>=1</SUB><I><SUP>n</SUP></I>, where the variables <I><B>x</B><SUP>i</SUP></I> are still vectors in ℝ<I><SUP>p</SUP></I>, but the <I>y<SUB>i</SUB></I>’s have integer values in {1,2,…,<I>r</I>}. The formulation we address is the following multi-class learning problem
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>W</B></I> ∈ ℝ<SUP><I>p</I> × <I>r</I></SUP>, <I><B>b</B></I> ∈ ℝ<I><SUP>r</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
</TABLE></TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> log</TD><TD CLASS="dcell">⎛<BR>
⎜<BR>
⎝</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>r</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>j</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> <I>e</I><SUP> (<I><B>w</B>j</I>−<I><B>wy</B><SUB>i</SUB></I>)⊤<I><B>x</B>i</I> + <I><B>b</B><SUB>j</SUB></I>−<I><B>b</B><SUB><B>y</B>i</SUB></I></SUP></TD><TD CLASS="dcell">⎞<BR>
⎟<BR>
⎠</TD><TD CLASS="dcell">+ λ</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>r</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>j</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell">ψ(<I><B>w</B><SUP>j</SUP></I>),<A NAME="software:eq:class"></A>
    (46)</TD></TR>
</TABLE><P>
where <I><B>W</B></I> = [<I><B>w</B></I><SUP>1</SUP>,…,<I><B>w</B><SUP>r</SUP></I>] and the optional vector <I><B>b</B></I> in ℝ<I><SUP>r</SUP></I> carries intercepts for each class.
</P><!--TOC subsubsection Multi-task Regression Problems with the Square Loss-->
<H4 CLASS="subsubsection"><!--SEC ANCHOR --><A NAME="htoc29">5.7.4</A>  Multi-task Regression Problems with the Square Loss</H4><!--SEC END --><P>
We are now considering a problem with <I>r</I> tasks, and a training set
{<I><B>x</B><SUP>i</SUP></I>,<I><B>y</B><SUP>i</SUP></I>}<SUB><I>i</I>=1</SUB><I><SUP>n</SUP></I>, where the variables <I><B>x</B><SUP>i</SUP></I> are still vectors in ℝ<I><SUP>p</SUP></I>, and <I><B>y</B><SUP>i</SUP></I>
is a vector in ℝ<I><SUP>r</SUP></I>. We are looking for <I>r</I> regression vectors <I><B>w</B><SUP>j</SUP></I>, for <I>j</I>∈ [ 1;<I>r</I> ], or equivalently for a matrix <I><B>W</B></I>=[<I><B>w</B></I><SUP>1</SUP>,…,<I><B>w</B><SUP>r</SUP></I>] in ℝ<SUP><I>p</I> × <I>r</I></SUP>. The formulation we address is the following
multi-task regression problem
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">         </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>W</B></I> ∈ ℝ<SUP><I>p</I> × <I>r</I></SUP>, <I><B>b</B></I> ∈ ℝ<I><SUP>r</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>r</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>j</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">(<I><B>y</B><SUB>j</SUB><SUP>i</SUP></I>−<I><B>w</B></I><SUP>⊤</SUP><I><B>x</B><SUP>i</SUP></I>−<I><B>b</B><SUB>j</SUB></I>)<SUP>2</SUP> + λψ(<I><B>W</B></I>),
</TD></TR>
</TABLE><P>
where ψ is any of the regularization function on matrices we have presented in the previous section.
Note that by introducing the appropriate variables <I><B>Y</B></I>, the problem without intercept could be equivalently rewritten
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">   </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>W</B></I> ∈ ℝ<SUP><I>p</I> × <I>r</I></SUP></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell"> ||<I><B>Y</B></I>−<B><I>XW</I></B>||<SUB>F</SUB><SUP>2</SUP> + λψ(<I><B>W</B></I>).
</TD></TR>
</TABLE><!--TOC subsubsection Multi-task Classification Problems with the Logistic Loss-->
<H4 CLASS="subsubsection"><!--SEC ANCHOR --><A NAME="htoc30">5.7.5</A>  Multi-task Classification Problems with the Logistic Loss</H4><!--SEC END --><P>
The multi-task version of the logistic regression follows the same principle.
We consider <I>r</I> tasks, and a training set
{<I><B>x</B><SUP>i</SUP></I>,<I><B>y</B><SUP>i</SUP></I>}<SUB><I>i</I>=1</SUB><I><SUP>n</SUP></I>, with the <I><B>x</B><SUP>i</SUP></I>’s in ℝ<I><SUP>p</SUP></I>, and the <I><B>y</B><SUP>i</SUP></I>’s
are vectors in {−1,+1}<I><SUP>r</SUP></I>. We look for a matrix <I><B>W</B></I>=[<I><B>w</B></I><SUP>1</SUP>,…,<I><B>w</B><SUP>r</SUP></I>] in ℝ<SUP><I>p</I> × <I>r</I></SUP>. The formulation is the following
multi-task regression problem
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">   </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>W</B></I> ∈ ℝ<SUP><I>p</I> × <I>r</I></SUP>, <I><B>b</B></I> ∈ ℝ<I><SUP>r</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>r</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>j</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
</TABLE></TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> log</TD><TD CLASS="dcell">⎛<BR>
⎜<BR>
⎝</TD><TD CLASS="dcell">1+<I>e</I><SUP>−<I><B>y</B><SUB>j</SUB>i</I>(<B><I>w</I></B>⊤<I><B>x</B>i</I>+<I><B>b</B><SUB>j</SUB></I>)</SUP></TD><TD CLASS="dcell">⎞<BR>
⎟<BR>
⎠</TD><TD CLASS="dcell">+ λψ(<I><B>W</B></I>).
</TD></TR>
</TABLE><!--TOC subsubsection Multi-task and Multi-class Classification Problems with the Softmax Loss-->
<H4 CLASS="subsubsection"><!--SEC ANCHOR --><A NAME="htoc31">5.7.6</A>  Multi-task and Multi-class Classification Problems with the Softmax Loss</H4><!--SEC END --><P>
The multi-task/multi-class version directly follows from the formulation of Eq. (<A HREF="#software:eq:class">46</A>), but associates with each class a task, and as a consequence, regularizes the matrix <I><B>W</B></I> in a particular way:
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">   </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>W</B></I> ∈ ℝ<SUP><I>p</I> × <I>r</I></SUP>, <I><B>b</B></I> ∈ ℝ<I><SUP>r</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
</TABLE></TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> log</TD><TD CLASS="dcell">⎛<BR>
⎜<BR>
⎝</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>r</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>j</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> <I>e</I><SUP> (<I><B>w</B>j</I>−<I><B>wy</B><SUB>i</SUB></I>)⊤<I><B>x</B>i</I> + <I><B>b</B><SUB>j</SUB></I>−<I><B>b</B><SUB><B>y</B>i</SUB></I></SUP></TD><TD CLASS="dcell">⎞<BR>
⎟<BR>
⎠</TD><TD CLASS="dcell">+ λψ(<I><B>W</B></I>).
</TD></TR>
</TABLE><P>
How duality gaps are computed for any of these formulations is presented in Appendix <A HREF="#appendix">A</A>.
We now present the main functions for solving these problems</P><!--TOC subsection Function spams.fistaFlat-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc32">5.8</A>  Function spams.fistaFlat</H3><!--SEC END --><P>
Given a matrix <I><B>X</B></I>=[<I><B>x</B></I><SUP>1</SUP>,…,<I><B>x</B><SUP>p</SUP></I>]<I><SUP>T</SUP></I> in ℝ<SUP><I>m</I> × <I>p</I></SUP>, and a matrix <I><B>Y</B></I>=[<I><B>y</B></I><SUP>1</SUP>,…,<I><B>y</B><SUP>n</SUP></I>], it solves the optimization problems presented in the previous section, with the same regularization functions as spams.proximalFlat.
see usage details below:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: fistaFlat<BR>
#<BR>
# Usage: spams.fistaFlat(Y,X,W0,return_optim_info = False,numThreads =-1,max_it =1000,L0=1.0,<BR>
#                 fixed_step=False,gamma=1.5,lambda1=1.0,delta=1.0,lambda2=0.,lambda3=0.,<BR>
#                 a=1.0,b=0.,c=1.0,tol=0.000001,it0=100,max_iter_backtracking=1000,<BR>
#                 compute_gram=False,lin_admm=False,admm=False,intercept=False,<BR>
#                 resetflow=False,regul="",loss="",verbose=False,pos=False,clever=False,<BR>
#                 log=False,ista=False,subgrad=False,logName="",is_inner_weights=False,<BR>
#                 inner_weights=np.array([0.]),size_group=1,groups = None,sqrt_step=True,<BR>
#                 transpose=False)<BR>
#<BR>
# Description:<BR>
#     fistaFlat solves sparse regularized problems.<BR>
#         X is a design matrix of size m x p<BR>
#         X=[x^1,...,x^n]', where the x_i's are the rows of X<BR>
#         Y=[y^1,...,y^n] is a matrix of size m x n<BR>
#         It implements the algorithms FISTA, ISTA and subgradient descent.<BR>
#         <BR>
#           - if loss='square' and regul is a regularization function for vectors,<BR>
#             the entries of Y are real-valued,  W = [w^1,...,w^n] is a matrix of size p x n<BR>
#             For all column y of Y, it computes a column w of W such that<BR>
#               w = argmin 0.5||y- X w||_2^2 + lambda1 psi(w)<BR>
#               <BR>
#           - if loss='square' and regul is a regularization function for matrices<BR>
#             the entries of Y are real-valued,  W is a matrix of size p x n. <BR>
#             It computes the matrix W such that<BR>
#               W = argmin 0.5||Y- X W||_F^2 + lambda1 psi(W)<BR>
#               <BR>
#           - loss='square-missing' same as loss='square', but handles missing data<BR>
#             represented by NaN (not a number) in the matrix Y<BR>
#             <BR>
#           - if loss='logistic' and regul is a regularization function for vectors,<BR>
#             the entries of Y are either -1 or +1, W = [w^1,...,w^n] is a matrix of size p x n<BR>
#             For all column y of Y, it computes a column w of W such that<BR>
#               w = argmin (1/m)sum_{j=1}^m log(1+e^(-y_j x^j' w)) + lambda1 psi(w),<BR>
#             where x^j is the j-th row of X.<BR>
#             <BR>
#           - if loss='logistic' and regul is a regularization function for matrices<BR>
#             the entries of Y are either -1 or +1, W is a matrix of size p x n<BR>
#               W = argmin sum_{i=1}^n(1/m)sum_{j=1}^m log(1+e^(-y^i_j x^j' w^i)) + lambda1 psi(W)<BR>
#               <BR>
#           - if loss='multi-logistic' and regul is a regularization function for vectors,<BR>
#             the entries of Y are in {0,1,...,N} where N is the total number of classes<BR>
#             W = [W^1,...,W^n] is a matrix of size p x Nn, each submatrix W^i is of size p x N<BR>
#             for all submatrix WW of W, and column y of Y, it computes<BR>
#               WW = argmin (1/m)sum_{j=1}^m log(sum_{j=1}^r e^(x^j'(ww^j-ww^{y_j}))) + lambda1 sum_{j=1}^N psi(ww^j),<BR>
#             where ww^j is the j-th column of WW.<BR>
#             <BR>
#           - if loss='multi-logistic' and regul is a regularization function for matrices,<BR>
#             the entries of Y are in {0,1,...,N} where N is the total number of classes<BR>
#             W is a matrix of size p x N, it computes<BR>
#               W = argmin (1/m)sum_{j=1}^m log(sum_{j=1}^r e^(x^j'(w^j-w^{y_j}))) + lambda1 psi(W)<BR>
#             where ww^j is the j-th column of WW.<BR>
#             <BR>
#           - loss='cur' useful to perform sparse CUR matrix decompositions, <BR>
#               W = argmin 0.5||Y-X*W*X||_F^2 + lambda1 psi(W)<BR>
#               <BR>
#               <BR>
#         The function psi are those used by proximalFlat (see documentation)<BR>
#         <BR>
#         This function can also handle intercepts (last row of W is not regularized),<BR>
#         and/or non-negativity constraints on W, and sparse matrices for X<BR>
#<BR>
# Inputs:<BR>
#       Y:  double dense m x n matrix<BR>
#       X:  double dense or sparse m x p matrix   <BR>
#       W0:  double dense p x n matrix or p x Nn matrix (for multi-logistic loss)<BR>
#            initial guess<BR>
#       return_optim_info:     <BR>
#               if true the function will return a tuple of matrices.<BR>
#       loss: (choice of loss, see above)<BR>
#       regul: (choice of regularization, see function proximalFlat)<BR>
#       lambda1: (regularization parameter)<BR>
#       lambda2: (optional, regularization parameter, 0 by default)<BR>
#       lambda3: (optional, regularization parameter, 0 by default)<BR>
#       verbose: (optional, verbosity level, false by default)<BR>
#       pos: (optional, adds positivity constraints on the<BR>
#           coefficients, false by default)<BR>
#       transpose: (optional, transpose the matrix in the regularization function)<BR>
#       size_group: (optional, for regularization functions assuming a group<BR>
#            structure)<BR>
#       groups: (int32, optional, for regularization functions assuming a group<BR>
#            structure, see proximalFlat)<BR>
#       numThreads: (optional, number of threads for exploiting<BR>
#           multi-core / multi-cpus. By default, it takes the value -1,<BR>
#           which automatically selects all the available CPUs/cores).<BR>
#       max_it: (optional, maximum number of iterations, 100 by default)<BR>
#       it0: (optional, frequency for computing duality gap, every 10 iterations by default)<BR>
#       tol: (optional, tolerance for stopping criteration, which is a relative duality gap<BR>
#           if it is available, or a relative change of parameters).<BR>
#       gamma: (optional, multiplier for increasing the parameter L in fista, 1.5 by default)<BR>
#       L0: (optional, initial parameter L in fista, 0.1 by default, should be small enough)<BR>
#       fixed_step: (deactive the line search for L in fista and use L0 instead)<BR>
#       compute_gram: (optional, pre-compute X^TX, false by default).<BR>
#       intercept: (optional, do not regularize last row of W, false by default).<BR>
#       ista: (optional, use ista instead of fista, false by default).<BR>
#       subgrad: (optional, if not ista, use subradient descent instead of fista, false by default).<BR>
#       a: <BR>
#       b: (optional, if subgrad, the gradient step is a/(t+b)<BR>
#       also similar options as proximalFlat<BR>
#       <BR>
#       the function also implements the ADMM algorithm via an option admm=true. It is not documented<BR>
#       and you need to look at the source code to use it.<BR>
#       delta:    undocumented; modify at your own risks!<BR>
#       c:    undocumented; modify at your own risks!<BR>
#       max_iter_backtracking:    undocumented; modify at your own risks!<BR>
#       lin_admm:    undocumented; modify at your own risks!<BR>
#       admm:    undocumented; modify at your own risks!<BR>
#       resetflow:    undocumented; modify at your own risks!<BR>
#       clever:    undocumented; modify at your own risks!<BR>
#       log:    undocumented; modify at your own risks!<BR>
#       logName:    undocumented; modify at your own risks!<BR>
#       is_inner_weights:    undocumented; modify at your own risks!<BR>
#       inner_weights:    undocumented; modify at your own risks!<BR>
#       sqrt_step:    undocumented; modify at your own risks!<BR>
#<BR>
# Output:<BR>
#       W:  double dense p x n matrix or p x Nn matrix (for multi-logistic loss)<BR>
#       optim: optional, double dense 4 x n matrix.<BR>
#           first row: values of the objective functions.<BR>
#           third row: values of the relative duality gap (if available)<BR>
#           fourth row: number of iterations<BR>
#       optim_info:        vector of size 4, containing information of the optimization.<BR>
#             W = spams.fistaFlat(Y,X,W0,return_optim_info = False,...)<BR>
#             (W,optim_info) = spams.fistaFlat(Y,X,W0,return_optim_info = True,...)<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2010 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#<BR>
# Note:<BR>
#     Valid values for the regularization parameter (regul) are:<BR>
#       "l0", "l1", "l2", "linf", "l2-not-squared", "elastic-net", "fused-lasso",<BR>
#       "group-lasso-l2", "group-lasso-linf", "sparse-group-lasso-l2",<BR>
#       "sparse-group-lasso-linf", "l1l2", "l1linf", "l1l2+l1", "l1linf+l1",<BR>
#       "tree-l0", "tree-l2", "tree-linf", "graph", "graph-ridge", "graph-l2",<BR>
#       "multi-task-tree", "multi-task-graph", "l1linf-row-column", "trace-norm",<BR>
#       "trace-norm-vec", "rank", "rank-vec", "none"<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
param = {<FONT COLOR="red">'numThreads'</FONT> : 1,<FONT COLOR="red">'verbose'</FONT> : True,<BR>
         <FONT COLOR="red">'lambda1'</FONT> : 0.05, <FONT COLOR="red">'it0'</FONT> : 10, <FONT COLOR="red">'max_it'</FONT> : 200,<BR>
         <FONT COLOR="red">'L0'</FONT> : 0.1, <FONT COLOR="red">'tol'</FONT> : 1e-3, <FONT COLOR="red">'intercept'</FONT> : False,<BR>
         <FONT COLOR="red">'pos'</FONT> : False}<BR>
np.random.seed(0)<BR>
m = 100;n = 200<BR>
X = np.asfortranarray(np.random.normal(size = (m,n)))<BR>
X = np.asfortranarray(X - np.tile(np.mean(X,0),(X.shape[0],1)))<BR>
X = spams.normalize(X)<BR>
Y = np.asfortranarray(np.random.normal(size = (m,1)))<BR>
Y = np.asfortranarray(Y - np.tile(np.mean(Y,0),(Y.shape[0],1)))<BR>
Y = spams.normalize(Y)<BR>
W0 = np.zeros((X.shape[1],Y.shape[1]),dtype=np.float64,order=<FONT COLOR="red">"FORTRAN"</FONT>)<BR>
<FONT COLOR="#007F00"># Regression experiments <BR>
# 100 regression problems with the same design matrix X.</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nVarious regression experiments'</FONT><BR>
param[<FONT COLOR="red">'compute_gram'</FONT>] = True<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression l1'</FONT><BR>
param[<FONT COLOR="red">'loss'</FONT>] = <FONT COLOR="red">'square'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l1'</FONT><BR>
<FONT COLOR="#007F00"># param.regul='group-lasso-l2';<BR>
# param.size_group=10;</FONT><BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<FONT COLOR="#007F00">##    print "XX %s" %str(optim_info.shape);return None</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:],0),np.mean(optim_info[2,:],0),np.mean(optim_info[3,:],0))<BR>
<FONT COLOR="#007F00">###</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nISTA + Regression l1'</FONT><BR>
param[<FONT COLOR="red">'ista'</FONT>] = True<BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f\n'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),np.mean(optim_info[3,:]))<BR>
<FONT COLOR="#007F00">##</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nSubgradient Descent + Regression l1'</FONT><BR>
param[<FONT COLOR="red">'ista'</FONT>] = False<BR>
param[<FONT COLOR="red">'subgrad'</FONT>] = True<BR>
param[<FONT COLOR="red">'a'</FONT>] = 0.1<BR>
param[<FONT COLOR="red">'b'</FONT>] = 1000 <FONT COLOR="#007F00"># arbitrary parameters</FONT><BR>
max_it = param[<FONT COLOR="red">'max_it'</FONT>]<BR>
it0 = param[<FONT COLOR="red">'it0'</FONT>]<BR>
param[<FONT COLOR="red">'max_it'</FONT>] = 500<BR>
param[<FONT COLOR="red">'it0'</FONT>] = 50<BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f\n'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),np.mean(optim_info[3,:]))<BR>
param[<FONT COLOR="red">'subgrad'</FONT>] = False<BR>
param[<FONT COLOR="red">'max_it'</FONT>] = max_it<BR>
param[<FONT COLOR="red">'it0'</FONT>] = it0<BR>
<BR>
<FONT COLOR="#007F00">###</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression l2'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l2'</FONT><BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f\n'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),np.mean(optim_info[3,:]))<BR>
<FONT COLOR="#007F00">###</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression l2 + sparse feature matrix'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l2'</FONT>;<BR>
(W, optim_info) = spams.fistaFlat(Y,ssp.csc_matrix(X),W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),np.mean(optim_info[3,:]))<BR>
<FONT COLOR="#007F00">###########</FONT><BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression Elastic-Net'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'elastic-net'</FONT><BR>
param[<FONT COLOR="red">'lambda2'</FONT>] = 0.1<BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[3,:]))<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Group Lasso L2'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'group-lasso-l2'</FONT><BR>
param[<FONT COLOR="red">'size_group'</FONT>] = 2<BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:],0),np.mean(optim_info[2,:],0),np.mean(optim_info[3,:],0))<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Group Lasso L2 with variable size of groups'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'group-lasso-l2'</FONT><BR>
param2=param.copy()<BR>
param2[<FONT COLOR="red">'groups'</FONT>] = np.array(np.random.random_integers(1,5,X.shape[1]),dtype = np.int32)<BR>
param2[<FONT COLOR="red">'lambda1'</FONT>] *= 10<BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:],0),np.mean(optim_info[2,:],0),np.mean(optim_info[3,:],0))<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Trace Norm'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'trace-norm-vec'</FONT><BR>
param[<FONT COLOR="red">'size_group'</FONT>] = 5<BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:],0),np.mean(optim_info[3,:]))<BR>
<BR>
<FONT COLOR="#007F00">####    </FONT><BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression Fused-Lasso'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'fused-lasso'</FONT><BR>
param[<FONT COLOR="red">'lambda2'</FONT>] = 0.1<BR>
param[<FONT COLOR="red">'lambda3'</FONT>] = 0.1; <FONT COLOR="#007F00">#</FONT><BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[3,:]))<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression no regularization'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'none'</FONT><BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[3,:]))<BR>
<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression l1 with intercept '</FONT><BR>
param[<FONT COLOR="red">'intercept'</FONT>] = True<BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l1'</FONT><BR>
x1 = np.asfortranarray(np.concatenate((X,np.ones((X.shape[0],1))),1))<BR>
W01 = np.asfortranarray(np.concatenate((W0,np.zeros((1,W0.shape[1]))),0))<BR>
(W, optim_info) = spams.fistaFlat(Y,x1,W01,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),np.mean(optim_info[3,:]))<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression l1 with intercept+ non-negative '</FONT><BR>
param[<FONT COLOR="red">'pos'</FONT>] = True<BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l1'</FONT><BR>
x1 = np.asfortranarray(np.concatenate((X,np.ones((X.shape[0],1))),1))<BR>
W01 = np.asfortranarray(np.concatenate((W0,np.zeros((1,W0.shape[1]))),0))<BR>
(W, optim_info) = spams.fistaFlat(Y,x1,W01,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[3,:]))<BR>
param[<FONT COLOR="red">'pos'</FONT>] = False<BR>
param[<FONT COLOR="red">'intercept'</FONT>] = False<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nISTA + Regression l0'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l0'</FONT><BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[3,:]))<BR>
<BR>
<FONT COLOR="#007F00"># Classification</FONT><BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nOne classification experiment'</FONT><BR>
Y = np.asfortranarray(2 * np.asarray(np.random.normal(size = (100,1)) &gt; 0,dtype=<FONT COLOR="red">'float64'</FONT>) - 1)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Logistic l1'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l1'</FONT><BR>
param[<FONT COLOR="red">'loss'</FONT>] = <FONT COLOR="red">'logistic'</FONT><BR>
param[<FONT COLOR="red">'lambda1'</FONT>] = 0.01<BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),np.mean(optim_info[3,:]))<BR>
<FONT COLOR="#007F00"># can be used of course with other regularization functions, intercept,...</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l1'</FONT><BR>
param[<FONT COLOR="red">'loss'</FONT>] = <FONT COLOR="red">'weighted-logistic'</FONT><BR>
param[<FONT COLOR="red">'lambda1'</FONT>] = 0.01<BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),np.mean(optim_info[3,:]))<BR>
<FONT COLOR="#007F00"># can be used of course with other regularization functions, intercept,...</FONT><BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Logistic l1 + sparse matrix'</FONT><BR>
param[<FONT COLOR="red">'loss'</FONT>] = <FONT COLOR="red">'logistic'</FONT><BR>
(W, optim_info) = spams.fistaFlat(Y,ssp.csc_matrix(X),W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),np.mean(optim_info[3,:]))<BR>
<FONT COLOR="#007F00"># can be used of course with other regularization functions, intercept,...<BR>
<BR>
<BR>
# Multi-Class classification</FONT><BR>
Y = np.asfortranarray(np.ceil(5 * np.random.random(size = (100,1000))) - 1)<BR>
param[<FONT COLOR="red">'loss'</FONT>] = <FONT COLOR="red">'multi-logistic'</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Multi-Class Logistic l1'</FONT><BR>
nclasses = np.max(Y[:])+1<BR>
W0 = np.zeros((X.shape[1],nclasses * Y.shape[1]),dtype=np.float64,order=<FONT COLOR="red">"FORTRAN"</FONT>)<BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),np.mean(optim_info[3,:]))<BR>
<FONT COLOR="#007F00"># can be used of course with other regularization functions, intercept,...<BR>
<BR>
<BR>
# Multi-Task regression</FONT><BR>
Y = np.asfortranarray(np.random.normal(size = (100,100)))<BR>
Y = np.asfortranarray(Y - np.tile(np.mean(Y,0),(Y.shape[0],1)))<BR>
Y = spams.normalize(Y)<BR>
param[<FONT COLOR="red">'compute_gram'</FONT>] = False<BR>
W0 = np.zeros((X.shape[1],Y.shape[1]),dtype=np.float64,order=<FONT COLOR="red">"FORTRAN"</FONT>)<BR>
param[<FONT COLOR="red">'loss'</FONT>] = <FONT COLOR="red">'square'</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression l1l2 '</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l1l2'</FONT><BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),np.mean(optim_info[3,:]))<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression l1linf '</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l1linf'</FONT><BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),np.mean(optim_info[3,:]))<BR>
<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression l1l2 + l1 '</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l1l2+l1'</FONT><BR>
param[<FONT COLOR="red">'lambda2'</FONT>] = 0.1<BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[3,:]))<BR>
<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression l1linf + l1 '</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l1linf+l1'</FONT><BR>
param[<FONT COLOR="red">'lambda2'</FONT>] = 0.1<BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[3,:]))<BR>
<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression l1linf + row + columns '</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l1linf-row-column'</FONT><BR>
param[<FONT COLOR="red">'lambda2'</FONT>] = 0.1<BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),np.mean(optim_info[3,:]))<BR>
<BR>
<FONT COLOR="#007F00"># Multi-Task Classification</FONT><BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Logistic + l1l2 '</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l1l2'</FONT><BR>
param[<FONT COLOR="red">'loss'</FONT>] = <FONT COLOR="red">'logistic'</FONT><BR>
Y = np.asfortranarray(2 * np.asarray(np.random.normal(size = (100,100)) &gt; 1,dtype=<FONT COLOR="red">'float64'</FONT>) - 1)<BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),np.mean(optim_info[3,:]))<BR>
<FONT COLOR="#007F00"># Multi-Class + Multi-Task Regularization</FONT><BR>
<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Multi-Class Logistic l1l2 '</FONT><BR>
Y = np.asfortranarray(np.ceil(5 * np.random.random(size = (100,1000))) - 1)<BR>
Y = spams.normalize(Y)<BR>
param[<FONT COLOR="red">'loss'</FONT>] = <FONT COLOR="red">'multi-logistic'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'l1l2'</FONT><BR>
nclasses = np.max(Y[:])+1<BR>
W0 = np.zeros((X.shape[1],nclasses * Y.shape[1]),dtype=np.float64,order=<FONT COLOR="red">"FORTRAN"</FONT>)<BR>
(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),np.mean(optim_info[3,:]))<BR>
<FONT COLOR="#007F00"># can be used of course with other regularization functions, intercept,...<BR>
<BR>
<BR>
#############</FONT></TD></TR>
</TABLE><!--TOC subsection Function spams.fistaTree-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc33">5.9</A>  Function spams.fistaTree</H3><!--SEC END --><P>
Given a matrix <I><B>X</B></I>=[<I><B>x</B></I><SUP>1</SUP>,…,<I><B>x</B><SUP>p</SUP></I>]<I><SUP>T</SUP></I> in ℝ<SUP><I>m</I> × <I>p</I></SUP>, and a matrix <I><B>Y</B></I>=[<I><B>y</B></I><SUP>1</SUP>,…,<I><B>y</B><SUP>n</SUP></I>], it solves the optimization problems presented in the previous section, with the same regularization functions as spams.proximalTree.
see usage details below:</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: fistaTree<BR>
#<BR>
# Usage: spams.fistaTree(Y,X,W0,tree,return_optim_info = False,numThreads =-1,max_it =1000,L0=1.0,<BR>
#                 fixed_step=False,gamma=1.5,lambda1=1.0,delta=1.0,lambda2=0.,lambda3=0.,<BR>
#                 a=1.0,b=0.,c=1.0,tol=0.000001,it0=100,max_iter_backtracking=1000,<BR>
#                 compute_gram=False,lin_admm=False,admm=False,intercept=False,<BR>
#                 resetflow=False,regul="",loss="",verbose=False,pos=False,clever=False,<BR>
#                 log=False,ista=False,subgrad=False,logName="",is_inner_weights=False,<BR>
#                 inner_weights=np.array([0.]),size_group=1,sqrt_step=True,transpose=False)<BR>
#<BR>
# Description:<BR>
#     fistaTree solves sparse regularized problems.<BR>
#         X is a design matrix of size m x p<BR>
#         X=[x^1,...,x^n]', where the x_i's are the rows of X<BR>
#         Y=[y^1,...,y^n] is a matrix of size m x n<BR>
#         It implements the algorithms FISTA, ISTA and subgradient descent for solving<BR>
#         <BR>
#           min_W  loss(W) + lambda1 psi(W)<BR>
#           <BR>
#         The function psi are those used by proximalTree (see documentation)<BR>
#         for the loss functions, see the documentation of fistaFlat<BR>
#         <BR>
#         This function can also handle intercepts (last row of W is not regularized),<BR>
#         and/or non-negativity constraints on W and sparse matrices X<BR>
#<BR>
# Inputs:<BR>
#       Y:  double dense m x n matrix<BR>
#       X:  double dense or sparse m x p matrix   <BR>
#       W0:  double dense p x n matrix or p x Nn matrix (for multi-logistic loss)<BR>
#            initial guess<BR>
#       tree: named list (see documentation of proximalTree)<BR>
#       return_optim_info:     <BR>
#               if true the function will return a tuple of matrices.<BR>
#       loss: (choice of loss, see above)<BR>
#       regul: (choice of regularization, see function proximalFlat)<BR>
#       lambda1: (regularization parameter)<BR>
#       lambda2: (optional, regularization parameter, 0 by default)<BR>
#       lambda3: (optional, regularization parameter, 0 by default)<BR>
#       verbose: (optional, verbosity level, false by default)<BR>
#       pos: (optional, adds positivity constraints on the<BR>
#           coefficients, false by default)<BR>
#       transpose: (optional, transpose the matrix in the regularization function)<BR>
#       size_group: (optional, for regularization functions assuming a group<BR>
#            structure)<BR>
#       numThreads: (optional, number of threads for exploiting<BR>
#           multi-core / multi-cpus. By default, it takes the value -1,<BR>
#           which automatically selects all the available CPUs/cores).<BR>
#       max_it: (optional, maximum number of iterations, 100 by default)<BR>
#       it0: (optional, frequency for computing duality gap, every 10 iterations by default)<BR>
#       tol: (optional, tolerance for stopping criteration, which is a relative duality gap<BR>
#           if it is available, or a relative change of parameters).<BR>
#       gamma: (optional, multiplier for increasing the parameter L in fista, 1.5 by default)<BR>
#       L0: (optional, initial parameter L in fista, 0.1 by default, should be small enough)<BR>
#       fixed_step: (deactive the line search for L in fista and use L0 instead)<BR>
#       compute_gram: (optional, pre-compute X^TX, false by default).<BR>
#       intercept: (optional, do not regularize last row of W, false by default).<BR>
#       ista: (optional, use ista instead of fista, false by default).<BR>
#       subgrad: (optional, if not ista, use subradient descent instead of fista, false by default).<BR>
#       a: <BR>
#       b: (optional, if subgrad, the gradient step is a/(t+b)<BR>
#       also similar options as proximalTree<BR>
#       <BR>
#       the function also implements the ADMM algorithm via an option admm=true. It is not documented<BR>
#       and you need to look at the source code to use it.<BR>
#       delta:    undocumented; modify at your own risks!<BR>
#       c:    undocumented; modify at your own risks!<BR>
#       max_iter_backtracking:    undocumented; modify at your own risks!<BR>
#       lin_admm:    undocumented; modify at your own risks!<BR>
#       admm:    undocumented; modify at your own risks!<BR>
#       resetflow:    undocumented; modify at your own risks!<BR>
#       clever:    undocumented; modify at your own risks!<BR>
#       log:    undocumented; modify at your own risks!<BR>
#       logName:    undocumented; modify at your own risks!<BR>
#       is_inner_weights:    undocumented; modify at your own risks!<BR>
#       inner_weights:    undocumented; modify at your own risks!<BR>
#       sqrt_step:    undocumented; modify at your own risks!<BR>
#<BR>
# Output:<BR>
#       W:  double dense p x n matrix or p x Nn matrix (for multi-logistic loss)<BR>
#       optim: optional, double dense 4 x n matrix.<BR>
#           first row: values of the objective functions.<BR>
#           third row: values of the relative duality gap (if available)<BR>
#           fourth row: number of iterations<BR>
#       optim_info:        vector of size 4, containing information of the optimization.<BR>
#             W = spams.fistaTree(Y,X,W0,tree,return_optim_info = False,...)<BR>
#             (W,optim_info) = spams.fistaTree(Y,X,W0,tree,return_optim_info = True,...)<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2010 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#<BR>
# Note:<BR>
#     Valid values for the regularization parameter (regul) are:<BR>
#       "l0", "l1", "l2", "linf", "l2-not-squared", "elastic-net", "fused-lasso",<BR>
#       "group-lasso-l2", "group-lasso-linf", "sparse-group-lasso-l2",<BR>
#       "sparse-group-lasso-linf", "l1l2", "l1linf", "l1l2+l1", "l1linf+l1",<BR>
#       "tree-l0", "tree-l2", "tree-linf", "graph", "graph-ridge", "graph-l2",<BR>
#       "multi-task-tree", "multi-task-graph", "l1linf-row-column", "trace-norm",<BR>
#       "trace-norm-vec", "rank", "rank-vec", "none"<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
param = {<FONT COLOR="red">'numThreads'</FONT> : -1,<FONT COLOR="red">'verbose'</FONT> : False,<BR>
         <FONT COLOR="red">'lambda1'</FONT> : 0.001, <FONT COLOR="red">'it0'</FONT> : 10, <FONT COLOR="red">'max_it'</FONT> : 200,<BR>
         <FONT COLOR="red">'L0'</FONT> : 0.1, <FONT COLOR="red">'tol'</FONT> : 1e-5, <FONT COLOR="red">'intercept'</FONT> : False,<BR>
         <FONT COLOR="red">'pos'</FONT> : False}<BR>
np.random.seed(0)<BR>
m = 100;n = 10<BR>
X = np.asfortranarray(np.random.normal(size = (m,n)))<BR>
X = np.asfortranarray(X - np.tile(np.mean(X,0),(X.shape[0],1)))<BR>
X = spams.normalize(X)<BR>
Y = np.asfortranarray(np.random.normal(size = (m,m)))<BR>
Y = np.asfortranarray(Y - np.tile(np.mean(Y,0),(Y.shape[0],1)))<BR>
Y = spams.normalize(Y)<BR>
W0 = np.zeros((X.shape[1],Y.shape[1]),dtype=np.float64,order=<FONT COLOR="red">"FORTRAN"</FONT>)<BR>
own_variables =  np.array([0,0,3,5,6,6,8,9],dtype=np.int32)<BR>
N_own_variables =  np.array([0,3,2,1,0,2,1,1],dtype=np.int32)<BR>
eta_g = np.array([1,1,1,2,2,2,2.5,2.5],dtype=np.float64)<BR>
groups = np.asfortranarray([[0, 0, 0, 0, 0, 0, 0, 0],<BR>
          [1, 0, 0, 0, 0, 0, 0, 0],<BR>
          [0, 1, 0, 0, 0, 0, 0, 0],<BR>
          [0, 1, 0, 0, 0, 0, 0, 0],<BR>
          [1, 0, 0, 0, 0, 0, 0, 0],<BR>
          [0, 0, 0, 0, 1, 0, 0, 0],<BR>
          [0, 0, 0, 0, 1, 0, 0, 0],<BR>
          [0, 0, 0, 0, 0, 0, 1, 0]],dtype = np.bool)<BR>
groups = ssp.csc_matrix(groups,dtype=np.bool)<BR>
tree = {<FONT COLOR="red">'eta_g'</FONT>: eta_g,<FONT COLOR="red">'groups'</FONT> : groups,<FONT COLOR="red">'own_variables'</FONT> : own_variables,<BR>
        <FONT COLOR="red">'N_own_variables'</FONT> : N_own_variables}<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nVarious regression experiments'</FONT><BR>
param[<FONT COLOR="red">'compute_gram'</FONT>] = True<BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression tree-l2'</FONT><BR>
param[<FONT COLOR="red">'loss'</FONT>] = <FONT COLOR="red">'square'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'tree-l2'</FONT><BR>
(W, optim_info) = spams.fistaTree(Y,X,W0,tree,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:],0),np.mean(optim_info[3,:],0))<BR>
<FONT COLOR="#007F00">###</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression tree-linf'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'tree-linf'</FONT><BR>
(W, optim_info) = spams.fistaTree(Y,X,W0,tree,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:],0),np.mean(optim_info[2,:]),np.mean(optim_info[3,:],0))<BR>
<FONT COLOR="#007F00">###<BR>
# works also with non tree-structured regularization. tree is ignored</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression Fused-Lasso'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'fused-lasso'</FONT><BR>
param[<FONT COLOR="red">'lambda2'</FONT>] = 0.001<BR>
param[<FONT COLOR="red">'lambda3'</FONT>] = 0.001<BR>
(W, optim_info) = spams.fistaTree(Y,X,W0,tree,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:],0),np.mean(optim_info[3,:],0))<BR>
<FONT COLOR="#007F00">###</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nISTA + Regression tree-l0'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'tree-l0'</FONT><BR>
(W, optim_info) = spams.fistaTree(Y,X,W0,tree,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:],0),np.mean(optim_info[3,:],0))<BR>
<FONT COLOR="#007F00">###</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression tree-l2 with intercept'</FONT><BR>
param[<FONT COLOR="red">'intercept'</FONT>] = True<BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'tree-l2'</FONT><BR>
x1 = np.asfortranarray(np.concatenate((X,np.ones((X.shape[0],1))),1))<BR>
W01 = np.asfortranarray(np.concatenate((W0,np.zeros((1,W0.shape[1]))),0))<BR>
(W, optim_info) = spams.fistaTree(Y,x1,W01,tree,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:],0),np.mean(optim_info[3,:],0))<BR>
<FONT COLOR="#007F00">###</FONT><BR>
param[<FONT COLOR="red">'intercept'</FONT>] = False<BR>
<BR>
<FONT COLOR="#007F00">#    Classification</FONT><BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nOne classification experiment'</FONT><BR>
Y = np.asfortranarray(2 * np.asarray(np.random.normal(size = (100,Y.shape[1])) &gt; 0,dtype=<FONT COLOR="red">'float64'</FONT>) - 1)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Logistic + tree-linf'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'tree-linf'</FONT><BR>
param[<FONT COLOR="red">'loss'</FONT>] = <FONT COLOR="red">'logistic'</FONT><BR>
param[<FONT COLOR="red">'lambda1'</FONT>] = 0.001<BR>
(W, optim_info) = spams.fistaTree(Y,X,W0,tree,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:],0),np.mean(optim_info[2,:]),np.mean(optim_info[3,:],0))<BR>
<FONT COLOR="#007F00">###<BR>
# can be used of course with other regularization functions, intercept,...<BR>
<BR>
#  Multi-Class classification</FONT><BR>
Y = np.asfortranarray(np.ceil(5 * np.random.random(size = (100,Y.shape[1]))) - 1)<BR>
param[<FONT COLOR="red">'loss'</FONT>] = <FONT COLOR="red">'multi-logistic'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'tree-l2'</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Multi-Class Logistic + tree-l2'</FONT><BR>
nclasses = np.max(Y[:])+1<BR>
W0 = np.zeros((X.shape[1],nclasses * Y.shape[1]),dtype=np.float64,order=<FONT COLOR="red">"FORTRAN"</FONT>)<BR>
(W, optim_info) = spams.fistaTree(Y,X,W0,tree,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:],0),np.mean(optim_info[3,:],0))<BR>
<FONT COLOR="#007F00"># can be used of course with other regularization functions, intercept,...<BR>
<BR>
# Multi-Task regression</FONT><BR>
Y = np.asfortranarray(np.random.normal(size = (100,100)))<BR>
Y = np.asfortranarray(Y - np.tile(np.mean(Y,0),(Y.shape[0],1)))<BR>
Y = spams.normalize(Y)<BR>
param[<FONT COLOR="red">'compute_gram'</FONT>] = False<BR>
param[<FONT COLOR="red">'verbose'</FONT>] = True;   <FONT COLOR="#007F00"># verbosity, False by default</FONT><BR>
W0 = np.zeros((X.shape[1],Y.shape[1]),dtype=np.float64,order=<FONT COLOR="red">"FORTRAN"</FONT>)<BR>
param[<FONT COLOR="red">'loss'</FONT>] = <FONT COLOR="red">'square'</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression  multi-task-tree'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'multi-task-tree'</FONT><BR>
param[<FONT COLOR="red">'lambda2'</FONT>] = 0.001<BR>
(W, optim_info) = spams.fistaTree(Y,X,W0,tree,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:],0),np.mean(optim_info[2,:]),np.mean(optim_info[3,:],0))<BR>
<BR>
<FONT COLOR="#007F00"># Multi-Task Classification</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Logistic + multi-task-tree'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'multi-task-tree'</FONT><BR>
param[<FONT COLOR="red">'lambda2'</FONT>] = 0.001<BR>
param[<FONT COLOR="red">'loss'</FONT>] = <FONT COLOR="red">'logistic'</FONT><BR>
Y = np.asfortranarray(2 * np.asarray(np.random.normal(size = (100,Y.shape[1])) &gt; 0,dtype=<FONT COLOR="red">'float64'</FONT>) - 1)<BR>
(W, optim_info) = spams.fistaTree(Y,X,W0,tree,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:],0),np.mean(optim_info[2,:]),np.mean(optim_info[3,:],0))<BR>
<BR>
<FONT COLOR="#007F00">#  Multi-Class + Multi-Task Regularization</FONT><BR>
param[<FONT COLOR="red">'verbose'</FONT>] = False<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Multi-Class Logistic +multi-task-tree'</FONT><BR>
Y = np.asfortranarray(np.ceil(5 * np.random.random(size = (100,Y.shape[1]))) - 1)<BR>
param[<FONT COLOR="red">'loss'</FONT>] = <FONT COLOR="red">'multi-logistic'</FONT><BR>
param[<FONT COLOR="red">'regul'</FONT>] = <FONT COLOR="red">'multi-task-tree'</FONT><BR>
nclasses = np.max(Y[:])+1<BR>
W0 = np.zeros((X.shape[1],nclasses * Y.shape[1]),dtype=np.float64,order=<FONT COLOR="red">"FORTRAN"</FONT>)<BR>
(W, optim_info) = spams.fistaTree(Y,X,W0,tree,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:],0),np.mean(optim_info[2,:]),np.mean(optim_info[3,:],0))<BR>
<FONT COLOR="#007F00"># can be used of course with other regularization functions, intercept,...</FONT><BR>
<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Multi-Class Logistic +multi-task-tree + sparse matrix'</FONT><BR>
nclasses = np.max(Y[:])+1<BR>
W0 = np.zeros((X.shape[1],nclasses * Y.shape[1]),dtype=np.float64,order=<FONT COLOR="red">"FORTRAN"</FONT>)<BR>
X2 = ssp.csc_matrix(X)<BR>
(W, optim_info) = spams.fistaTree(Y,X2,W0,tree,True,**param)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:],0),np.mean(optim_info[2,:]),np.mean(optim_info[3,:],0))</TD></TR>
</TABLE><!--TOC subsection Function spams.fistaGraph-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc34">5.10</A>  Function spams.fistaGraph</H3><!--SEC END --><P>
Given a matrix <I><B>X</B></I>=[<I><B>x</B></I><SUP>1</SUP>,…,<I><B>x</B><SUP>p</SUP></I>]<I><SUP>T</SUP></I> in ℝ<SUP><I>m</I> × <I>p</I></SUP>, and a matrix <I><B>Y</B></I>=[<I><B>y</B></I><SUP>1</SUP>,…,<I><B>y</B><SUP>n</SUP></I>], it solves the optimization problems presented in the previous section, with the same regularization functions as spams.proximalGraph.
see usage details below:</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: fistaGraph<BR>
#<BR>
# Usage: spams.fistaGraph(Y,X,W0,graph,return_optim_info = False,numThreads =-1,max_it =1000,L0=1.0,<BR>
#                  fixed_step=False,gamma=1.5,lambda1=1.0,delta=1.0,lambda2=0.,lambda3=0.,<BR>
#                  a=1.0,b=0.,c=1.0,tol=0.000001,it0=100,max_iter_backtracking=1000,<BR>
#                  compute_gram=False,lin_admm=False,admm=False,intercept=False,<BR>
#                  resetflow=False,regul="",loss="",verbose=False,pos=False,clever=False,<BR>
#                  log=False,ista=False,subgrad=False,logName="",is_inner_weights=False,<BR>
#                  inner_weights=np.array([0.]),size_group=1,sqrt_step=True,transpose=False)<BR>
#<BR>
# Description:<BR>
#     fistaGraph solves sparse regularized problems.<BR>
#         X is a design matrix of size m x p<BR>
#         X=[x^1,...,x^n]', where the x_i's are the rows of X<BR>
#         Y=[y^1,...,y^n] is a matrix of size m x n<BR>
#         It implements the algorithms FISTA, ISTA and subgradient descent.<BR>
#         <BR>
#         It implements the algorithms FISTA, ISTA and subgradient descent for solving<BR>
#         <BR>
#           min_W  loss(W) + lambda1 psi(W)<BR>
#           <BR>
#         The function psi are those used by proximalGraph (see documentation)<BR>
#         for the loss functions, see the documentation of fistaFlat<BR>
#         <BR>
#         This function can also handle intercepts (last row of W is not regularized),<BR>
#         and/or non-negativity constraints on W.<BR>
#<BR>
# Inputs:<BR>
#       Y:  double dense m x n matrix<BR>
#       X:  double dense or sparse m x p matrix   <BR>
#       W0:  double dense p x n matrix or p x Nn matrix (for multi-logistic loss)<BR>
#            initial guess<BR>
#       graph: struct (see documentation of proximalGraph)<BR>
#       return_optim_info:     <BR>
#               if true the function will return a tuple of matrices.<BR>
#       loss: (choice of loss, see above)<BR>
#       regul: (choice of regularization, see function proximalFlat)<BR>
#       lambda1: (regularization parameter)<BR>
#       lambda2: (optional, regularization parameter, 0 by default)<BR>
#       lambda3: (optional, regularization parameter, 0 by default)<BR>
#       verbose: (optional, verbosity level, false by default)<BR>
#       pos: (optional, adds positivity constraints on the<BR>
#           coefficients, false by default)<BR>
#       numThreads: (optional, number of threads for exploiting<BR>
#           multi-core / multi-cpus. By default, it takes the value -1,<BR>
#           which automatically selects all the available CPUs/cores).<BR>
#       max_it: (optional, maximum number of iterations, 100 by default)<BR>
#       it0: (optional, frequency for computing duality gap, every 10 iterations by default)<BR>
#       tol: (optional, tolerance for stopping criteration, which is a relative duality gap<BR>
#           if it is available, or a relative change of parameters).<BR>
#       gamma: (optional, multiplier for increasing the parameter L in fista, 1.5 by default)<BR>
#       L0: (optional, initial parameter L in fista, 0.1 by default, should be small enough)<BR>
#       fixed_step: (deactive the line search for L in fista and use L0 instead)<BR>
#       compute_gram: (optional, pre-compute X^TX, false by default).<BR>
#       intercept: (optional, do not regularize last row of W, false by default).<BR>
#       ista: (optional, use ista instead of fista, false by default).<BR>
#       subgrad: (optional, if not ista, use subradient descent instead of fista, false by default).<BR>
#       a: <BR>
#       b: (optional, if subgrad, the gradient step is a/(t+b)<BR>
#       also similar options as proximalTree<BR>
#       <BR>
#       the function also implements the ADMM algorithm via an option admm=true. It is not documented<BR>
#       and you need to look at the source code to use it.<BR>
#       delta:    undocumented; modify at your own risks!<BR>
#       c:    undocumented; modify at your own risks!<BR>
#       max_iter_backtracking:    undocumented; modify at your own risks!<BR>
#       lin_admm:    undocumented; modify at your own risks!<BR>
#       admm:    undocumented; modify at your own risks!<BR>
#       resetflow:    undocumented; modify at your own risks!<BR>
#       clever:    undocumented; modify at your own risks!<BR>
#       log:    undocumented; modify at your own risks!<BR>
#       logName:    undocumented; modify at your own risks!<BR>
#       is_inner_weights:    undocumented; modify at your own risks!<BR>
#       inner_weights:    undocumented; modify at your own risks!<BR>
#       sqrt_step:    undocumented; modify at your own risks!<BR>
#       size_group:    undocumented; modify at your own risks!<BR>
#       transpose:    undocumented; modify at your own risks!<BR>
#<BR>
# Output:<BR>
#       W:  double dense p x n matrix or p x Nn matrix (for multi-logistic loss)<BR>
#       optim: optional, double dense 4 x n matrix.<BR>
#           first row: values of the objective functions.<BR>
#           third row: values of the relative duality gap (if available)<BR>
#           fourth row: number of iterations<BR>
#       optim_info:        vector of size 4, containing information of the optimization.<BR>
#             W = spams.fistaGraph(Y,X,W0,graph,return_optim_info = False,...)<BR>
#             (W,optim_info) = spams.fistaGraph(Y,X,W0,graph,return_optim_info = True,...)<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2010 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#<BR>
# Note:<BR>
#     Valid values for the regularization parameter (regul) are:<BR>
#       "l0", "l1", "l2", "linf", "l2-not-squared", "elastic-net", "fused-lasso",<BR>
#       "group-lasso-l2", "group-lasso-linf", "sparse-group-lasso-l2",<BR>
#       "sparse-group-lasso-linf", "l1l2", "l1linf", "l1l2+l1", "l1linf+l1",<BR>
#       "tree-l0", "tree-l2", "tree-linf", "graph", "graph-ridge", "graph-l2",<BR>
#       "multi-task-tree", "multi-task-graph", "l1linf-row-column", "trace-norm",<BR>
#       "trace-norm-vec", "rank", "rank-vec", "none"<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
np.random.seed(0)<BR>
num_threads = -1 <FONT COLOR="#007F00"># all cores (-1 by default)</FONT><BR>
verbose = False   <FONT COLOR="#007F00"># verbosity, false by default</FONT><BR>
lambda1 = 0.1 <FONT COLOR="#007F00"># regularization ter</FONT><BR>
it0 = 1      <FONT COLOR="#007F00"># frequency for duality gap computations</FONT><BR>
max_it = 100 <FONT COLOR="#007F00"># maximum number of iterations</FONT><BR>
L0 = 0.1<BR>
tol = 1e-5<BR>
intercept = False<BR>
pos = False<BR>
<BR>
eta_g = np.array([1, 1, 1, 1, 1],dtype=np.float64)<BR>
<BR>
groups = ssp.csc_matrix(np.array([[0, 0, 0, 1, 0],<BR>
                   [0, 0, 0, 0, 0],<BR>
                   [0, 0, 0, 0, 0],<BR>
                   [0, 0, 0, 0, 0],<BR>
                   [0, 0, 1, 0, 0]],dtype=np.bool),dtype=np.bool)<BR>
<BR>
groups_var = ssp.csc_matrix(np.array([[1, 0, 0, 0, 0],<BR>
                       [1, 0, 0, 0, 0],<BR>
                       [1, 0, 0, 0, 0],<BR>
                       [1, 1, 0, 0, 0],<BR>
                       [0, 1, 0, 1, 0],<BR>
                       [0, 1, 0, 1, 0],<BR>
                       [0, 1, 0, 0, 1],<BR>
                       [0, 0, 0, 0, 1],<BR>
                       [0, 0, 0, 0, 1],<BR>
                       [0, 0, 1, 0, 0]],dtype=np.bool),dtype=np.bool)<BR>
<BR>
graph = {<FONT COLOR="red">'eta_g'</FONT>: eta_g,<FONT COLOR="red">'groups'</FONT> : groups,<FONT COLOR="red">'groups_var'</FONT> : groups_var}<BR>
<BR>
verbose = True<BR>
X = np.asfortranarray(np.random.normal(size = (100,10)))<BR>
X = np.asfortranarray(X - np.tile(np.mean(X,0),(X.shape[0],1)))<BR>
X = spams.normalize(X)<BR>
Y = np.asfortranarray(np.random.normal(size = (100,1)))<BR>
Y = np.asfortranarray(Y - np.tile(np.mean(Y,0),(Y.shape[0],1)))<BR>
Y = spams.normalize(Y)<BR>
W0 = np.zeros((X.shape[1],Y.shape[1]),dtype=np.float64,order=<FONT COLOR="red">"FORTRAN"</FONT>)<BR>
<FONT COLOR="#007F00"># Regression experiments <BR>
# 100 regression problems with the same design matrix X.</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nVarious regression experiments'</FONT><BR>
compute_gram = True<BR>
<FONT COLOR="#007F00">#</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression graph'</FONT><BR>
loss = <FONT COLOR="red">'square'</FONT><BR>
regul = <FONT COLOR="red">'graph'</FONT><BR>
tic = time.time()<BR>
(W, optim_info) = spams.fistaGraph(<BR>
    Y,X,W0,graph,True,numThreads = num_threads,verbose = verbose,<BR>
    lambda1 = lambda1,it0 = it0,max_it = max_it,L0 = L0,tol = tol,<BR>
    intercept = intercept,pos = pos,compute_gram = compute_gram,<BR>
    loss = loss,regul = regul)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, time: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),t,np.mean(optim_info[3,:]))<BR>
<FONT COLOR="#007F00">#</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nADMM + Regression graph'</FONT><BR>
admm = True<BR>
lin_admm = True<BR>
c = 1<BR>
delta = 1<BR>
tic = time.time()<BR>
(W, optim_info) = spams.fistaGraph(<BR>
    Y,X,W0,graph,True,numThreads = num_threads,verbose = verbose,<BR>
    lambda1 = lambda1,it0 = it0,max_it = max_it,L0 = L0,tol = tol,<BR>
    intercept = intercept,pos = pos,compute_gram = compute_gram,<BR>
    loss = loss,regul = regul,admm = admm,lin_admm = lin_admm,c = c,delta = delta)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, time: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),t,np.mean(optim_info[3,:]))<BR>
<FONT COLOR="#007F00">#</FONT><BR>
admm = False<BR>
max_it = 5<BR>
it0 = 1<BR>
tic = time.time()<BR>
(W, optim_info) = spams.fistaGraph(<BR>
    Y,X,W0,graph,True,numThreads = num_threads,verbose = verbose,<BR>
    lambda1 = lambda1,it0 = it0,max_it = max_it,L0 = L0,tol = tol,<BR>
    intercept = intercept,pos = pos,compute_gram = compute_gram,<BR>
    loss = loss,regul = regul,admm = admm,lin_admm = lin_admm,c = c,delta = delta)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, time: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),t,np.mean(optim_info[3,:]))<BR>
<FONT COLOR="#007F00">#<BR>
#  works also with non graph-structured regularization. graph is ignored</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression Fused-Lasso'</FONT><BR>
regul = <FONT COLOR="red">'fused-lasso'</FONT><BR>
lambda2 = 0.01<BR>
lambda3 = 0.01<BR>
tic = time.time()<BR>
(W, optim_info) = spams.fistaGraph(<BR>
    Y,X,W0,graph,True,numThreads = num_threads,verbose = verbose,<BR>
    lambda1 = lambda1,it0 = it0,max_it = max_it,L0 = L0,tol = tol,<BR>
    intercept = intercept,pos = pos,compute_gram = compute_gram,<BR>
    loss = loss,regul = regul,admm = admm,lin_admm = lin_admm,c = c,<BR>
    lambda2 = lambda2,lambda3 = lambda3,delta = delta)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, time: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),t,np.mean(optim_info[3,:]))<BR>
<FONT COLOR="#007F00">#</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression graph with intercept'</FONT><BR>
regul = <FONT COLOR="red">'graph'</FONT><BR>
intercept = True<BR>
tic = time.time()<BR>
(W, optim_info) = spams.fistaGraph(<BR>
    Y,X,W0,graph,True,numThreads = num_threads,verbose = verbose,<BR>
    lambda1 = lambda1,it0 = it0,max_it = max_it,L0 = L0,tol = tol,<BR>
    intercept = intercept,pos = pos,compute_gram = compute_gram,<BR>
    loss = loss,regul = regul,admm = admm,lin_admm = lin_admm,c = c,<BR>
    lambda2 = lambda2,lambda3 = lambda3,delta = delta)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, time: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),t,np.mean(optim_info[3,:]))<BR>
intercept = False<BR>
<BR>
<FONT COLOR="#007F00"># Classification</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nOne classification experiment'</FONT><BR>
Y = np.asfortranarray( 2 * np.asfortranarray(np.random.normal(size = (100,Y.shape[1])) &gt; 0,dtype = np.float64) -1)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA +  Logistic + graph-linf'</FONT><BR>
loss = <FONT COLOR="red">'logistic'</FONT><BR>
regul = <FONT COLOR="red">'graph'</FONT><BR>
lambda1 = 0.01<BR>
tic = time.time()<BR>
(W, optim_info) = spams.fistaGraph(<BR>
    Y,X,W0,graph,True,numThreads = num_threads,verbose = verbose,<BR>
    lambda1 = lambda1,it0 = it0,max_it = max_it,L0 = L0,tol = tol,<BR>
    intercept = intercept,pos = pos,compute_gram = compute_gram,<BR>
    loss = loss,regul = regul,admm = admm,lin_admm = lin_admm,c = c,<BR>
    lambda2 = lambda2,lambda3 = lambda3,delta = delta)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, time: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),t,np.mean(optim_info[3,:]))<BR>
<FONT COLOR="#007F00">#<BR>
# can be used of course with other regularization functions, intercept,...<BR>
<BR>
# Multi-Class classification</FONT><BR>
<BR>
Y = np.asfortranarray(np.ceil(5 * np.random.random(size = (100,Y.shape[1]))) - 1)<BR>
loss = <FONT COLOR="red">'multi-logistic'</FONT><BR>
regul = <FONT COLOR="red">'graph'</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Multi-Class Logistic + graph'</FONT><BR>
nclasses = np.max(Y) + 1<BR>
W0 = np.zeros((X.shape[1],nclasses * Y.shape[1]),dtype=np.float64,order=<FONT COLOR="red">"FORTRAN"</FONT>)<BR>
tic = time.time()<BR>
(W, optim_info) = spams.fistaGraph(<BR>
    Y,X,W0,graph,True,numThreads = num_threads,verbose = verbose,<BR>
    lambda1 = lambda1,it0 = it0,max_it = max_it,L0 = L0,tol = tol,<BR>
    intercept = intercept,pos = pos,compute_gram = compute_gram,<BR>
    loss = loss,regul = regul,admm = admm,lin_admm = lin_admm,c = c,<BR>
    lambda2 = lambda2,lambda3 = lambda3,delta = delta)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, time: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),t,np.mean(optim_info[3,:]))<BR>
<FONT COLOR="#007F00">#<BR>
# can be used of course with other regularization functions, intercept,...<BR>
# Multi-Task regression</FONT><BR>
Y = np.asfortranarray(np.random.normal(size = (100,Y.shape[1])))<BR>
Y = np.asfortranarray(Y - np.tile(np.mean(Y,0),(Y.shape[0],1)))<BR>
Y = spams.normalize(Y)<BR>
W0 = W0 = np.zeros((X.shape[1],Y.shape[1]),dtype=np.float64,order=<FONT COLOR="red">"FORTRAN"</FONT>)<BR>
compute_gram = False<BR>
verbose = True<BR>
loss = <FONT COLOR="red">'square'</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Regression multi-task-graph'</FONT><BR>
regul = <FONT COLOR="red">'multi-task-graph'</FONT><BR>
lambda2 = 0.01<BR>
tic = time.time()<BR>
(W, optim_info) = spams.fistaGraph(<BR>
    Y,X,W0,graph,True,numThreads = num_threads,verbose = verbose,<BR>
    lambda1 = lambda1,it0 = it0,max_it = max_it,L0 = L0,tol = tol,<BR>
    intercept = intercept,pos = pos,compute_gram = compute_gram,<BR>
    loss = loss,regul = regul,admm = admm,lin_admm = lin_admm,c = c,<BR>
    lambda2 = lambda2,lambda3 = lambda3,delta = delta)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, time: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),t,np.mean(optim_info[3,:]))<BR>
<FONT COLOR="#007F00">#<BR>
# Multi-Task Classification</FONT><BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Logistic + multi-task-graph'</FONT><BR>
regul = <FONT COLOR="red">'multi-task-graph'</FONT><BR>
lambda2 = 0.01<BR>
loss = <FONT COLOR="red">'logistic'</FONT><BR>
Y = np.asfortranarray( 2 * np.asfortranarray(np.random.normal(size = (100,Y.shape[1])) &gt; 0,dtype = np.float64) -1)<BR>
tic = time.time()<BR>
(W, optim_info) = spams.fistaGraph(<BR>
    Y,X,W0,graph,True,numThreads = num_threads,verbose = verbose,<BR>
    lambda1 = lambda1,it0 = it0,max_it = max_it,L0 = L0,tol = tol,<BR>
    intercept = intercept,pos = pos,compute_gram = compute_gram,<BR>
    loss = loss,regul = regul,admm = admm,lin_admm = lin_admm,c = c,<BR>
    lambda2 = lambda2,lambda3 = lambda3,delta = delta)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, time: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),t,np.mean(optim_info[3,:]))<BR>
<FONT COLOR="#007F00"># Multi-Class + Multi-Task Regularization</FONT><BR>
verbose = False<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'\nFISTA + Multi-Class Logistic +multi-task-graph'</FONT><BR>
Y = np.asfortranarray(np.ceil(5 * np.random.random(size = (100,Y.shape[1]))) - 1)<BR>
loss = <FONT COLOR="red">'multi-logistic'</FONT><BR>
regul = <FONT COLOR="red">'multi-task-graph'</FONT><BR>
nclasses = np.max(Y) + 1<BR>
W0 = np.zeros((X.shape[1],nclasses * Y.shape[1]),dtype=np.float64,order=<FONT COLOR="red">"FORTRAN"</FONT>)<BR>
tic = time.time()<BR>
(W, optim_info) = spams.fistaGraph(<BR>
    Y,X,W0,graph,True,numThreads = num_threads,verbose = verbose,<BR>
    lambda1 = lambda1,it0 = it0,max_it = max_it,L0 = L0,tol = tol,<BR>
    intercept = intercept,pos = pos,compute_gram = compute_gram,<BR>
    loss = loss,regul = regul,admm = admm,lin_admm = lin_admm,c = c,<BR>
    lambda2 = lambda2,lambda3 = lambda3,delta = delta)<BR>
tac = time.time()<BR>
t = tac - tic<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">'mean loss: %f, mean relative duality_gap: %f, time: %f, number of iterations: %f'</FONT> %(np.mean(optim_info[0,:]),np.mean(optim_info[2,:]),t,np.mean(optim_info[3,:]))<BR>
<FONT COLOR="#007F00"># can be used of course with other regularization functions, intercept,...</FONT></TD></TR>
</TABLE><!--TOC subsection Function spams.fistaPathCoding-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc35">5.11</A>  Function spams.fistaPathCoding</H3><!--SEC END --><P>
Similarly, the toolbox handles the penalties of [<A HREF="#mairal14">23</A>] with the following function
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# The python function is not yet implemented.<BR>
#</FONT></TD></TR>
</TABLE><!--TOC section Miscellaneous Functions-->
<H2 CLASS="section"><!--SEC ANCHOR --><A NAME="htoc36">6</A>  Miscellaneous Functions</H2><!--SEC END --><!--TOC subsection Function spams.conjGrad-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc37">6.1</A>  Function spams.conjGrad</H3><!--SEC END --><P>Implementation of a conjugate gradient for solving a linear system <B><I>Ax</I></B>=<I><B>b</B></I>
when <I><B>A</B></I> is positive definite. In some cases, it is faster than the Matlab
function <CODE>pcg</CODE>, especially when the library uses the Intel Math Kernel Library.
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: conjGrad<BR>
#<BR>
# Usage: spams.conjGrad(A,b,x0 = None,tol = 1e-10,itermax = None)<BR>
#<BR>
# Description:<BR>
#     Conjugate gradient algorithm, sometimes faster than the <BR>
#    equivalent python function solve. In order to solve Ax=b;<BR>
#<BR>
# Inputs:<BR>
#       A:  double square n x n matrix. HAS TO BE POSITIVE DEFINITE<BR>
#       b:  double vector of length n.<BR>
#       x0: double vector of length n. (optional) initial guess.<BR>
#       tol: (optional) tolerance.<BR>
#       itermax: (optional) maximum number of iterations.<BR>
#<BR>
# Output:<BR>
#       x: double vector of length n.<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2009 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
A = np.asfortranarray(np.random.normal(size = (5000,500)))<BR>
A = np.asfortranarray(np.dot(A.T,A))<BR>
b = np.ones((A.shape[1],),dtype=np.float64,order=<FONT COLOR="red">"FORTRAN"</FONT>)<BR>
x0 = b<BR>
tol = 1e-4<BR>
itermax = int(0.5 * len(b))<BR>
<BR>
tic = time.time()<BR>
<FONT COLOR="blue"><B>for</B></FONT> i <FONT COLOR="blue"><B>in</B></FONT> xrange(0,20):<BR>
    y1 = np.linalg.solve(A,b)<BR>
tac = time.time()<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"  Time (numpy): "</FONT>, tac - tic<BR>
x1 = np.abs(b - np.dot(A,y1))<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"Mean error on b : %f"</FONT> %(x1.sum() / b.shape[0])<BR>
<BR>
tic = time.time()<BR>
<FONT COLOR="blue"><B>for</B></FONT> i <FONT COLOR="blue"><B>in</B></FONT> xrange(0,20):<BR>
    y2 = spams.conjGrad(A,b,x0,tol,itermax)<BR>
tac = time.time()<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"  Time (spams): "</FONT>, tac - tic<BR>
x1 = np.dot(A,y2)<BR>
x2 = np.abs(b - x1)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"Mean error on b : %f"</FONT> %(x2.sum() / b.shape[0])<BR>
<BR>
err = abs(y1 - y2)</TD></TR>
</TABLE><!--TOC subsection Function spams.bayer-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc38">6.2</A>  Function spams.bayer</H3><!--SEC END --><P>Apply a Bayer pattern to an input image
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: conjGrad<BR>
#<BR>
# Usage: spams.conjGrad(A,b,x0 = None,tol = 1e-10,itermax = None)<BR>
#<BR>
# Description:<BR>
#     Conjugate gradient algorithm, sometimes faster than the <BR>
#    equivalent python function solve. In order to solve Ax=b;<BR>
#<BR>
# Inputs:<BR>
#       A:  double square n x n matrix. HAS TO BE POSITIVE DEFINITE<BR>
#       b:  double vector of length n.<BR>
#       x0: double vector of length n. (optional) initial guess.<BR>
#       tol: (optional) tolerance.<BR>
#       itermax: (optional) maximum number of iterations.<BR>
#<BR>
# Output:<BR>
#       x: double vector of length n.<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2009 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
A = np.asfortranarray(np.random.normal(size = (5000,500)))<BR>
A = np.asfortranarray(np.dot(A.T,A))<BR>
b = np.ones((A.shape[1],),dtype=np.float64,order=<FONT COLOR="red">"FORTRAN"</FONT>)<BR>
x0 = b<BR>
tol = 1e-4<BR>
itermax = int(0.5 * len(b))<BR>
<BR>
tic = time.time()<BR>
<FONT COLOR="blue"><B>for</B></FONT> i <FONT COLOR="blue"><B>in</B></FONT> xrange(0,20):<BR>
    y1 = np.linalg.solve(A,b)<BR>
tac = time.time()<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"  Time (numpy): "</FONT>, tac - tic<BR>
x1 = np.abs(b - np.dot(A,y1))<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"Mean error on b : %f"</FONT> %(x1.sum() / b.shape[0])<BR>
<BR>
tic = time.time()<BR>
<FONT COLOR="blue"><B>for</B></FONT> i <FONT COLOR="blue"><B>in</B></FONT> xrange(0,20):<BR>
    y2 = spams.conjGrad(A,b,x0,tol,itermax)<BR>
tac = time.time()<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"  Time (spams): "</FONT>, tac - tic<BR>
x1 = np.dot(A,y2)<BR>
x2 = np.abs(b - x1)<BR>
<FONT COLOR="blue"><B>print</B></FONT> <FONT COLOR="red">"Mean error on b : %f"</FONT> %(x2.sum() / b.shape[0])<BR>
<BR>
err = abs(y1 - y2)</TD></TR>
</TABLE><!--TOC subsection Function spams.calcAAt-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc39">6.3</A>  Function spams.calcAAt</H3><!--SEC END --><P>For an input sparse matrix <I><B>A</B></I>, this function returns <I><B>AA</B><SUP>T</SUP></I>. For some reasons, when <I><B>A</B></I> has a lot more columns than rows, this function can be much faster than the equivalent matlab command <CODE>X*A'</CODE>. 
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: calcAAt<BR>
#<BR>
# Usage: spams.calcAAt(A)<BR>
#<BR>
# Description:<BR>
#     Compute efficiently AAt = A*A', when A is sparse <BR>
#   and has a lot more columns than rows. In some cases, it is<BR>
#   up to 20 times faster than the equivalent python expression<BR>
#   AAt=A*A';<BR>
#<BR>
# Inputs:<BR>
#       A:  double sparse m x n matrix   <BR>
#<BR>
# Output:<BR>
#       AAt: double m x m matrix <BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2009 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
<FONT COLOR="red">"""<BR>
test A * A'<BR>
"""</FONT><BR>
m=200; n = 200000; d= 0.05<BR>
A = ssprand(m,n,density=d,format=<FONT COLOR="red">'csc'</FONT>,dtype=np.float64)<BR>
result = spams.calcAAt(A)</TD></TR>
</TABLE><!--TOC subsection Function spams.calcXAt-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc40">6.4</A>  Function spams.calcXAt</H3><!--SEC END --><P>
For an input sparse matrix <I><B>A</B></I> and a matrix <I><B>X</B></I>, this function returns <I><B>XA</B><SUP>T</SUP></I>. For some reasons, when <I><B>A</B></I> has a lot more columns than rows, this function can be much faster than the equivalent matlab command <CODE>X*A'</CODE>. 
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: calcXAt<BR>
#<BR>
# Usage: spams.calcXAt(X,A)<BR>
#<BR>
# Description:<BR>
#     Compute efficiently XAt = X*A', when A is sparse and has a <BR>
#   lot more columns than rows. In some cases, it is up to 20 times <BR>
#   faster than the equivalent python expression;<BR>
#<BR>
# Inputs:<BR>
#       X:  double m x n matrix<BR>
#       A:  double sparse p x n matrix   <BR>
#<BR>
# Output:<BR>
#       XAt: double m x p matrix <BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2009 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
m=200; n = 200000; d= 0.05<BR>
A = ssprand(m,n,density=d,format=<FONT COLOR="red">'csc'</FONT>,dtype=np.float64)<BR>
X = np.asfortranarray(np.random.normal(size = (64,n)))<BR>
<BR>
result = spams.calcXAt(X,A)</TD></TR>
</TABLE><!--TOC subsection Function spams.calcXY-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc41">6.5</A>  Function spams.calcXY</H3><!--SEC END --><P>
For two input matrices <I><B>X</B></I> and <I><B>Y</B></I>, this function returns <B><I>XY</I></B>. 
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: calcXY<BR>
#<BR>
# Usage: spams.calcXY(X,Y)<BR>
#<BR>
# Description:<BR>
#     Compute Z=XY using the BLAS library used by SPAMS.<BR>
#<BR>
# Inputs:<BR>
#       X:  double m x n matrix<BR>
#       Y:  double n x p matrix   <BR>
#<BR>
# Output:<BR>
#       Z: double m x p matrix <BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2009 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
X = np.asfortranarray(np.random.normal(size = (64,200)))<BR>
Y = np.asfortranarray(np.random.normal(size = (200,20000)))<BR>
result = spams.calcXY(X,Y)</TD></TR>
</TABLE><!--TOC subsection Function spams.calcXYt-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc42">6.6</A>  Function spams.calcXYt</H3><!--SEC END --><P>
For two input matrices <I><B>X</B></I> and <I><B>Y</B></I>, this function returns <I><B>XY</B><SUP>T</SUP></I>.
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: calcXYt<BR>
#<BR>
# Usage: spams.calcXYt(X,Y)<BR>
#<BR>
# Description:<BR>
#     Compute Z=XY' using the BLAS library used by SPAMS.<BR>
#<BR>
# Inputs:<BR>
#       X:  double m x n matrix<BR>
#       Y:  double p x n matrix   <BR>
#<BR>
# Output:<BR>
#       Z: double m x p matrix <BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2009 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
X = np.asfortranarray(np.random.normal(size = (64,200)))<BR>
Y = np.asfortranarray(np.random.normal(size = (20000,200)))<BR>
result = spams.calcXYt(X,Y)</TD></TR>
</TABLE><!--TOC subsection Function spams.calcXtY-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc43">6.7</A>  Function spams.calcXtY</H3><!--SEC END --><P>
For two input matrices <I><B>X</B></I> and <I><B>Y</B></I>, this function returns <I><B>X</B><SUP>T</SUP><B>Y</B></I>.
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: calcXtY<BR>
#<BR>
# Usage: spams.calcXtY(X,Y)<BR>
#<BR>
# Description:<BR>
#     Compute Z=X'Y using the BLAS library used by SPAMS.<BR>
#<BR>
# Inputs:<BR>
#       X:  double n x m matrix<BR>
#       Y:  double n x p matrix   <BR>
#<BR>
# Output:<BR>
#       Z: double m x p matrix <BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2009 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
X = np.asfortranarray(np.random.normal(size = (200,64)))<BR>
Y = np.asfortranarray(np.random.normal(size = (200,20000)))<BR>
result = spams.calcXtY(X,Y)</TD></TR>
</TABLE><!--TOC subsection Function spams.invSym-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc44">6.8</A>  Function spams.invSym</H3><!--SEC END --><P>
For an input symmetric matrices <I><B>A</B></I> in ℝ<SUP><I>n</I> × <I>n</I></SUP>, this function returns <I><B>A</B></I><SUP>−1</SUP>.
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: invSym<BR>
#<BR>
# Usage: spams.invSym(A)<BR>
#<BR>
# Description:<BR>
#     returns the inverse of a symmetric matrix A<BR>
#<BR>
# Inputs:<BR>
#       A:  double n x n matrix   <BR>
#<BR>
# Output:<BR>
#       B: double n x n matrix <BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2009 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
A = np.asfortranarray(np.random.random(size = (1000,1000)))<BR>
A =np.asfortranarray( np.dot(A.T,A))<BR>
result = spams.invSym(A)</TD></TR>
</TABLE><!--TOC subsection Function spams.normalize-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc45">6.9</A>  Function spams.normalize</H3><!--SEC END --><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: normalize<BR>
#<BR>
# Usage: spams.normalize(A)<BR>
#<BR>
# Description:<BR>
#     rescale the columns of X so that they have<BR>
#        unit l2-norm.<BR>
#<BR>
# Inputs:<BR>
#       X:  double m x n matrix   <BR>
#<BR>
# Output:<BR>
#       Y: double m x n matrix <BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2010 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
A = np.asfortranarray(np.random.random(size = (100,1000)))<BR>
res2 = spams.normalize(A)</TD></TR>
</TABLE><!--TOC subsection Function spams.sort-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc46">6.10</A>  Function spams.sort</H3><!--SEC END --><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# Name: sort<BR>
#<BR>
# Usage: spams.sort(X,mode=True)<BR>
#<BR>
# Description:<BR>
#     sort the elements of X using quicksort<BR>
#<BR>
# Inputs:<BR>
#       X:  double vector of size n<BR>
#       mode: false for decreasing order (true by default)<BR>
#<BR>
# Output:<BR>
#       Y: double  vector of size n<BR>
#<BR>
# Authors:<BR>
# Julien MAIRAL, 2010 (spams, matlab interface and documentation)<BR>
# Jean-Paul CHIEZE 2011-2012 (python interface)<BR>
#</FONT></TD></TR>
</TABLE><P>
The following piece of code contains usage examples:
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="blue"><B>import</B></FONT> spams<BR>
<FONT COLOR="blue"><B>import</B></FONT> numpy as np<BR>
n = 2000000<BR>
X = np.random.normal(size = (n,))<BR>
result = spams.sort(X,True)</TD></TR>
</TABLE><!--TOC subsection Function mexDisplayPatches-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc47">6.11</A>  Function mexDisplayPatches</H3><!--SEC END --><P>
Print to the screen a matrix containing as columns image patches.</P><!--TOC subsection Function spams.countPathsDAG-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc48">6.12</A>  Function spams.countPathsDAG</H3><!--SEC END --><P>
This function counts the number of paths in a DAG using dynamic programming.
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# The python function is not yet implemented.<BR>
#</FONT></TD></TR>
</TABLE><!--TOC subsection Function spams.removeCyclesGraph-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc49">6.13</A>  Function spams.removeCyclesGraph</H3><!--SEC END --><P>
One heuristic to remove cycles from a graph.
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# The python function is not yet implemented.<BR>
#</FONT></TD></TR>
</TABLE><!--TOC subsection Function spams.countConnexComponents-->
<H3 CLASS="subsection"><!--SEC ANCHOR --><A NAME="htoc50">6.14</A>  Function spams.countConnexComponents</H3><!--SEC END --><P>
Count the number of connected components of a subgraph from a graph.
</P><TABLE CLASS="lstframe" STYLE="padding:1ex;background-color:#E5EDFF;"><TR><TD CLASS="mouselstlisting"><FONT COLOR="#007F00">#<BR>
# The python function is not yet implemented.<BR>
#</FONT></TD></TR>
</TABLE><!--TOC section Duality Gaps with Fenchel Duality-->
<H2 CLASS="section"><!--SEC ANCHOR --><A NAME="htoc51">A</A>  Duality Gaps with Fenchel Duality</H2><!--SEC END --><P><A NAME="appendix"></A>
This section is taken from the appendix D of Julien Mairal’s PhD thesis [<A HREF="#mairal11">18</A>].
We are going to use intensively Fenchel Duality (see [<A HREF="#borwein">2</A>]).
Let us consider the problem
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">min</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I><B>w</B></I> ∈ ℝ<I><SUP>p</SUP></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"> [<I>g</I>(<I><B>w</B></I>) </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=2>▵</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">=</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
</TABLE></TD><TD CLASS="dcell"> <I>f</I>(<I><B>w</B></I>) + λψ(<I><B>w</B></I>)],<A NAME="software:eq:prb"></A>
    (47)</TD></TR>
</TABLE><P>
We first notice that for all the formulations we have been
interested in, <I>g</I>(<I><B>w</B></I>) can be rewritten
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
<I>g</I>(<I><B>w</B></I>) = f(<I><B>X</B></I><SUP>⊤</SUP><I><B>w</B></I>) + λψ(<I><B>w</B></I>), <A NAME="software:eq:prb2"></A>
    (48)</TD></TR>
</TABLE><P>
where <I><B>X</B></I>=[<I><B>x</B></I><SUP>1</SUP>,…,<I><B>x</B><SUP>n</SUP></I>] are training vectors, and f is an
appropriated smooth real-valued function of ℝ<I><SUP>n</SUP></I>,
and ψ one of the regularization functions we have introduced.</P><P>Given a primal variable <I><B>w</B></I> in ℝ<I><SUP>p</SUP></I> and a dual variable κ in
ℝ<I><SUP>n</SUP></I>, we obtain using classical Fenchel duality rules [<A HREF="#borwein">2</A>], 
that the following quantity is a duality gap for problem (<A HREF="#software:eq:prb">47</A>):
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">                δ(<I><B>w</B></I>,κ) </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=2>▵</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">=</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
</TABLE></TD><TD CLASS="dcell"> <I>g</I>(<I><B>w</B></I>) + f<SUP>∗</SUP>(κ) + λψ<SUP>∗</SUP>(−<I><B>X</B></I>κ / λ),
</TD></TR>
</TABLE><P>
where f<SUP>∗</SUP> and ψ<SUP>∗</SUP> are respectively the Fenchel conjugates
of f and ψ. Denoting by <I><B>w</B></I><SUP>⋆</SUP> the solution of
Eq. (<A HREF="#software:eq:prb">47</A>), the duality gap is interesting in the sense that
it upperbounds the difference with the optimal value of the function:
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">                δ(<I><B>w</B></I>,κ) ≥  <I>g</I>(<I><B>w</B></I>)−<I>g</I>(<I><B>w</B></I><SUP>⋆</SUP>) ≥ 0.
</TD></TR>
</TABLE><P>
Similarly, we will consider pairs of primal-dual variables (<I><B>W</B></I>,<I><B>K</B></I>) when 
dealing with matrices.</P><P>During the optimization, sequences of primal variables <I><B>w</B></I> are available, 
and one wishes to exploit duality gaps for estimating the difference
<I>g</I>(<I><B>w</B></I>)−<I>g</I>(<I><B>w</B></I><SUP>⋆</SUP>). This requires the following components:
</P><UL CLASS="itemize"><LI CLASS="li-itemize">
being able to efficiently compute f<SUP>∗</SUP> and ψ<SUP>∗</SUP>.
</LI><LI CLASS="li-itemize">being able to obtain a “good” dual variable κ given a primal
variable <I><B>w</B></I>, such that δ(<I><B>w</B></I>,κ) is close to
<I>g</I>(<I><B>w</B></I>)−<I>g</I>(<I><B>w</B></I><SUP>⋆</SUP>).
</LI></UL><P>We suppose that the first point is satisfied (we will detail these computations
for every loss and regularization functions in the sequel), and explain how to
choose κ in general (details will also be given in the sequel).</P><P>Let us first consider the choice that associates with a primal variable <I><B>w</B></I>, the
dual variable 
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">
κ(<I><B>w</B></I>) </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=2>▵</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">=</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
</TABLE></TD><TD CLASS="dcell"> ∇ f(<I><B>X</B></I><SUP>⊤</SUP><I><B>w</B></I>), <A NAME="software:eq:kappab"></A>
    (49)</TD></TR>
</TABLE><P>
and let us compute δ(<I><B>w</B></I>,κ(<I><B>w</B></I>)).
First, easy computations show that for all vectors <I><B>z</B></I> in ℝ<I><SUP>n</SUP></I>,
f<SUP>∗</SUP>(∇f(<I><B>z</B></I>)) = <I><B>z</B></I><SUP>⊤</SUP>∇f(<I><B>z</B></I>)−f(<I><B>z</B></I>),
which gives
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">


     

</TD><TD CLASS="dcell"><TABLE CELLSPACING=6 CELLPADDING=0><TR><TD ALIGN=right NOWRAP>                δ(<I><B>w</B></I>,κ(<I><B>w</B></I>))</TD><TD ALIGN=center NOWRAP>=</TD><TD ALIGN=left NOWRAP> f(<I><B>X</B></I><SUP>⊤</SUP><I><B>w</B></I>) + λ ψ(<I><B>w</B></I>) + f<SUP>∗</SUP>(∇f(<I><B>X</B></I><SUP>⊤</SUP><I><B>w</B></I>)) + λψ<SUP>∗</SUP>(−<I><B>X</B></I>∇f(<I><B>X</B></I><SUP>⊤</SUP><I><B>w</B></I>)/ λ), </TD><TD ALIGN=right NOWRAP>    (50)</TD></TR>
<TR><TD ALIGN=right NOWRAP>&nbsp;</TD><TD ALIGN=center NOWRAP>=</TD><TD ALIGN=left NOWRAP> λ ψ(<I><B>w</B></I>) + <I><B>w</B></I><SUP>⊤</SUP><I><B>X</B></I>∇f(<I><B>X</B></I><SUP>⊤</SUP><I><B>w</B></I>) + λψ<SUP>∗</SUP>(−<I><B>X</B></I>∇f(<I><B>X</B></I><SUP>⊤</SUP><I><B>w</B></I>)/ λ). 
</TD><TD ALIGN=right NOWRAP>    (51)</TD></TR>
</TABLE></TD></TR>
</TABLE><P>
We now use the classical Fenchel-Young inequality (see, Proposition
3.3.4 of [<A HREF="#borwein">2</A>]) on the function ψ, which gives
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">                                         </TD><TD CLASS="dcell"><TABLE CELLSPACING=6 CELLPADDING=0><TR><TD ALIGN=right NOWRAP>                                         δ(<I><B>w</B></I>,κ(<I><B>w</B></I>))  ≥  <I><B>w</B></I><SUP>⊤</SUP><I><B>X</B></I>∇f(<I><B>X</B></I><SUP>⊤</SUP><I><B>w</B></I>) − <I><B>w</B></I><SUP>⊤</SUP><I><B>X</B></I>∇f(<I><B>X</B></I><SUP>⊤</SUP><I><B>w</B></I>) = 0,</TD></TR>
</TABLE></TD></TR>
</TABLE><P>
with equality if and only if −<I><B>X</B></I>∇f(<I><B>X</B></I><SUP>⊤</SUP><I><B>w</B></I>) belongs to
∂ ψ(<I><B>w</B></I>). Interestingly, we now that first-order optimality
conditions for Eq. (<A HREF="#software:eq:prb2">48</A>) gives that
−<I><B>X</B></I>∇f(<I><B>X</B></I><SUP>⊤</SUP><I><B>w</B></I><SUP>⋆</SUP>) ∈ ∂ ψ(<I><B>w</B></I><SUP>⋆</SUP>).
We have now in hand a non-negative function <I><B>w</B></I> ↦ δ(<I><B>w</B></I>,κ(<I><B>w</B></I>)) of <I><B>w</B></I>, that
upperbounds <I>g</I>(<I><B>w</B></I>)−<I>g</I>(<I><B>w</B></I><SUP>⋆</SUP>) and satisfying δ(<I><B>w</B></I><SUP>⋆</SUP>,κ(<I><B>w</B></I><SUP>⋆</SUP>))=0.</P><P>This is however not a sufficient property to make it a good measure
of the quality of the optimization, and further work is required, 
that will be dependent on f and ψ.
We have indeed proven that δ(<I><B>w</B></I><SUP>⋆</SUP>,κ(<I><B>w</B></I><SUP>⋆</SUP>)) is always 0. However, 
for <I><B>w</B></I> different than <I><B>w</B></I><SUP>⋆</SUP>, δ(<I><B>w</B></I><SUP>⋆</SUP>,κ(<I><B>w</B></I><SUP>⋆</SUP>)) can be infinite, 
making it a non-informative duality-gap. The reasons for this can be one of the following:
</P><UL CLASS="itemize"><LI CLASS="li-itemize">
The term ψ<SUP>∗</SUP>(−<I><B>X</B></I>∇f(<I><B>X</B></I><SUP>⊤</SUP><I><B>w</B></I>)/ λ) might have an infinite value.
</LI><LI CLASS="li-itemize">Intercepts make the problem more complicated. One can write the formulation with an intercept by
adding a row to <I><B>X</B></I> filled with the value 1, add one dimension to the vector <I><B>w</B></I>, and consider
a regularization function ψ that does regularize the last entry of
<I><B>w</B></I>. This further complexifies the computation of ψ<SUP>∗</SUP> and its
definition, as shown in the next section.
</LI></UL><P>Let us now detail how we proceed to solve these problems, but first without considering the intercept.
The analysis is similar when working with matrices <I><B>W</B></I> instead of vectors <I><B>w</B></I>.
</P><!--TOC subsubsection Duality Gaps without Intercepts-->
<H4 CLASS="subsubsection"><!--SEC ANCHOR --><A NAME="htoc52">A.0.1</A>  Duality Gaps without Intercepts</H4><!--SEC END --><P>
Let us show how to compute the Fenchel conjugate of the functions we have introduced.
We now present the Fenchel conjugate of the loss functions f.
</P><UL CLASS="itemize"><LI CLASS="li-itemize">
<B>The square loss</B> 
<TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell"><TABLE CELLSPACING=6 CELLPADDING=0><TR><TD ALIGN=left NOWRAP><TABLE CLASS="display"><TR VALIGN="middle"><TD CLASS="dcell">                                         f(<I><B>z</B></I>)=</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2<I>n</I></TD></TR>
</TABLE></TD><TD CLASS="dcell">||<I><B>y</B></I>−<I><B>z</B></I>||<SUB>2</SUB><SUP>2</SUP>, </TD></TR>
</TABLE></TD></TR>
<TR><TD ALIGN=left NOWRAP><TABLE CLASS="display"><TR VALIGN="middle"><TD CLASS="dcell">                                            f<SUP>∗</SUP>(κ)=</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">2</TD></TR>
</TABLE></TD><TD CLASS="dcell">||κ||<SUB>2</SUB><SUP>2</SUP> + κ<SUP>⊤</SUP><I><B>y</B></I>.</TD></TR>
</TABLE></TD></TR>
</TABLE></TD></TR>
</TABLE>
</LI><LI CLASS="li-itemize"><B>The logistic loss</B> 
<TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell"><TABLE CELLSPACING=6 CELLPADDING=0><TR><TD ALIGN=left NOWRAP><TABLE CLASS="display"><TR VALIGN="middle"><TD CLASS="dcell">                                            f(<I><B>z</B></I>)=</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
</TABLE></TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> log(1+<I>e</I><SUP>−<I>y<SUB>i</SUB><B>z</B><SUB>i</SUB></I></SUP>) </TD></TR>
</TABLE></TD></TR>
<TR><TD ALIGN=left NOWRAP><TABLE CLASS="display"><TR VALIGN="middle"><TD CLASS="dcell">                                               f<SUP>∗</SUP>(κ)=</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR VALIGN="middle"><TD CLASS="dcell">⎧<BR>
⎪<BR>
⎪<BR>
⎨<BR>
⎪<BR>
⎪<BR>
⎩</TD><TD CLASS="dcell"><TABLE CELLSPACING=6 CELLPADDING=0><TR><TD ALIGN=left NOWRAP>                                               +∞  if  ∃ <I>i</I>∈ [ 1;<I>n</I> ]   s.t.   <I>y<SUB>i</SUB></I>κ<I><SUB>i</SUB></I> ∉ [−1,0],</TD></TR>
<TR><TD ALIGN=left NOWRAP><TABLE CLASS="display"><TR VALIGN="middle"><TD CLASS="dcell">                                                  </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell">(1+<I>y<SUB>i</SUB></I>κ<I><SUB>i</SUB></I>)log(1+<I>y<SUB>i</SUB></I>κ<I><SUB>i</SUB></I>)−<I>y<SUB>i</SUB></I>κ<I><SUB>i</SUB></I>log(−<I>y<SUB>i</SUB></I>κ<I><SUB>i</SUB></I>)  otherwise.</TD></TR>
</TABLE></TD></TR>
</TABLE></TD></TR>
</TABLE></TD></TR>
</TABLE></TD></TR>
</TABLE></TD></TR>
</TABLE>
</LI><LI CLASS="li-itemize"><B>The multiclass logistic loss (or softmax)</B>. The primal variable is now a matrix <I><B>Z</B></I>, in ℝ<SUP><I>n</I> × <I>r</I></SUP>, which represents the product <I><B>X</B></I><SUP>⊤</SUP><I><B>W</B></I>. We denote by <I><B>K</B></I> the dual variable in ℝ<SUP><I>n</I> × <I>r</I></SUP>.
<TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell"><TABLE CELLSPACING=6 CELLPADDING=0><TR><TD ALIGN=left NOWRAP><TABLE CLASS="display"><TR VALIGN="middle"><TD CLASS="dcell">                                                  f(<I><B>Z</B></I>)=</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
</TABLE></TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> log</TD><TD CLASS="dcell">⎛<BR>
⎜<BR>
⎝</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>r</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>j</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> <I>e</I><SUP> <I><B>Z</B><SUB>ij</SUB></I> − <I><B>Z</B><SUB>i<B>y</B>i</SUB></I></SUP></TD><TD CLASS="dcell">⎞<BR>
⎟<BR>
⎠</TD></TR>
</TABLE></TD></TR>
<TR><TD ALIGN=left NOWRAP><TABLE CLASS="display"><TR VALIGN="middle"><TD CLASS="dcell">                                                     f<SUP>∗</SUP>(<I><B>K</B></I>)=</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR VALIGN="middle"><TD CLASS="dcell">⎧<BR>
⎪<BR>
⎪<BR>
⎨<BR>
⎪<BR>
⎪<BR>
⎩</TD><TD CLASS="dcell"><TABLE CELLSPACING=6 CELLPADDING=0><TR><TD ALIGN=left NOWRAP>                                                     +∞  if  ∃ <I>i</I> ∈[ 1;<I>n</I> ]   s.t.   { <I><B>K</B><SUB>ij</SUB></I> &lt; 0  and  <I>j</I> ≠ <I><B>y</B><SUB>i</SUB></I> }  or  <I><B>K</B><SUB>i<B>y</B>i</SUB></I> &lt; −1,</TD></TR>
<TR><TD ALIGN=left NOWRAP><TABLE CLASS="display"><TR VALIGN="middle"><TD CLASS="dcell">                                                        </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> </TD><TD CLASS="dcell">⎡<BR>
⎢<BR>
⎣</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>j</I> ≠ <I><B>y</B><SUB>i</SUB></I></TD></TR>
</TABLE></TD><TD CLASS="dcell"><I><B>K</B><SUB>ij</SUB></I>log(<I><B>K</B><SUB>ij</SUB></I>) + (1+<I><B>K</B></I><SUB><I>i</I> <I><B>y</B>i</I></SUB>)log(1+<I><B>K</B></I><SUB><I>i</I> <I><B>y</B>i</I></SUB>)</TD><TD CLASS="dcell">⎤<BR>
⎥<BR>
⎦</TD><TD CLASS="dcell">.</TD></TR>
</TABLE></TD></TR>
</TABLE></TD></TR>
</TABLE></TD></TR>
</TABLE></TD></TR>
</TABLE></TD></TR>
</TABLE>
</LI></UL><P>Our first remark is that the choice Eq. (<A HREF="#software:eq:kappab">49</A>) ensures
that f(κ) is not infinite.</P><P>As for the regularization function, except for the Tikhonov regularization
which is self-conjugate (it is equal to its Fenchel conjugate), we have
considered functions that are norms. There exists therefore a norm ||.|| such
that ψ(<I><B>w</B></I>)=||<I><B>w</B></I>||, and we denote by ||.||<SUB>∗</SUB> its dual-norm. In such a
case, the Fenchel conjugate of ψ for a vector γ in ℝ<I><SUP>p</SUP></I> takes the
form
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">                                                        ψ<SUP>∗</SUP>(γ) = </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR VALIGN="middle"><TD CLASS="dcell">⎧<BR>
⎨<BR>
⎩</TD><TD CLASS="dcell"><TABLE CELLSPACING=6 CELLPADDING=0><TR><TD ALIGN=left NOWRAP>                                                        0</TD><TD ALIGN=left NOWRAP> if  ||γ||<SUB>∗</SUB>≤ 1, </TD></TR>
<TR><TD ALIGN=left NOWRAP>                                                           +∞</TD><TD ALIGN=left NOWRAP> otherwise.</TD></TR>
</TABLE></TD></TR>
</TABLE></TD></TR>
</TABLE><P>
It turns out that for almost all the norms we have presented, there exists (i)
either a closed form for the dual-norm or (ii) there exists an 
efficient algorithm evaluating it. The only one which does not conform to this
statement is the tree-structured sum of ℓ<SUB>2</SUB>-norms, for which we do not know
how to evaluate it efficiently.</P><P>One can now slightly modify the definition of κ
to ensure that ψ<SUP>∗</SUP>(−<I><B>X</B></I>κ/λ) ≠
+∞. A natural choice is
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">   κ(<I><B>w</B></I>) </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=2>▵</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">=</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
</TABLE></TD><TD CLASS="dcell">
min</TD><TD CLASS="dcell">⎛<BR>
⎜<BR>
⎝</TD><TD CLASS="dcell">1,</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">λ</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">||<I><B>X</B></I>∇f(<I><B>X</B></I><SUP>⊤</SUP><I><B>w</B></I>)||<SUB>∗</SUB></TD></TR>
</TABLE></TD><TD CLASS="dcell">⎞<BR>
⎟<BR>
⎠</TD><TD CLASS="dcell">∇
f(<I><B>X</B></I><SUP>⊤</SUP><I><B>w</B></I>), 
</TD></TR>
</TABLE><P>
which is the one we have implemented. With this new choice, it is easy to see
that for all vectors <I><B>w</B></I> in ℝ<I><SUP>p</SUP></I>, we still have f<SUP>∗</SUP>(κ) ≠ + ∞, and
finally, we also have δ(<I><B>w</B></I>,κ(<I><B>w</B></I>)) &lt; + ∞ and
δ(<I><B>w</B></I><SUP>⋆</SUP>,κ(<I><B>w</B></I><SUP>⋆</SUP>))=0, making it potentially a good
duality gap.</P><!--TOC subsubsection Duality Gaps with Intercepts-->
<H4 CLASS="subsubsection"><!--SEC ANCHOR --><A NAME="htoc53">A.0.2</A>  Duality Gaps with Intercepts</H4><!--SEC END --><P>
Even though adding an intercept does seem a simple modification to the original
problem, it induces difficulties for finding good dual variables.</P><P>We recall that having an intercept is equivalent to having a problem of the
type (<A HREF="#software:eq:prb2">48</A>), by adding a row to <I><B>X</B></I> filled with the value
1, adding one dimension to the vector <I><B>w</B></I> (or one row for matrices <I><B>W</B></I>),
and by using a regularization function that does not depend on the last entry
of <I><B>w</B></I> (or the last row of <I><B>W</B></I>).</P><P>Suppose that we are considering a problem of
type (<A HREF="#software:eq:prb2">48</A>) of dimension <I>p</I>+1, but we are using a
regularization function ψ: ℝ<SUP><I>p</I>+1</SUP> → ℝ, such that for a
vector <I><B>w</B></I> in ℝ<SUP><I>p</I>+1</SUP>, ψ(<I><B>w</B></I>) =<FONT SIZE=2><SUP>▵</SUP></FONT> ψ(<I><B>w</B></I><SUB>[ 1;<I>p</I> ]</SUB>),
where ψ: ℝ<I><SUP>p</SUP></I> → ℝ is one of the regularization function we have
introduced. Then, considering a primal variable <I><B>w</B></I>, a dual variable κ, 
and writing γ=<FONT SIZE=2><SUP>▵</SUP></FONT>−<I><B>X</B></I>κ/λ, we are interested in computing
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">   ψ<SUP>∗</SUP>(γ) = </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR VALIGN="middle"><TD CLASS="dcell">⎧<BR>
⎨<BR>
⎩</TD><TD CLASS="dcell"><TABLE CELLSPACING=6 CELLPADDING=0><TR><TD ALIGN=left NOWRAP>   +∞  if  γ<SUB><I>p</I>+1</SUB> ≠ 0 </TD></TR>
<TR><TD ALIGN=left NOWRAP>      ψ<SUP>∗</SUP>(γ<SUB>[ 1;<I>p</I> ]</SUB>)  otherwise,</TD></TR>
</TABLE></TD></TR>
</TABLE></TD></TR>
</TABLE><P>
which means that in order the duality gap not to be infinite, one needs in addition to ensure
that γ<SUB><I>p</I>+1</SUB> be zero. Since the last row of <I><B>X</B></I> is filled with ones, this writes
down ∑<SUB><I>i</I>=1</SUB><SUP><I>p</I>+1</SUP> κ<I><SUB>i</SUB></I>=0.
For the formulation with matrices <I><B>W</B></I> and <I><B>K</B></I>, the constraint is similar but for every
column of <I><B>K</B></I>.</P><P>Let us now detail how we proceed for every loss function to find a “good”
dual variable κ satisfying this additional constraint, given a primal
variable <I><B>w</B></I> in ℝ<SUP><I>p</I>+1</SUP>, we first define the auxiliary function
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">   κ′(<I><B>w</B></I>) </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=2>▵</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">=</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
</TABLE></TD><TD CLASS="dcell"> ∇f(<I><B>X</B></I><SUP>⊤</SUP><I><B>w</B></I>),
</TD></TR>
</TABLE><P>
(which becomes <I><B>K</B></I>′(<I><B>W</B></I>)=<FONT SIZE=2><SUP>▵</SUP></FONT> ∇f(<I><B>X</B></I><SUP>⊤</SUP><I><B>W</B></I>) for matrices),
and then define another auxiliary function κ″(<I><B>w</B></I>) as follows,
to take into account the additional constraint ∑<SUB><I>i</I>=1</SUB><SUP><I>p</I>+1</SUP> κ<I><SUB>i</SUB></I>=0.
</P><UL CLASS="itemize"><LI CLASS="li-itemize">
<B>For the square loss</B>, we define another auxiliary function:
<TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">   κ″(<I><B>w</B></I>) </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=2>▵</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">=</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
</TABLE></TD><TD CLASS="dcell"> κ′(<I><B>w</B></I>) − </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">1</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
</TABLE></TD><TD CLASS="dcell"><B>1</B><SUB><I>p</I>+1</SUB><SUP>⊤</SUP>κ′(<I><B>w</B></I>)<B>1</B><SUB><I>p</I>+1</SUB> 
</TD></TR>
</TABLE>
where <B>1</B><SUB><I>p</I>+1</SUB> is a vector of size <I>p</I>+1 filled with ones. This step,
ensures that ∑<SUB><I>i</I>=1</SUB><SUP><I>p</I>+1</SUP>κ″(<I><B>w</B></I>)<I><SUB>i</SUB></I>= 0.
</LI><LI CLASS="li-itemize"><B>For the logistic loss</B>, the situation is slightly more complicated since additional constraints are involved in the definition of f<SUP>∗</SUP>.
<TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">   κ″(<I><B>w</B></I>) </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=2>▵</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">=</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
</TABLE></TD><TD CLASS="dcell"> <I>arg</I> <I>min</I><SUB>κ ∈ ℝ<SUP><I>n</I></SUP></SUB> ||κ−κ′(<I><B>w</B></I>)||<SUB>2</SUB><SUP>2</SUP>   s.t.   </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> κ<I><SUB>i</SUB></I>=0  and  ∀ <I>i</I> ∈ [ 1;<I>n</I> ], κ<I><SUB>i</SUB></I> ∈ [−1,0].
</TD></TR>
</TABLE>
This problem can be solved in linear-time [<A HREF="#brucker">3</A>]
using a similar algorithm as for the projection onto the ℓ<SUB>1</SUB>-ball,
since it is an instance of a <EM>quadratic knapsack problem</EM>.
</LI><LI CLASS="li-itemize"><B>For the multi-class logistic loss</B>, we proceed in a similar way, for every column <I><B>K</B><SUP>j</SUP></I> of <I><B>K</B></I>, <I>j</I> ∈ [ 1;<I>r</I> ]:
<DIV CLASS="flushleft"><TABLE CLASS="display"><TR><TD CLASS="dcell">
<I><B>K</B></I><SUP>′′ <I>j</I></SUP>(<I><B>w</B></I>) </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=2>▵</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">=</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
</TABLE></TD><TD CLASS="dcell"> <I>arg</I> <I>min</I><SUB>κ ∈ ℝ<SUP><I>n</I></SUP></SUB> ||<I><B>K</B></I><SUP>′ <I>j</I></SUP>−κ′(<I><B>w</B></I>)||<SUB>2</SUB><SUP>2</SUP>   s.t.   </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><I>n</I></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=6>∑</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center"><I>i</I>=1</TD></TR>
</TABLE></TD><TD CLASS="dcell"> κ<I><SUB>i</SUB></I>=0  and  </TD></TR>
</TABLE></DIV><DIV CLASS="flushright"> ∀ <I>i</I> ∈ [ 1;<I>n</I> ], {κ<I><SUB>i</SUB></I> ≥ 0  if  <I>j</I> ≠ <I><B>y</B><SUB>i</SUB></I>}  and  {κ<I><SUB>i</SUB></I> ≥ −1  if  <I><B>y</B><SUB>i</SUB></I>=<I>j</I>}.
</DIV>
</LI></UL><P>
When the function ψ is the Tykhonov regularization function, we end the process by setting κ(<I><B>w</B></I>)=κ″(<I><B>w</B></I>).
When it is a norm, we choose, as before for taking into account the constraint ||<I><B>X</B></I>κ||<SUB>∗</SUB>≤ λ,
</P><TABLE CLASS="display dcenter"><TR VALIGN="middle"><TD CLASS="dcell">   κ(<I><B>w</B></I>) </TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center"><FONT SIZE=2>▵</FONT></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">=</TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">&nbsp;</TD></TR>
</TABLE></TD><TD CLASS="dcell">  min</TD><TD CLASS="dcell">⎛<BR>
⎜<BR>
⎝</TD><TD CLASS="dcell">1,</TD><TD CLASS="dcell"><TABLE CLASS="display"><TR><TD CLASS="dcell" ALIGN="center">λ</TD></TR>
<TR><TD CLASS="hbar"></TD></TR>
<TR><TD CLASS="dcell" ALIGN="center">||<I><B>X</B></I>κ″(<I><B>w</B></I>)||<SUB>∗</SUB></TD></TR>
</TABLE></TD><TD CLASS="dcell">⎞<BR>
⎟<BR>
⎠</TD><TD CLASS="dcell">κ″(<I><B>w</B></I>),
</TD></TR>
</TABLE><P>
with a similar formulation for matrices <I><B>W</B></I> and <I><B>K</B></I>.</P><P>Even though finding dual variables while taking into account the intercept
requires quite a lot of engineering, notably implementing a quadratic knapsack
solver, it can be done efficiently.</P><!--TOC section References-->
<H2 CLASS="section"><!--SEC ANCHOR -->References</H2><!--SEC END --><DL CLASS="thebibliography"><DT CLASS="dt-thebibliography">
<A NAME="beck"><FONT COLOR=purple>[1]</FONT></A></DT><DD CLASS="dd-thebibliography">
A. Beck and M. Teboulle.
A fast iterative shrinkage-thresholding algorithm for linear inverse
problems.
<EM>SIAM Journal on Imaging Sciences</EM>, 2(1):183–202, 2009.</DD><DT CLASS="dt-thebibliography"><A NAME="borwein"><FONT COLOR=purple>[2]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. M. Borwein and A. S. Lewis.
<EM>Convex analysis and nonlinear optimization: Theory and
examples</EM>.
Springer, 2006.</DD><DT CLASS="dt-thebibliography"><A NAME="brucker"><FONT COLOR=purple>[3]</FONT></A></DT><DD CLASS="dd-thebibliography">
P. Brucker.
An O(n) algorithm for quadratic knapsack problems.
3:163–166, 1984.</DD><DT CLASS="dt-thebibliography"><A NAME="candes4"><FONT COLOR=purple>[4]</FONT></A></DT><DD CLASS="dd-thebibliography">
E. J. Candès, M. Wakin, and S. Boyd.
Enhancing sparsity by reweighted l1 minimization.
<EM>Journal of Fourier Analysis and Applications</EM>, 14:877–905,
2008.</DD><DT CLASS="dt-thebibliography"><A NAME="cherkassky"><FONT COLOR=purple>[5]</FONT></A></DT><DD CLASS="dd-thebibliography">
B. V. Cherkassky and A. V. Goldberg.
On implementing the push-relabel method for the maximum flow problem.
<EM>Algorithmica</EM>, 19(4):390–410, 1997.</DD><DT CLASS="dt-thebibliography"><A NAME="cotter"><FONT COLOR=purple>[6]</FONT></A></DT><DD CLASS="dd-thebibliography">
S. F. Cotter, J. Adler, B. Rao, and K. Kreutz-Delgado.
Forward sequential algorithms for best basis selection.
In <EM>IEEE Proceedings of Vision Image and Signal Processing</EM>,
pages 235–244, 1999.</DD><DT CLASS="dt-thebibliography"><A NAME="duchi"><FONT COLOR=purple>[7]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra.
Efficient projections onto the ℓ<SUB>1</SUB>-ball for learning in high
dimensions.
In <EM>Proceedings of the International Conference on Machine
Learning (ICML)</EM>, 2008.</DD><DT CLASS="dt-thebibliography"><A NAME="efron"><FONT COLOR=purple>[8]</FONT></A></DT><DD CLASS="dd-thebibliography">
B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani.
Least angle regression.
<EM>Annals of statistics</EM>, 32(2):407–499, 2004.</DD><DT CLASS="dt-thebibliography"><A NAME="friedman"><FONT COLOR=purple>[9]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. Friedman, T. Hastie, H. Hölfling, and R. Tibshirani.
Pathwise coordinate optimization.
<EM>Annals of statistics</EM>, 1(2):302–332, 2007.</DD><DT CLASS="dt-thebibliography"><A NAME="Friedman2010"><FONT COLOR=purple>[10]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. Friedman, T. Hastie, and R. Tibshirani.
A note on the group lasso and a sparse group lasso.
Technical report, Preprint arXiv:1001.0736, 2010.</DD><DT CLASS="dt-thebibliography"><A NAME="fu"><FONT COLOR=purple>[11]</FONT></A></DT><DD CLASS="dd-thebibliography">
W. J. Fu.
Penalized regressions: The bridge versus the Lasso.
<EM>Journal of computational and graphical statistics</EM>, 7:397–416,
1998.</DD><DT CLASS="dt-thebibliography"><A NAME="goldberg"><FONT COLOR=purple>[12]</FONT></A></DT><DD CLASS="dd-thebibliography">
A. V. Goldberg and R. E. Tarjan.
A new approach to the maximum flow problem.
In <EM>Proc. of ACM Symposium on Theory of Computing</EM>, pages
136–146, 1986.</DD><DT CLASS="dt-thebibliography"><A NAME="hoyer"><FONT COLOR=purple>[13]</FONT></A></DT><DD CLASS="dd-thebibliography">
P. O. Hoyer.
Non-negative sparse coding.
In <EM>Proc. IEEE Workshop on Neural Networks for Signal
Processing</EM>, 2002.</DD><DT CLASS="dt-thebibliography"><A NAME="jenatton3"><FONT COLOR=purple>[14]</FONT></A></DT><DD CLASS="dd-thebibliography">
R. Jenatton, J. Mairal, G. Obozinski, and F. Bach.
Proximal methods for sparse hierarchical dictionary learning.
In <EM>Proceedings of the International Conference on Machine
Learning (ICML)</EM>, 2010.</DD><DT CLASS="dt-thebibliography"><A NAME="jenatton4"><FONT COLOR=purple>[15]</FONT></A></DT><DD CLASS="dd-thebibliography">
R. Jenatton, J. Mairal, G. Obozinski, and F. Bach.
Proximal methods for hierarchical sparse coding.
<EM>Journal of Machine Learning Research</EM>, 12:2297–2334, 2011.</DD><DT CLASS="dt-thebibliography"><A NAME="lee2"><FONT COLOR=purple>[16]</FONT></A></DT><DD CLASS="dd-thebibliography">
D. D. Lee and H. S. Seung.
Algorithms for non-negative matrix factorization.
In <EM>Advances in Neural Information Processing Systems</EM>, 2001.</DD><DT CLASS="dt-thebibliography"><A NAME="maculan"><FONT COLOR=purple>[17]</FONT></A></DT><DD CLASS="dd-thebibliography">
N. Maculan and J. R. G. Galdino de Paula.
A linear-time median-finding algorithm for projecting a vector on the
simplex of Rn.
<EM>Operations research letters</EM>, 8(4):219–222, 1989.</DD><DT CLASS="dt-thebibliography"><A NAME="mairal11"><FONT COLOR=purple>[18]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. Mairal.
<EM>Sparse coding for machine learning, image processing and
computer vision</EM>.
PhD thesis, Ecole Normale Supérieure, Cachan, 2010.</DD><DT CLASS="dt-thebibliography"><A NAME="mairal7"><FONT COLOR=purple>[19]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. Mairal, F. Bach, J. Ponce, and G. Sapiro.
Online dictionary learning for sparse coding.
In <EM>Proceedings of the International Conference on Machine
Learning (ICML)</EM>, 2009.</DD><DT CLASS="dt-thebibliography"><A NAME="mairal9"><FONT COLOR=purple>[20]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. Mairal, F. Bach, J. Ponce, and G. Sapiro.
Online learning for matrix factorization and sparse coding.
<EM>Journal of Machine Learning Research</EM>, 11:19–60, 2010.</DD><DT CLASS="dt-thebibliography"><A NAME="mairal10"><FONT COLOR=purple>[21]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. Mairal, R. Jenatton, G. Obozinski, and F. Bach.
Network flow algorithms for structured sparsity.
In <EM>Advances in Neural Information Processing Systems</EM>, 2010.</DD><DT CLASS="dt-thebibliography"><A NAME="mairal13"><FONT COLOR=purple>[22]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. Mairal, R. Jenatton, G. Obozinski, and F. Bach.
Convex and network flow optimization for structured sparsity.
<EM>Journal of Machine Learning Research</EM>, 12:2649–2689, 2011.</DD><DT CLASS="dt-thebibliography"><A NAME="mairal14"><FONT COLOR=purple>[23]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. Mairal and B. Yu.
Supervised feature selection in graphs with path coding penalties and
network flows.
Technical report, Preprint arXiv:1204.4539, 2012.</DD><DT CLASS="dt-thebibliography"><A NAME="mallat4"><FONT COLOR=purple>[24]</FONT></A></DT><DD CLASS="dd-thebibliography">
S. Mallat and Z. Zhang.
Matching pursuit in a time-frequency dictionary.
<EM>IEEE Transactions on Signal Processing</EM>, 41(12):3397–3415,
1993.</DD><DT CLASS="dt-thebibliography"><A NAME="meinshausen"><FONT COLOR=purple>[25]</FONT></A></DT><DD CLASS="dd-thebibliography">
N. Meinshausen and P. Buehlmann.
Stability selection.
Technical report.
ArXiv:0809.2932.</DD><DT CLASS="dt-thebibliography"><A NAME="obozinski"><FONT COLOR=purple>[26]</FONT></A></DT><DD CLASS="dd-thebibliography">
G. Obozinski, B. Taskar, and M.I. Jordan.
Joint covariate selection and joint subspace selection for multiple
classification problems.
<EM>Statistics and Computing</EM>, pages 1–22.</DD><DT CLASS="dt-thebibliography"><A NAME="osborne"><FONT COLOR=purple>[27]</FONT></A></DT><DD CLASS="dd-thebibliography">
M. R. Osborne, B. Presnell, and B. A. Turlach.
On the Lasso and its dual.
<EM>Journal of Computational and Graphical Statistics</EM>,
9(2):319–37, 2000.</DD><DT CLASS="dt-thebibliography"><A NAME="sprechmann"><FONT COLOR=purple>[28]</FONT></A></DT><DD CLASS="dd-thebibliography">
P. Sprechmann, I. Ramirez, G. Sapiro, and Y. C. Eldar.
Collaborative hierarchical sparse modeling.
Technical report, 2010.
Preprint arXiv:1003.0400v1.</DD><DT CLASS="dt-thebibliography"><A NAME="tibshirani2"><FONT COLOR=purple>[29]</FONT></A></DT><DD CLASS="dd-thebibliography">
R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight.
Sparsity and smoothness via the fused lasso.
<EM>Journal of the Royal Statistical Society Series B</EM>,
67(1):91–108, 2005.</DD><DT CLASS="dt-thebibliography"><A NAME="tropp3"><FONT COLOR=purple>[30]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. A. Tropp.
Algorithms for simultaneous sparse approximation. part ii: Convex
relaxation.
<EM>Signal Processing, special issue "Sparse approximations in
signal and image processing"</EM>, 86:589–602, April 2006.</DD><DT CLASS="dt-thebibliography"><A NAME="tropp2"><FONT COLOR=purple>[31]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. A. Tropp, A. C. Gilbert, and M. J. Strauss.
Algorithms for simultaneous sparse approximation. part i: Greedy
pursuit.
<EM>Signal Processing, special issue "sparse approximations in
signal and image processing"</EM>, 86:572–588, April 2006.</DD><DT CLASS="dt-thebibliography"><A NAME="weisberg"><FONT COLOR=purple>[32]</FONT></A></DT><DD CLASS="dd-thebibliography">
S. Weisberg.
<EM>Applied Linear Regression</EM>.
Wiley, New York, 1980.</DD><DT CLASS="dt-thebibliography"><A NAME="wu"><FONT COLOR=purple>[33]</FONT></A></DT><DD CLASS="dd-thebibliography">
T. T. Wu and K. Lange.
Coordinate descent algorithms for Lasso penalized regression.
<EM>Annals of Applied Statistics</EM>, 2(1):224–244, 2008.</DD><DT CLASS="dt-thebibliography"><A NAME="yuan"><FONT COLOR=purple>[34]</FONT></A></DT><DD CLASS="dd-thebibliography">
M. Yuan and Y. Lin.
Model selection and estimation in regression with grouped variables.
<EM>Journal of the Royal Statistical Society Series B</EM>, 68:49–67,
2006.</DD><DT CLASS="dt-thebibliography"><A NAME="zou"><FONT COLOR=purple>[35]</FONT></A></DT><DD CLASS="dd-thebibliography">
H. Zou and T. Hastie.
Regularization and variable selection via the elastic net.
<EM>Journal of the Royal Statistical Society Series B</EM>,
67(2):301–320, 2005.</DD></DL><!--CUT END -->
<!--HTMLFOOT-->
<!--ENDHTML-->
<!--FOOTER-->
<HR SIZE=2><BLOCKQUOTE CLASS="quote"><EM>This document was translated from L<sup>A</sup>T<sub>E</sub>X by
<A HREF="http://hevea.inria.fr/index.html">H<FONT SIZE=2><sup>E</sup></FONT>V<FONT SIZE=2><sup>E</sup></FONT>A</A>.</EM></BLOCKQUOTE></BODY>
</HTML>
